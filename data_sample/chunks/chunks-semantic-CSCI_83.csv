chunk_id,document,class,page,chunk_text,chunk_length,chunk_method
9612f272-2d6b-4384-9e16-5184e67ebdfe,Regularization and Sparse Models.pdf,CSCI_83,0,"Regularization and Sparse Models
Steve Elston
11/03/2022",56,semantic
032b394c-2a8c-4872-a30f-3fd391c4a740,Regularization and Sparse Models.pdf,CSCI_83,1,"Dealing with Overï¬t Models
Example: We suspect that the terms, ses and prog are signiï¬cant in predicting social science test scores:
##                             OLS Regression Results                            ## ==============================================================================## Dep. Variable:                  socst   R-squared:                       0.348## Model:                            OLS   Adj. R-squared:                  0.309## Method:                 Least Squares   F-statistic:                     8.753## Date:                Fri, 11 Nov 2022   Prob (F-statistic):           1.55e-09## Time:                        07:55:39   Log-Likelihood:                -495.01## No. Observations:                 140   AIC:                             1008.## Df Residuals:                     131   BIC:                             1035.## Df Model:                           8                                         ## Covariance Type:            nonrobust                                         ## ============================================================================================##                                coef    std err          t      P>|t|      [0.025      0.975]## --------------------------------------------------------------------------------------------## Intercept                   46.1667      2.478     18.627      0.000      41.264      51.070## C(ses)[T.2]                  3.7564      3.437      1.093      0.276      -3.043      10.556## C(ses)[T.3]                 10.3333      4.293      2.407      0.017       1.841      18.826## C(prog)[T.2]                 6.0476      3.378      1.791      0.076      -0.634      12.729## C(prog)[T.3]               -10.2917      3.919     -2.626      0.010     -18.044      -2.539## C(ses)[T.2]:C(prog)[T.2]    -0.8457      4.402     -0.192      0.848      -9.555       7.863## C(ses)[T.3]:C(prog)[T.2]    -2.5476      5.114     -0.498      0.619     -12.664       7.569## C(ses)[T.2]:C(prog)[T.3]     9.2257      4.954      1.862      0.065      -0.573      19.025## C(ses)[T.3]:C(prog)[T.3]     1.2917      6.788      0.190      0.849     -12.136      14.719## ==============================================================================## Omnibus:                       10.190   Durbin-Watson:                   2.059## Prob(Omnibus):                  0.006   Jarque-Bera (JB):               10.371## Skew:                          -0.651   Prob(JB):                      0.00560## Kurtosis:                       3.290   Cond. No. 18.7## ==============================================================================## ## Notes:## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",2732,semantic
74a47c4c-b673-4d67-a688-53f37b2fe55f,Regularization and Sparse Models.pdf,CSCI_83,1,"formula = 'socst ~ C(ses)*C(prog)'
#linear_model = smf.ols(""socst ~ C(ses)*C(prog)"", data=test_scores_train).fit()linear_model = smf.ols(formula, data=test_scores_train).fit()print(linear_model.summary())",204,semantic
60cfb9b3-b4c1-4624-9b79-bb4002e85d6d,Regularization and Sparse Models.pdf,CSCI_83,2,"Dealing with Overï¬t Models
Example: After 3 or 4 rounds of guess and cut feature pruning, we arrive at a model with only signiï¬cant coefï¬cients:
## <class 'statsmodels.iolib.summary.Summary'>## """"""##                             OLS Regression Results                            ## ==============================================================================## Dep. Variable:                  socst   R-squared:                       0.348## Model:                            OLS   Adj. R-squared:                  0.309## Method:                 Least Squares   F-statistic:                     8.753## Date:                Fri, 11 Nov 2022   Prob (F-statistic):           1.55e-09## Time:                        07:55:39   Log-Likelihood:                -495.01## No.",770,semantic
b5905208-6586-4a06-8252-ce2110d7ad25,Regularization and Sparse Models.pdf,CSCI_83,2,"Observations:                 140   AIC:                             1008.## Df Residuals:                     131   BIC:                             1035.## Df Model:                           8                                         ## Covariance Type:            nonrobust                                         ## ========================================================================================##                            coef    std err          t      P>|t|      [0.025      0.975]## ----------------------------------------------------------------------------------------## C(ses)[1]:C(prog)[1]    46.1667      2.478     18.627      0.000      41.264      51.070## C(ses)[2]:C(prog)[1]    49.9231      2.381     20.965      0.000      45.212      54.634## C(ses)[3]:C(prog)[1]    56.5000      3.505     16.120      0.000      49.566      63.434## C(ses)[1]:C(prog)[2]    52.2143      2.295     22.755      0.000      47.675      56.754## C(ses)[2]:C(prog)[2]    55.1250      1.518     36.320      0.000      52.123      58.127## C(ses)[3]:C(prog)[2]    60.0000      1.568     38.277      0.000      56.899      63.101## C(ses)[1]:C(prog)[3]    35.8750      3.035     11.819      0.000      29.870      41.880## C(ses)[2]:C(prog)[3]    48.8571      1.874     26.077      0.000      45.151      52.563## C(ses)[3]:C(prog)[3]    47.5000      4.293     11.065      0.000      39.008      55.992## ==============================================================================## Omnibus:                       10.190   Durbin-Watson:                   2.059## Prob(Omnibus):                  0.006   Jarque-Bera (JB):               10.371## Skew:                          -0.651   Prob(JB):                      0.00560## Kurtosis:                       3.290   Cond. No. 2.83## ==============================================================================## ## Notes:## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.## """"""
linear_model = smf.ols(""socst ~ - 1 + C(ses):C(prog)"", data=test_scores_train).fit()
linear_model.summary()",2091,semantic
4ba0bbdb-887c-4015-8e3c-f8d1183c6fb5,Regularization and Sparse Models.pdf,CSCI_83,3,"Dealing with Overï¬t Models
Letâ€™s compare the results of the unpruned and pruned models
The metrics indicate the ï¬t is exactly the same
Why prefer the sparse (pruned) model? What are the consequences of the over-ï¬t model? Several predictors (features) are included that are not needed
Including non-signiï¬cant predictors can only increase noise and reduce generalization of a model
Colinear features confound model ï¬ting - change of coefï¬cient values correlated
For model with linear response, consider the effect of an unexpected value of a non-signiï¬cant predictor
But manually pruning a model with a great many features is a doomed task!",639,semantic
61128a67-f2f7-46e6-811d-012a06248335,Regularization and Sparse Models.pdf,CSCI_83,3,"What if we included all the interactions with type of school, race and sex? We now have up to 5th order interaction - 45 model coefï¬cients with none signiï¬cant!!",161,semantic
ed16462e-7cb7-4d08-9c35-5ae30ce76d12,Regularization and Sparse Models.pdf,CSCI_83,4,"Dealing with Overï¬t Models
We want our models to be sparse
A sparse model has the minimum complexity required to explain the data
The sparse model is a manifestation of Occamâ€™s Razor
A scientiï¬c principle that the simplest of competing theories is the preferred one
Sparse models use the minimum number of independent variables (features)
Are considered parsimonious
Generalize well
Use regularization methods to identify minimum coefï¬cient set",444,semantic
6a640f1b-96a9-4f7c-8001-48fa93f77e98,Regularization and Sparse Models.pdf,CSCI_83,5,"Dealing with Overï¬t Models
Idea!: Try systematically pruning the model using some metric
Leads to the the step-wise regression algorithm
Forward step-wise regression adds most explanatory variable one at a time
Backward step-wise regression removes lest explanatory variable one at a time
Can go both directions - see the R documentation
Hard to ï¬nd a good metric
But, making multiple hypothesis tests is a fraught undertaking
 is null or insigniï¬cant predictor
High probability of Type 1 or Type 2 error
Type 1 error, fail to accept ,  include insigniï¬cant predictor
Type 2 error, fail to reject ,  drop signiï¬cant predictor
ğ»0
ğ»0â†’
ğ»0â†’",636,semantic
a25ac2f9-482a-4fff-b733-008feadaa797,Regularization and Sparse Models.pdf,CSCI_83,6,"Regularization - The Bias-Variance Trade-Off
Regularization is a systematic approach to preventing over-ï¬tting
To understand regularization need to understand the bias-variance trade-off
To better understand this trade-off decompose mean square error:
 the label vector the feature matrix the trained model
Expanding this relation gives us:
Î”=ğ¸[(ğ‘Œâˆ’(ğ‘‹) ]ğ‘¦2 ğ‘“Ì‚Â  )2
ğ‘Œ=ğ‘‹=(ğ‘¥)=ğ‘“Ì‚Â 
Î”=(ğ¸[(ğ‘‹)]âˆ’(ğ‘‹) +ğ¸[((ğ‘‹)âˆ’ğ¸[(ğ‘‹)] ]+ğ‘¦2 ğ‘“Ì‚Â  ğ‘“Ì‚Â  )
2 ğ‘“Ì‚Â  ğ‘“Ì‚Â  )2 ğœ2
Î”=ğµğ‘–ğ‘ +ğ‘‰ğ‘ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘ğ‘’+ğ¼ğ‘Ÿğ‘Ÿğ‘’ğ‘‘ğ‘¢ğ‘ğ‘–ğ‘ğ‘™ğ‘’Â ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿğ‘¦2 ğ‘ 2",473,semantic
d1c1bb1c-a290-49cd-a84d-39b8d5a12e27,Regularization and Sparse Models.pdf,CSCI_83,7,"Regularization - The Bias-Variance Trade-Off
How do we interpret the bias-variance trade-off relationship:
- , the expected squared difference between the model output and the expected model output is the variance of the model
- For low variance model, 
- Model generalizes since variance is low for each prediction, 
, the expected value of the difference between the model output and the expected model output is the bias of the model
For unbiased model, 
Example: OLS model with  is unbiased
 is inherent or irreducable error in data
Î”=(ğ¸[(ğ‘‹)]âˆ’(ğ‘‹) +ğ¸[((ğ‘‹)âˆ’ğ¸[(ğ‘‹)] ]+ğ‘¦2 ğ‘“Ì‚Â  ğ‘“Ì‚Â  )
2 ğ‘“Ì‚Â  ğ‘“Ì‚Â  )2 ğœ2
ğ¸[((ğ‘‹)âˆ’ğ¸[(ğ‘‹)] ]ğ‘“Ì‚Â  ğ‘“Ì‚Â  )2
ğ¸[((ğ‘‹)âˆ’ğ¸[(ğ‘‹)] ]â†’0ğ‘“Ì‚Â  ğ‘“Ì‚Â  )2
(ğ‘‹)ğ‘“Ì‚Â 
(ğ¸[(ğ‘‹)]âˆ’(ğ‘‹)ğ‘“Ì‚Â  ğ‘“Ì‚Â  )
2
ğ¸[ (ğ‘‹)]âˆ’(ğ‘‹)]=0ğ‘“Ì‚Â  ğ‘“Ì‚Â 
ğ‘Ÿğ‘’ğ‘ ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™ğ‘ âˆ¼ ğ‘(0, )ğœ2
ğœ2",725,semantic
342d0fcc-0f86-4bb6-8b9c-a015e6325892,Regularization and Sparse Models.pdf,CSCI_83,8,"Regularization - The Bias-Variance Trade-Off
There is a trade-off between bias and variance
Need to ï¬nd the trade-off point
The trade-off between bias and variance",163,semantic
23ce0456-478c-4e51-845a-a26dae1e1e6c,Regularization and Sparse Models.pdf,CSCI_83,9,"Eigendecomposition - Review
For OLS model need to ï¬nd  for  design matrix 
Decompose the  covariance matrix  into eigenvalues and eigenvectors:
where,
 is the  matrix of  orthonormal eigenvectors
And, the  eigenvalues are represented as diagonal matrix:
For real-valued  the eigenvectors  are real valued so:
( ğ´ ğ´âˆ ğ¶ğ‘œğ‘£(ğ´ ğ´ğ´ğ‘‡ )âˆ’1 )âˆ’1 ğ‘›Ã—ğ‘ ğ´
ğ‘Ã—ğ‘ ğ‘ğ‘œğ‘£(ğ´)= ğ´1ğ‘ğ´ğ‘‡
ğ‘ğ‘œğ‘£(ğ´)=ğ‘„Î›ğ‘„âˆ’1
ğ‘„ ğ‘Ã—ğ‘ ğ‘
ğ‘
Î›=
â¡
â£
â¢â¢â¢â¢
,0,â€¦,0ğœ†1
0, ,â€¦ğœ†2
â‹® ,Â â‹® ,Â â‹® ,Â â‹® 0,0,â€¦,ğœ†ğ‘
â¤
â¦
â¥â¥â¥â¥
ğ‘ğ‘œğ‘£(ğ´) ğ‘„
=ğ‘„âˆ’1 ğ‘„ğ‘‡",458,semantic
8d45928c-ae3d-4547-93ea-65d32fcba7f5,Regularization and Sparse Models.pdf,CSCI_83,10,"Eigendecomposition - Review
The inverse of the covariance can be computed from its eigendecomposition
Where  is written:
ğ‘ğ‘œğ‘£(ğ´ = ğ‘„)âˆ’1 ğ‘„ğ‘‡Î›âˆ’1
ğ‘„âˆ’1
=Î›âˆ’1
â¡
â£
â¢â¢â¢â¢â¢
,0,â€¦,01ğœ†1
0, ,â€¦1ğœ†2
â‹® ,Â â‹® ,Â â‹® ,Â â‹® 0,0,â€¦,1ğœ†ğ‘
â¤
â¦
â¥â¥â¥â¥â¥",211,semantic
802740c1-fa6f-4e07-812d-be59b71ab10e,Regularization and Sparse Models.pdf,CSCI_83,11,"Eigendecomposition - Review
At ï¬rst look eigen-decomposition seems a bit mysterious
The eigenvalues are the roots of the covariance matrix
Similar to the familiar roots of a polynomial
For square matrix, , and some norm 1 vector, , we can ï¬nd a root, :
For  dimensional matrix there are  eigenvalues and orthogonal eigenvalue
But, there is no guarantee that the  values of  are unique
If columns of  are colinear, there are  unique eigenvectors
ğ´ ğ‘¥ ğœ†
ğ´ğ‘¥ğ´ğ‘¥âˆ’ğœ†ğ‘¥(ğ´âˆ’ğ¼ğœ†)ğ‘¥
=ğœ†ğ‘¥=0=0
ğ‘Ã—ğ‘ ğ‘
ğ‘ ğœ†
ğ´ <ğ‘",488,semantic
795116b6-ce49-4d3f-828c-38a5c269e465,Regularization and Sparse Models.pdf,CSCI_83,12,"Regularization - Ill-Posed Problems
If columns of  are not linearly independent, the covariance matrix is ill-posed and the eigenvectors are under-determined
The eigenvalues are ordered, 
For ill-posed covariance matrix
The smallest 
So, 
Uh oh! The inverse covariance matrix does not exist! With colinear features  confounded ï¬tting
With unifromative features  projecting random noise
ğ´
ğ‘ğ‘œğ‘£(ğ´)=ğ‘„Î›ğ‘„ğ‘‡
ğ‘ğ‘œğ‘£(ğ´ = ğ‘„)âˆ’1 ğ‘„ğ‘‡Î›âˆ’1
â‰¥ â‰¥â€¦â‰¥ğœ†1 ğœ†2 ğœ†ğ‘
â†’0ğœ†ğ‘–
â†’âˆ1ğœ†ğ‘–
â†’0Â â†’ğœ†ğ‘–
â†’0Â â†’ğœ†ğ‘–",457,semantic
6719d42a-600a-4183-be91-3f243c0c6b3a,Regularization and Sparse Models.pdf,CSCI_83,13,"L2 Regularization
L2 regularization constrains the Euclidean norm of the parameter vector, 
Recall:
The normal equations provide a solution:
This solution requires ï¬nding the inverse of the covariance matrix, 
But this inverse may be unstable
In mathematical terminology we say the problem is ill-posed
ğ‘âƒ—Â 
ğ‘¥=ğ´ğ‘+ğœ–
ğ‘=( ğ´ ğ‘¥ğ´ğ‘‡ )âˆ’1ğ´ğ‘‡
( ğ´ğ´ğ‘‡ )âˆ’1",339,semantic
c6bde2f4-9f54-49c6-9723-535955ae78ec,Regularization and Sparse Models.pdf,CSCI_83,14,"L2 Regularization
L2 regularization constrains the Euclidean norm of the parameter vector, 
We can add a bias term to the diagonal of the covariance matrix
the L2 or Euclidean norm minimization problem):
Where the L2 norm of the coefï¬cient vector is:
ğ‘âƒ—Â 
ğ‘šğ‘–ğ‘›[âˆ¥ ğ´â‹… ğ‘¥âˆ’ğ‘âˆ¥ +Â  âˆ¥ ğ‘âˆ¥ ]ğ›¼2
ğ‘œğ‘Ÿğ‘=( ğ´+ ğ¼ ğ‘¥ğ´ğ‘‡ ğ›¼2)âˆ’1ğ´ğ‘‡
||ğ‘||=( + +â€¦+ =(ğ›½21 ğ›½22 ğ›½2ğ‘š)
12
âˆ‘ğ‘–=1
ğ‘š
ğ›½2ğ‘–)
12",350,semantic
e28488c3-7675-427d-87a4-2acb49eab18e,Regularization and Sparse Models.pdf,CSCI_83,15,"L2 Regularization
How can we understand this relationship? Adds values along the diagonal of the covariance matrix
This creates a so called ridge in the covariance, 
Leads to the term ridge regression
Constrain the L2 norm values of the model coefï¬cients using the penalty term 
Larger  is more bias but lover variance
Larger  makes the inverse of the covariance more stable
L2 regularization is a soft constraint on the model coefï¬cients
Even smallest coefï¬cients are not driven to 0
Coefï¬cients can grow in value, but under the constraint
ğ‘=( ğ´+ ğ¼ ğ‘¥ğ´ğ‘‡ ğ›¼2)âˆ’1ğ´ğ‘‡
ğ‘ğ‘œ = ğ´+ğ‘£ğ‘Ÿğ‘’ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘§ğ‘’ğ‘‘ ğ´ğ‘‡ ğ›¼2
ğ›¼
ğ›¼
ğ›¼",593,semantic
4c2bf7ed-dae4-4a16-b161-1d9fccde1a8d,Regularization and Sparse Models.pdf,CSCI_83,16,"L2 Regularization
Eigen-decomposition of the regularized covariance matrix:
The inverse covariance matrix is then:
With any , the inverse eigenvalues of the inverse covariance matrix are bounded
Increasing  increases bias, but increases the stability of the inverse
ğ´+ ğ¼=ğ‘„ğ´ğ‘‡ ğ›¼2
â¡
â£
â¢â¢â¢â¢
+ ,0,â€¦,0ğœ†1 ğ›¼2
0, + ,â€¦ğœ†2 ğ›¼2
â‹® ,Â â‹® ,Â â‹® ,Â â‹® 0,0,â€¦, +ğœ†ğ‘ ğ›¼2
â¤
â¦
â¥â¥â¥â¥ğ‘„ğ‘‡
( ğ´+ ğ¼ = ğ‘„ğ´ğ‘‡ ğ›¼2)
âˆ’1 ğ‘„ğ‘‡
â¡
â£
â¢â¢â¢â¢â¢â¢
,0,â€¦,01+ğœ†1 ğ›¼2
0, ,â€¦1+ğœ†2 ğ›¼2
â‹® ,Â â‹® ,Â â‹® ,Â â‹® 0,0,â€¦, 1+ğœ†ğ‘ğ›¼2
â¤
â¦
â¥â¥â¥â¥â¥â¥
ğ›¼>0
ğ›¼",458,semantic
156bcb0a-6bae-4525-87c4-c99806a948d6,Regularization and Sparse Models.pdf,CSCI_83,17,"L2 Regularization
Example: compute the eigenvalues of a covariance matrix
## array([41.33783544, 13.34471885,  8.85600018,  2.37491505,  1.7218998 ,##         1.6220889 ,  0.11774255,  0.38848204,  0.45853941])
The condition number of the covariance is 
Add regularization with  and compute the eigenvalues
## array([41.43783544, 13.44471885,  8.95600018,  2.47491505,  1.8218998 ,##         1.7220889 ,  0.21774255,  0.48848204,  0.55853941])
The condition number of the covariance is 
test_scores['socst_zero_mean'] = test_scores['socst'] - np.mean(test_scores['socst'])
Y, X = dmatrices(""socst_zero_mean ~ C(ses, levels=[1,2,3])*C(prog, levels=[1,2,3])"", data=test_scores)cov_X = np.matmul(np.transpose(X),X)cov_X = np.divide(cov_X, float(cov_X.shape[0 ]))
np.real(np.linalg.eigvals(cov_X))
âˆ¼ 90
=0.1ğ›¼2
alpha_sqr = 0 . 1  alpha_sqr = np.diag([alpha_sqr] * cov_X.shape[0 ])#alpha_sqr = np.diag([alpha] * cov_X.shape[0])#cov_X = np.divide(np.matmul(np.transpose(X),X), float(cov_X.shape[0]))
cov_X = np.add(cov_X, alpha_sqr)np.real(np.linalg.eigvals(cov_X))
âˆ¼ 72",1063,semantic
900d2f55-4103-4fc0-a550-7f3040ac6da1,Regularization and Sparse Models.pdf,CSCI_83,18,"L2 Regularization
L2 regularization constrains the Euclidean norm of the parameter vector, 
The norm of the coefï¬cient vector, , is constrained
L2 norm constraint of model coefï¬cients
ğ‘âƒ—Â 
ğ‘âƒ—",190,semantic
402492f7-7e97-41ca-95a0-1f5851fc1492,Regularization and Sparse Models.pdf,CSCI_83,19,"L2 Regularization
Example: Increasing constraint on model coefï¬cients with larger L2 regularization hyperparameter
def regularized_coefs(df_train, df_test, alphas, L1_wt=0 . 0 , n_coefs=8 ,
                      formula = 'socst_zero_mean ~ C(ses)*C(prog)',                       label='socst_zero_mean'):    '''Function that computes a linear model for each value of the regularization 
    parameter alpha and returns an array of the coefficient values. The L1_wt     determines the trade-off between L1 and L2 regularization'''    coefs = np.zeros((len(alphas),n_coefs + 1 ))    MSE_train = []
    MSE_test = []    for i,alpha in enumerate(alphas):        ## First compute the training MSE
        temp_mod = smf.ols(formula, data=df_train).fit_regularized(alpha=alpha,L1_wt=L1_wt)        coefs[i,:] = temp_mod.params        MSE_train.append(np.mean(np.square(df_train[label] - temp_mod.predict(df_train))))        ## Then compute the test MSE
        MSE_test.append(np.mean(np.square(df_test[label] - temp_mod.predict(df_test))))            return coefs, MSE_train, MSE_test
alphas = np.arange(0 . 0 , 0 . 3 , step = 0 .",1125,semantic
c09f45bd-da06-499d-ad73-c29ab2931daa,Regularization and Sparse Models.pdf,CSCI_83,19,"0 0 3 )   #alphas = np.arange(0.0, 100.0, step = 1.0)  Betas, MSE_test, MSE_train = regularized_coefs(test_scores_train, test_scores_test, alphas) #, formula=formula)
#Betas[:5]",177,semantic
ff1abcc2-f620-4912-8161-cee7a7e21d3f,Regularization and Sparse Models.pdf,CSCI_83,20,"L2 Regularization
Example: Increasing constraint on model coefï¬cients with larger L2 regularization hyperparameter",114,semantic
af37a787-0634-4171-8d89-5f976ed17bdd,Regularization and Sparse Models.pdf,CSCI_83,21,"L1 Regularization
The L1 norm provides regularization with different properties
Constrains the model parameters using the L1 norm:
This form looks a lot like the L2 regularization formulation
 is the L1 norm
Compute the l1 norm of the  model parameters:
 is the absolute value of . ğ‘šğ‘–ğ‘›[âˆ¥ ğ´â‹… ğ‘¥âˆ’ğ‘âˆ¥ +Â  âˆ¥ ğ‘ ]ğ›¼2 âˆ¥ 1
âˆ¥ ğ‘âˆ¥ 1
ğ‘š
||ğ‘| =(| |+| |+â€¦+| |)=( | ||1 ğ‘1 ğ‘2 ğ‘ğ‘š âˆ‘ğ‘–=1
ğ‘š
ğ‘ğ‘–)
1
| |ğ‘ğ‘– ğ‘ğ‘–",380,semantic
68acfe24-1d63-4e14-85e5-8a91fcbba178,Regularization and Sparse Models.pdf,CSCI_83,22,"L1 Regularization
What are the properties of the L1 regularization
L1 norm is a hard constraint
L1 regularization drives coefï¬cients to zero
The hard constraint property leads to the term lasso regularization
Lasso regression is a method of feature selection",258,semantic
238eff15-c85f-4837-8ab8-f11bf6c430ba,Regularization and Sparse Models.pdf,CSCI_83,23,"L1 Regularization
The Lasso regularization is a strong constraint on coefï¬cient values
Some coefï¬cients are forced to zero
The constraint curve is like a lasso
L1 norm constraint of model coefï¬cients",199,semantic
dc637e35-0fb5-4ace-87f8-83d43b233950,Regularization and Sparse Models.pdf,CSCI_83,24,"L1 Regularization
Example: Increasing constraint on model coefï¬cients with larger L1 regularization hyperparameter
## array([[ -6.13412699,   3.62867936,  10.20615579,   5.88403028,##         -10.44293454,  -0.68215407,  -2.3846305 ,   9.37695361,##           1.44233432],##        [ -5.72441839,   3.43435166,   8.75855958,   4.84586199,##          -9.40956564,   0.05313331,   0. ,   7.99487046,##           0. ],##        [ -5.55039369,   3.33123726,   8.51053097,   4.79924961,##          -9.03159592,   0.",510,semantic
d2c27d64-9805-486b-8741-c64fb182325b,Regularization and Sparse Models.pdf,CSCI_83,24,",   0. ,   7.41265712,##           0. ],##        [ -5.35379746,   3.19004869,   8.26727483,   4.72128125,##          -8.68047004,   0. ,   0. ,   6.87279024,##           0. ],##        [ -5.15720816,   3.04893453,   8.02408465,   4.64326246,##          -8.32934311,   0. ,   0. ,   6.33285484,##           0. ]])
alphas = np.arange(0 .",336,semantic
cc387bf0-8e57-4377-a2b0-92ca1e6e0f8b,Regularization and Sparse Models.pdf,CSCI_83,24,"0 , 0 . 6 , step = 0 . 0 2 )
Betas, MSE_train, MSE_test = regularized_coefs(test_scores_train, test_scores_test, alphas, L1_wt=1 . 0 ) #, formula=formula)Betas[:5 ]",164,semantic
00fe9fdb-8a0c-4629-91bc-a8f483c4cea3,Regularization and Sparse Models.pdf,CSCI_83,25,"L1 Regularization
Example: Increasing constraint on model coefï¬cients with larger L1 regularization hyperparameter",114,semantic
6ad2d68f-00a1-4b20-bf9c-b59832cf2078,Regularization and Sparse Models.pdf,CSCI_83,26,"Elastic Net Regularization
Do we always have to choose between the soft constraint of L2 and the hard constraint of L1? L2 regularization works well for colinear features as soft constraint
Down-weights colinear features
But soft constraint so poor model selection
L1 regularization provides good model selection as hard constraint
Drives coefï¬cients of non-informative variables to 0
But poor selection for colinear features
The Elastic Net weights L1 and L2 regularization
Hyperparameter  weights L1 vs. L2 regularization
Hyperparameter  sets strength of regularization
ğœ†
ğ›¼
ğ‘šğ‘–ğ‘›[âˆ¥ ğ´â‹… ğ‘¥âˆ’ğ‘âˆ¥ +Â ğœ†Â ğ›¼âˆ¥ ğ‘ +Â (1âˆ’ğœ†)Â ğ›¼âˆ¥ ğ‘ ]âˆ¥ 1 âˆ¥ 2",620,semantic
5ac29d0d-5d00-41ef-a4ed-591fa95e9ae4,Regularization and Sparse Models.pdf,CSCI_83,27,"Elastic Net Regularization
Example: Increasing constraint on model coefï¬cients with larger L1 regularization hyperparameter
alphas = np.arange(0 . 0 , 0 . 5 , step = 0 .",169,semantic
09a2b79f-ee79-4892-956a-4172625ed769,Regularization and Sparse Models.pdf,CSCI_83,27,"0 2 )
Betas, MSE_train, MSE_test = regularized_coefs(test_scores_train, test_scores_test, alphas, L1_wt=0 . 5 )",111,semantic
54c54d37-775f-40a7-b9fb-9b5d9af258ba,Regularization and Sparse Models.pdf,CSCI_83,28,"Elastic Net Regularization
Example: Increasing constraint on model coefï¬cients with larger L1 regularization hyperparameter",123,semantic
251b92c6-edcb-4547-8388-9378db386ca7,Regularization and Sparse Models.pdf,CSCI_83,29,"Elastic Net Regularization
Check the model summary for , 
## Intercept                  -3.715924## C(ses)[T.2]                 2.048593## C(ses)[T.3]                 4.615190## C(prog)[T.2]                3.589684## C(prog)[T.3]               -5.103465## C(ses)[T.2]:C(prog)[T.2]    0.455652## C(ses)[T.3]:C(prog)[T.2]    2.310118## C(ses)[T.2]:C(prog)[T.3]    2.149526## C(ses)[T.3]:C(prog)[T.3]    0.000000## dtype: float64
ğœ†=0.5ğ›¼=0.10
lm_elastic = smf.ols(""socst_zero_mean ~ C(ses)*C(prog)"", data=test_scores_train).fit_regularized(alpha=0 . 1 , L1_wt=0 . 5 )
lm_elastic.params",581,semantic
ad580293-7dd8-4a64-9603-769077f3d60c,Regularization and Sparse Models.pdf,CSCI_83,30,"Summary
Over-ï¬t models and regularization
Bias variance trade-off between ï¬t to training data (bias) and generalization error (vaiance)
Prefer minimal or sparse models
L2 regularization is a soft constraint
L1 regularization is a hard constraint
ElasticNet trade-off between L1 and L2",284,semantic
82226b61-73e1-4527-baa9-706cd97a6bd6,Forecasting And Time Series Analysis.pdf,CSCI_83,0,"Forecasting And Time Series Analysis
Steve Elston
11/10/2022",60,semantic
4c29c79d-9de3-48ff-aefb-736d3e9d0240,Forecasting And Time Series Analysis.pdf,CSCI_83,1,"Why Are Time Series Useful? Data are often time-ordered
Estimates 30% of data science problems include time series data
Must use speciï¬c time series models",155,semantic
04cc6bf8-0b7a-47b9-a157-4897886d8b6f,Forecasting And Time Series Analysis.pdf,CSCI_83,2,"Why Are Time Series Useful? â€œItâ€™s tough to make predictions, especially about the future!â€Karl Kristian Steincke, Danish politician, ca 1937
Demand forecasting: Electricity production, Internet bandwidth, Trafï¬c management, Inventory management, sales forecasting
Medicine: Time dependent treatment effects, EKG, EEG
Engineering and Science: Signal analysis, Analysis of physical processes
Capital markets and economics: Seasonal unemployment, Price/return series, Risk analysis",478,semantic
4d44e2f2-ee93-4090-b638-4057f97df43e,Forecasting And Time Series Analysis.pdf,CSCI_83,3,"Why Are Time Series Data Different? Models must account for time series behavior
Most statistical and machine learning assume data samples are independent identically distributed (iid)
But, this is not the case for time series data
Time series values are correlated in time
Time series data exhibit Serial correlation
Serial correlation of values
Serial correlation of errors
Violate iid assumptions of many statistical and ML Models`",434,semantic
9f625620-f5f6-45a7-b1d7-0dd923df92c2,Forecasting And Time Series Analysis.pdf,CSCI_83,4,"Why Are Time Series Data Different
Examples of series correlation:
Temperature forecasts, where the future values are correlated with the current values
The opening price of a stock is correlated with the price at the previous close
The daily sales volume of a product is correlated with the previous sales volume
A medical patientâ€™s blood pressure reading is correlated with the previous observations",401,semantic
190a07c4-7eb3-41a0-92d2-d3c9687114ab,Forecasting And Time Series Analysis.pdf,CSCI_83,5,"History of Time Series Analysis
Time series analysis have a long history- Recognized the serial dependency in time series data early on- Joseph Fourier and Siemon Poisson worked on time series problems in the early 19th Century
Joseph Fourier",242,semantic
553ce4b5-8721-4173-92d4-2d9c6b2d7efb,Forecasting And Time Series Analysis.pdf,CSCI_83,6,"History of Time Series Analysis
Modern history of time series analysis started with George Udny Yule (1927) and Gilbert Walker (1931)
Yule worked on sunspot time series
Walker was attempting to forecast the tropical monsoon cycle
Developed the autoregressive (AR) model to account for serial correlation
The AR model is foundation of modern time series models
George Yule, time series pioneer",392,semantic
4137c46e-a733-4f5d-a42f-80d0fcbc6c38,Forecasting And Time Series Analysis.pdf,CSCI_83,7,"History of Time Series Analysis
Mathematical prodigy, Norbert Weiner, invented ï¬lters for stochastic time series processes, starting in the 1920s
Weinerâ€™s ï¬lter theory is the basis of many time series ï¬lter methods
Predictive ï¬lters for noisy signals; not discussed here
Weiner process model for random walks is widely used
Norbert Weiner: Invented time series ï¬lters",367,semantic
9178ec2a-b4f4-42d4-8cfe-46325b457710,Forecasting And Time Series Analysis.pdf,CSCI_83,8,"History of Time Series Analysis
George Box and Gwilym Jenkins fully developed a statistical theory of time series by extending the work of Yule and Walker in the 1950s and 1960s
Extended the AR model to include moving average (MA) terms
Included the integrative term to create the ARIMA model
The ARIMA model is our focus
George EP Box created general time series model
Seminal time series analysis book",403,semantic
3bba8a53-bc84-4478-9763-14682d10b699,Forecasting And Time Series Analysis.pdf,CSCI_83,9,"History of Time Series Analysis
21st Century time series analysis
Considerable research continues to expand the frontiers
Bayesian time series models
R bsts package and Python PyMC3
Long short term memory (LSTM) model
Hidden Markov Models (HMMs) widely used
Python Scikit Learn HMM or R HMM package",298,semantic
69faa7fa-596e-4de4-ba0f-a8b1be0c163a,Forecasting And Time Series Analysis.pdf,CSCI_83,10,"Software for Time Series Analysis
Most statistical packages have considerable time series modeling capability
R time series analysis packages are wide and deep
Much leading edge research appears ï¬rst in R packages
CRAN Time Series Task View, maintained by Rob Hyndman, contains curated index to R time series packages
Primary Python time series analysis package in Statsmodels.tsa
Bayesian time series models supported in PyMC. Many newer Python time series packages packages, including:
Darts package includes cutting edge methods like hierarcical models
Facebook Kats package - strong in forecasting including the PROFIT model
GrayKite Linkedinâ€™s forecasting package",668,semantic
6ab5fc33-1102-4cdb-85eb-70f47d6789b2,Forecasting And Time Series Analysis.pdf,CSCI_83,11,"Fundamentals of Time Series
What are the fundamental properties of time series
Representation and sampling
White noise series
Stationary time series
Autocorrelation and partial autocorrelation
Random walk series
Trend
Seasonal effects",234,semantic
2b491e92-6467-4269-aa4d-ccbe6ecacd95,Forecasting And Time Series Analysis.pdf,CSCI_83,12,"Time Series Representation
Time series are expressed as a time ordered sequence of values 
We work with discrete samples in time order
In regular time series the sample interval  is ï¬xed
Time measured from start of series 
Or, time measured within an interval, multiples of 
Even continuous time processes are sampled in practice
Temperature
Pressure
Home price
( , , ,â€¦, )ğ‘¥1ğ‘¥2ğ‘¥3 ğ‘¥ğ‘›
Î”ğ‘¡
(0,Î”ğ‘¡,2Î”ğ‘¡,â€¦,ğ‘›Î”ğ‘¡)
Î”ğ‘¡",405,semantic
e3ac92aa-5cb9-4c21-853e-1a0696e79d61,Forecasting And Time Series Analysis.pdf,CSCI_83,13,"White Noise Series
White noise series are fundamental
Values are independent identically distributed (iid) Normally distributed
Can express values, , of a white noise series as:
No serial correlation between values
There is no predictive information in a white noise series
We want the residuals of time series models to be white noise series
( , , ,â€¦, )ğ‘¤1ğ‘¤2ğ‘¤3 ğ‘¤ğ‘›
ğ‘‹(ğ‘¡)=( , , ,â€¦, )ğ‘¤1ğ‘¤2ğ‘¤3 ğ‘¤ğ‘›
ğ‘¤â„ ğ‘’ğ‘Ÿğ‘’âˆ¼ ğ‘(0, )ğ‘¤ğ‘¡ ğœ2",409,semantic
98d0b1c3-eafa-4679-980e-bd490882d838,Forecasting And Time Series Analysis.pdf,CSCI_83,14,"White Noise Series
What does a white noise series look like?",60,semantic
b0999923-1b85-4ad0-919f-ea8a70c2d07f,Forecasting And Time Series Analysis.pdf,CSCI_83,15,"White Noise Series
What does a white noise series look like? Each value is a sample is iid Normally distributed
No trend",120,semantic
064f8a5d-f365-45e0-be4e-e761c5867c00,Forecasting And Time Series Analysis.pdf,CSCI_83,16,"Stationary Time Series
A white noise series is stationary
A stationary time series has statistical properties constant in time
For example, a stationary time series has constant mean and variance over any sample interval
Many time series models require stationarity
Often transform time series to make them stationary
More on this shortly",338,semantic
cc909546-bf53-4936-8e5f-af01eab80d52,Forecasting And Time Series Analysis.pdf,CSCI_83,17,"Stationary Time Series
What tests can be used for stationarity? Plots
Qualitative
Nonstationarity from seasonality and trend are usually visible
The Dicky Fuller test
Hypothesis test for stationarity
Null hypothesis is stationarity
Based on roots of AR(1) model; to be discussed shortly",286,semantic
dd45d81b-e044-4773-8b0e-1a87c2c3eb24,Forecasting And Time Series Analysis.pdf,CSCI_83,18,"Autocorrelation Properties of White Noise Series
Can measure the correlation of a time series with itself
The time series is correlated at different time offsets
Each time step of offset is called a lag
The autocorrelation function (ACF) is measured between the series and the series lagged in time
Always 1 at lag 0; ğ´ğ¶ğ¹(0)=1.0",328,semantic
e7392343-40c4-48e2-a3aa-8ae48b9830a9,Forecasting And Time Series Analysis.pdf,CSCI_83,19,"Autocorrelation Properties of White Noise Series
We compute the autocorrelation at lag :
Where:
Notice that for any series, 
Autocorrelation at each lag has values in the range 
ğ‘˜
= = ( âˆ’ğœ‡)â‹… ( âˆ’ğœ‡)ğœŒğ‘˜ ğ›¾ğ‘˜
ğ‘›ğœ2 1ğ‘›ğœ2âˆ‘ğ‘¡=1
ğ‘
ğ‘¦ğ‘¡ ğ‘¦ğ‘¡âˆ’ğ‘˜
ğ‘˜ğ‘¦ğ‘¡
ğ›¾ğ‘˜
ğœ‡
ğœ2
=ğ‘™ğ‘ğ‘”=ğ‘œğ‘ğ‘ ğ‘’ğ‘Ÿğ‘£ğ‘ğ‘¡ğ‘œğ‘›Â ğ‘ğ‘¡Â ğ‘¡ğ‘–ğ‘šğ‘’Â ğ‘¡=ğ‘ğ‘œğ‘£ğ‘ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘ğ‘’Â ğ‘™ğ‘ğ‘”Â ğ‘˜=ğ‘šğ‘’ğ‘ğ‘›Â ğ‘œğ‘“Â ğ‘¡â„ ğ‘’Â ğ‘ ğ‘’ğ‘Ÿğ‘–ğ‘’ğ‘ 
=ğ‘£ğ‘ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘ğ‘’Â ğ‘œğ‘“Â ğ‘¡â„ ğ‘’Â ğ‘ ğ‘’ğ‘Ÿğ‘–ğ‘’ğ‘ = ( âˆ’ğœ‡)â‹… ( âˆ’ğœ‡)1ğ‘›âˆ’1Î£ğ‘ğ‘¡=1ğ‘¦ğ‘¡ ğ‘¦ğ‘¡
=1ğœŒ0
âˆ’1.0â‰¥ğœŒâ‰¥1.0",368,semantic
c77ef7ad-9691-44d6-b1a5-fecc4e1063e1,Forecasting And Time Series Analysis.pdf,CSCI_83,20,"Autocorrelation Properties of White Noise Series
The partial autocorrelation is another important property of time series
The partial autocorrelation function (PACF) is the residual autocorrelation once autocorrelation is accounted for
To compute the partial autocorrelation to lag :
Compute the autocorrelation to lag 
Remove the linearly predictable autocorrelation component of the time series
Compute the (partial) autocorrelation of the residual to lag 
The 0 lag value of the partial autocorrelation is always 1.0
ğ‘˜
ğ‘˜
ğ‘˜",525,semantic
bebeedc0-6c22-4e52-9425-11548879a84f,Forecasting And Time Series Analysis.pdf,CSCI_83,21,"Autocorrelation Properties of White Noise Series
What are the autocorrelation and partial autocorrelation properties of a white noise series? The autocorrelation and partial autocorrelation are 0 for all 
Autocorrelation plot shows value at each lag selected
ğ‘˜>0",262,semantic
81a50522-7cfb-4344-bbd0-82ecb62c132c,Forecasting And Time Series Analysis.pdf,CSCI_83,22,"Random Walk Time Series
Random walks are a commonly encountered properties of time series
Change in value of random walk series at one time step:
The next value in the random walk is then:
Or, with a little bit of algebra:
-  is the ith innovation-  observarition at time - A random walk is an integrative process; sum or integral of innovations
Note: innovations are referred to by other names:- Shocks in the stochastic process literature- Returns in ï¬nancial analytics
= âˆ’ğ‘¤ğ‘¡ ğ‘¦ğ‘¡ ğ‘¦ğ‘¡âˆ’1
= +ğ‘¦ğ‘¡ ğ‘¦ğ‘¡âˆ’1 ğ‘¦ğ‘¡
= +ğ‘¦ğ‘¡ ğ‘¤ğ‘¡ âˆ‘ğ‘–=0
ğ‘¡âˆ’1
ğ‘¤ğ‘–
ğ‘¤ğ‘–=ğ‘¦ğ‘¡ ğ‘¡",528,semantic
aa93411c-e985-416a-ad02-c095f78a221b,Forecasting And Time Series Analysis.pdf,CSCI_83,23,"Random Walk Time Series
What does a random walk time series look like? Integrating innovations leads to a â€˜drift-likeâ€™ behavior
No actual trend; random walk will eventually change apparent slope
Example with iid Normal innovations:",231,semantic
bccb455a-4593-480d-bd9e-3658bd9f6935,Forecasting And Time Series Analysis.pdf,CSCI_83,24,"Random Walk Time Series
Autocorrelation of white noise series dies slowly
Partial autocorrealtion nonzero at one lag",116,semantic
664e7256-aa06-48fa-82dc-76af1e3f69ec,Forecasting And Time Series Analysis.pdf,CSCI_83,25,"Random Walk Time Series
Random walk series is not Normally distributed",70,semantic
0321a978-b985-4c86-8fbf-9b6fb7089708,Forecasting And Time Series Analysis.pdf,CSCI_83,26,"Random Walk Time Series
Random walk time series are non-stationary
Consider the covariance of a time series at lag :
For a random walk, the increase in covariance is unbounded in time:
Unbounded and time dependent variance make a random walk non-stationary
ğ‘˜
=ğ¶ğ‘œğ‘£(, )ğ›¾ğ‘˜ ğ‘¦ğ‘¡ ğ‘¦ğ‘¡+ğ‘˜
=ğ¶ğ‘œğ‘£(, )=ğ‘¡ â†’âˆÂ ğ‘ğ‘ Â ğ‘¡â†’âˆğ›¾ğ‘˜ ğ‘¦ğ‘¡ ğ‘¦ğ‘¡+ğ‘˜ ğœ2",311,semantic
e3ccbaf4-6bc3-423d-857f-67e49afda706,Forecasting And Time Series Analysis.pdf,CSCI_83,27,"Time Series With Trend
Many real-world time series have a long-term trend
A trend is a long term change in the mean value of the time series
Typically model trend as linear, polynomial, non-parametric splines, etc. PROFIT algorithm uses generalized additive model (GAM)
Consider an example of a white noise series with a linear trend",333,semantic
04d8b074-ed31-4e7c-b3f1-e97acc1ac513,Forecasting And Time Series Analysis.pdf,CSCI_83,28,"Time Series With Trend
Trend models are not just strait lines
Polynomial regression
Piece-wise polynomial regression - e.g. splines
Used in PROFIT algorithm
A generalized additive model
Local polynomial regression - e.g. LOESS
Used in Statsmodels",246,semantic
71674531-70f8-4f73-8f25-399df9cf0ecb,Forecasting And Time Series Analysis.pdf,CSCI_83,29,"Time Series With Trend
Time series with trend are non-stationary
Any time series with trend is non-stationary
Mean and variance are dependent of window used to compute them
The distribution of even a white noise series with trend is non-Normal",243,semantic
3ddfe829-24b2-43e5-863b-68e114721e48,Forecasting And Time Series Analysis.pdf,CSCI_83,30,"Time Series With Trend
ACF and PACF are only properly deï¬ned for stationary series
For non-stationary series, the ACF dies off slowly
Integrative innovations lead to long-term dependency
The PACF dies off quickly with lag
Example: ACF and PACF of the white noise series with trend",280,semantic
b9d403c1-ad6d-4d41-be5a-550378604622,Forecasting And Time Series Analysis.pdf,CSCI_83,31,"Time Series With Seasonal Effects
Many (most?) real-world time series have seasonal effect
A seasonal effect has a measurable effect that occurs periodically
Examples of seasonal events include:
Day of the week
Last day of the month
Month of the year
Annual holiday
Option expiration date
Game day, e.g. Supper Bowl
Electrical impulses in a heart - EKG
Orbits of planets
Time series with seasonal effects are non-stationary
Mean and variance depends of sample window
â€¦",468,semantic
e5f22abf-a99e-4f5f-bebb-41e451be51c6,Forecasting And Time Series Analysis.pdf,CSCI_83,32,"Time Series With Seasonal Effects
Use regression models for seasonal effects
Simple regression model:
Coefï¬cient for each interval in period; e.g. 12 coefï¬cients for monthly effects
Coefï¬cient for speciï¬c effect - e.g. date of holiday
Basis function regression
PROFIT algorithm uses Fourier basis functions
A generalized additive model",335,semantic
746c9ce3-1e11-4d8e-8111-ddabe2186161,Forecasting And Time Series Analysis.pdf,CSCI_83,33,"Time Series With Seasonal Effects
Example of a time series with a seasonal effect
A white noise series with trend and seasonal behavior
The seasonal behavior is periodic",169,semantic
38143d92-726f-45b0-afb7-77e6d7cfbb21,Forecasting And Time Series Analysis.pdf,CSCI_83,34,"Time Series Models
Many types of time series models; we will only consider the most widely used
Time series decomposition
Exponential models
ARIMA model
Serially correlated components
Integrative component",205,semantic
565ee5a2-4b80-4b6e-9f49-e2cb2d26d4ba,Forecasting And Time Series Analysis.pdf,CSCI_83,35,"Time Series Decomposition
Two possible models for seasonal effects
Goal, decompose the time series into its components
The Seasonal Trend decomonsition model using Loess (STL) model
Uses a nonparametric regression model to decompose time series into components
Components are seasonal (S), trend (T), and the residual (R)
Additive decomposition model
Multiplicative decomposition model
Differencing model",404,semantic
788ad550-08cf-4c32-b333-b959d82b8f80,Forecasting And Time Series Analysis.pdf,CSCI_83,36,"Time Series Decomposition
The additive decomposition model is expressed as as the sum of the components:
Used when seasonal effect is constant in time
Examples: Physical process
ğ‘‡ğ‘†(ğ‘¡)=ğ‘†(ğ‘¡)+ğ‘‡(ğ‘¡)+ğ‘…(ğ‘¡)",198,semantic
e4514a58-f68e-4dad-946d-eb3ec6bfd857,Forecasting And Time Series Analysis.pdf,CSCI_83,37,"Time Series Decomposition
The Multiplicative decomposition model is expressed as as the product of the components:
The multiplicative form is can be hard to work with, so log transform to additive model
Use when seasonal effect changes in time
Example, economic time series
ğ‘‡ğ‘†(ğ‘¡)=ğ‘†(ğ‘¡)âˆ— ğ‘‡(ğ‘¡)âˆ— ğ‘…(ğ‘¡)
ğ‘™ğ‘œğ‘”(ğ‘‡ğ‘†(ğ‘¡))=ğ‘™ğ‘œğ‘”(ğ‘†(ğ‘¡))+ğ‘™ğ‘œğ‘”(ğ‘‡(ğ‘¡))+ğ‘™ğ‘œğ‘”(ğ‘…(ğ‘¡))
= (ğ‘¡)+ (ğ‘¡)+ (ğ‘¡)ğ‘†ğ‘™ ğ‘‡ğ‘™ ğ‘…ğ‘™",361,semantic
ab091e80-4a2c-4af7-84da-d69370b6ab2f,Forecasting And Time Series Analysis.pdf,CSCI_83,38,"Time Series Decomposition
Example of addative STL decomposition of time series with linear trend and seasonal effect
The original series plot is on top
Notice the estimated trend is not a straight line; a result of noise
Residuals are relatively small and homoscedastic",269,semantic
9537618d-73ba-4b47-ba53-60a371425dfc,Forecasting And Time Series Analysis.pdf,CSCI_83,39,"Time Series Difference Operators
Is there an alternative for dealing with trend? How do we deal with random walks? Difference operators are useful for both cases
Difference operators return the innovations
Difference operators can be of any order in principle
Typically use ï¬rst order differences
Difference operator of order n computes a series n shorter than original
âˆ‡ = âˆ’ğ‘¦ğ‘¡ ğ‘¦ğ‘¡ ğ‘¦ğ‘¡âˆ’1",385,semantic
91053a1f-07a2-42bf-be64-6d7d44393eae,Forecasting And Time Series Analysis.pdf,CSCI_83,40,"Time Series Difference Operators
Example of a ï¬rst order difference operator applied to random walk
The innovations look random
Need to verify statistical properties",165,semantic
0ef0f61f-a178-4283-8fcc-1821801fcb91,Forecasting And Time Series Analysis.pdf,CSCI_83,41,"Time Series Difference Operators
Statistical properties of the difference series
Compute the ACF and PACF
The plots indicate the difference series is white noise",161,semantic
19d30777-079c-494e-bcc2-4d077f7b7055,Forecasting And Time Series Analysis.pdf,CSCI_83,42,"Time Series Forecasting Models
Forecasting is the goal of much of time series analysis
Exponential models; extrapolation from simple smoothers
ARIMA and SARIMAX models; time series linear models
For comprehensive introduction see Forecasting: Principles and Practice, Hyndman and Athanaosopoulos, 3rd edition, 2018, available as book or free online
Rob Hyndmanâ€™s blog is a source of many interesting ideas and example in time series analysis",441,semantic
b8aed954-6b6d-41bf-933c-07888a036d10,Forecasting And Time Series Analysis.pdf,CSCI_83,43,"Exponential Smoothing Models
Exponential smoothing models are simple and widely used
Consider the simple ï¬rst order model
Set initial conditions:
The smoothed update is:
And, the smoothing coefï¬cient is, 
But, model only works if no trend
=ğ‘ 0 ğ‘¦0
=ğ›¼ +(1âˆ’ğ›¼)ğ‘ ğ‘¡ ğ‘¦ğ‘¡ ğ‘ ğ‘¡âˆ’1
= ğ›¼( âˆ’ ),ğ‘ ğ‘¡âˆ’1 ğ‘¦ğ‘¡ ğ‘ ğ‘¡âˆ’1
ğ‘¡>0
0â‰¤ğ›¼â‰¤1",297,semantic
96b4162e-c1e6-4f7c-a9bf-ba06e9fee8d9,Forecasting And Time Series Analysis.pdf,CSCI_83,44,"Exponential Smoothing Models
Decay and exponential smoothing
We can understand the smoothing parameter  in terms of a decay constant, 
An innovation or shock has an effect for all future time
Effect decays exponentially with time, 
ğ›¼ ğœ
ğ›¼=1âˆ’ğ‘’( )Î”ğ‘‡ğœ
Î”ğ‘‡",250,semantic
3f1e8291-5ca6-41f2-8669-0107b46f2640,Forecasting And Time Series Analysis.pdf,CSCI_83,45,"Exponential Smoothing Models
Can extend exponential smoothing model to accommodate trend
Algorithm known as double exponential smoothing or Holt-Winters double exponential smoothing
Update smoothed values and slope at each time step
Start with initial values
Update relationships for both smoothed value and slope
Additional slope smoothing hyperparameter, 
Use third order update includes seasonality in Holt-Winters smoother
=ğ‘ 1 ğ‘¦1
= âˆ’ğ‘1 ğ‘¦2 ğ‘¦1
=ğ›¼ +(1âˆ’ğ›¼)( + )ğ‘ ğ‘¡ ğ‘¦ğ‘¡ ğ‘ ğ‘¡âˆ’1 ğ‘ğ‘¡âˆ’1
=ğ›½( âˆ’ )+(1âˆ’ğ›½)ğ‘ğ‘¡ ğ‘ ğ‘¡ ğ‘ ğ‘¡âˆ’1 ğ‘ğ‘¡âˆ’1
0â‰¤ğ›½â‰¤1",510,semantic
c430da55-866f-47f2-88a2-ad879ea83455,Forecasting And Time Series Analysis.pdf,CSCI_83,46,"Exponential Smoothing Models
Exponential smoothing models are useful for forecasting
Forecast dependent on the choice of smoothing parameters
Can forecast with ï¬rst, second, third order models
For second order model (with trend) the forecast  steps ahead is:
Third order update include seasonal terms
Holt-Winters smoother is a linear model! ğ‘š
= +ğ‘šğ¹ğ‘¡+ğ‘š ğ‘ ğ‘¡ ğ‘ğ‘¡",358,semantic
475a4067-9c8b-4b92-a0c3-943c9cbd9c94,Forecasting And Time Series Analysis.pdf,CSCI_83,47,"Exponential Smoothing Models
Example of smoothing trend plus white noise series
Decreasing the smoothing parameter, , increases smoothing
Additionally, smooth trend
Additional examples in Statsmodels user documentation
ğ›¼",220,semantic
665a5cbd-927e-4167-873a-29a6c4d97a3b,Forecasting And Time Series Analysis.pdf,CSCI_83,48,"The ARIMA and SARIMAX Model
The ARIMA model is composed three components:
Autoregressive component (AR) accounts for partial autocorrelation
Serial correlation of observatons
Integrative component (I) accounts random walks and trend
Moving Average (MA) accounts for autocorrelation
Serial correlation of model error
SARIMAX model adds:
Seasonal components (S)
Exogenous variables (X)",383,semantic
0b6f39b3-dcc8-4000-8682-cbdda7f022c4,Forecasting And Time Series Analysis.pdf,CSCI_83,49,"The Autoregressive Model
Autoregressive model relates past observed values to the current value
An autoregressive model of order , , uses the last p observations:
An AR process has the following properties:
 always
Number of nonzero PACF values 
A shock at any time will affect the result as 
AR model assume stationary time series
ğ‘ğ´ğ‘…(ğ‘)
= + ,â€¦, +ğ‘¥ğ‘¡ ğœ™1ğ‘¦ğ‘¡âˆ’1 ğœ™2ğ‘¦ğ‘¡âˆ’2 ğœ™ğ‘ğ‘¦ğ‘¡âˆ’ğ‘ ğ‘¤ğ‘¡
ğ‘¤â„ ğ‘’ğ‘Ÿğ‘’ğœ™ğ‘˜
ğ‘¤ğ‘¡
ğ‘¦ğ‘¡
=ğ‘šğ‘œğ‘‘ğ‘’ğ‘™Â ğ‘ğ‘œğ‘’ğ‘“ğ‘“ğ‘–ğ‘ğ‘–ğ‘’ğ‘›ğ‘¡Â ğ‘ğ‘¡Â ğ‘™ğ‘ğ‘”Â ğ‘˜=ğ‘¤â„ ğ‘–ğ‘¡ğ‘’Â ğ‘›ğ‘œğ‘–ğ‘ ğ‘’Â ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘ŸÂ ğ‘ğ‘¡Â ğ‘¡ğ‘–ğ‘šğ‘’Â ğ‘¡;Â âˆ¼ ğ™½(0, )ğœ2
=ğ‘œğ‘ğ‘ ğ‘’ğ‘Ÿğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›Â ğ‘ğ‘¡Â ğ‘¡ğ‘–ğ‘šğ‘’Â ğ‘¡
=1ğœŒ0
=ğ‘ğ‘˜ ğœ™ğ‘˜
=ğ‘
ğ‘¡â†’âˆ",500,semantic
8d888cb4-6b38-4080-b542-3a1944d07ace,Forecasting And Time Series Analysis.pdf,CSCI_83,50,"The Autoregressive Model
How can we understand the AR model? Consider an AR(2) model
The value of  is a weighted sum of  previous values plus an error term
Illustration of the AR(2) model
ğ‘¦ğ‘¡ ğ‘˜",192,semantic
332d8ed5-0ce3-4b23-9c5a-0e25bdab7e6b,Forecasting And Time Series Analysis.pdf,CSCI_83,51,"The Autoregressive Model
How can we understand the AR model? Model matrix of AR(2) model
AR model is a linear model! For coefï¬cient vector, , solve linear system:
ğ´=
â¡
â£
â¢â¢â¢â¢â¢â¢â¢â¢â¢
,Â  ,Â ğ‘¦ğ‘¡ ğ‘¦1âˆ’1 ğ‘¦ğ‘¡âˆ’2
,Â  ,Â ğ‘¦ğ‘¡âˆ’1 ğ‘¦1âˆ’2 ğ‘¦ğ‘¡âˆ’3
,Â  ,Â ğ‘¦ğ‘¡âˆ’2 ğ‘¦1âˆ’3 ğ‘¦ğ‘¡âˆ’4
â‹® ,Â Â Â Â Â â‹® ,Â Â Â Â Â â‹® ,Â Â Â Â  ,Â Â Â Â ğ‘¦2 ğ‘¦1 ğ‘¦0
,Â Â Â Â  Â Â Â 0ğ‘¦1 ğ‘¦0
,Â Â Â Â 0,Â Â Â Â 0ğ‘¦0
â¤
â¦
â¥â¥â¥â¥â¥â¥â¥â¥â¥
Î¦=[ , ,â€¦, ]ğœ™1ğœ™2 ğœ™ğ‘
ğ‘Œ=ğ´Î¦",344,semantic
3c89feb7-a267-4c24-9673-5400c5d83646,Forecasting And Time Series Analysis.pdf,CSCI_83,52,"The Autoregressive Model
We can rewrite the AR(1) model in terms of exceptions:
The AR model is unstable for the roots of the polynomial 
To be a stable AR process, 
Violation of this condition leads to unstable model! ğ”¼()= ğ”¼( )+ğ”¼()ğ‘¦ğ‘¡ ğœ™ğ‘¡ ğ‘¦ğ‘¡âˆ’1 ğœ–ğ‘¡ğ‘œğ‘Ÿğœ‡=ğ‘+ğœ‡+0ğœ™ğ‘¡
ğ‘¡â„ ğ‘’ğ‘Ÿğ‘’ğ‘“ğ‘œğ‘Ÿğ‘’
ğœ‡= ğ‘1âˆ’ğœ™2ğ‘¡
1âˆ’ğœ™2ğ‘¡
â‰¤1ğœ™2ğ‘¡",289,semantic
77403a5e-29db-4b64-a8a9-5c30ea0ca7a0,Forecasting And Time Series Analysis.pdf,CSCI_83,53,"The Autoregressive Model
Example of AR(2) time series with coefï¬cients :
Time series looks a bit random
But, notice the statistical properties; ACF, PACF
PACF has 2 non-zero lag values, so 
=(1.0,0.75,0.25)
ğ‘=2",210,semantic
53de7b5b-6387-4657-864e-244fd0f4179e,Forecasting And Time Series Analysis.pdf,CSCI_83,55,"The Autoregressive Model
Example model summary for AR(2) model:
Both AR coefï¬cients are statistically signiï¬cant
Variance term is statistically signiï¬cant",154,semantic
b89ba94c-2b08-46ca-bbba-2703142c4522,Forecasting And Time Series Analysis.pdf,CSCI_83,56,"The Moving Average Model
A moving average model of order , , uses the last q error terms or shocks:
An MA process has the following properties:
For autocorrelation,  always
Number of , , 
Shocks die off quickly in MA processes
MA model assumes stationary time series
ğ‘ğ‘€ğ´(ğ‘)
=ğœ‡+ + + +â€¦+ğ‘¦ğ‘¡ ğœ–ğ‘¡ ğœƒ1ğœ–ğ‘¡âˆ’1 ğœƒ2ğœ–ğ‘¡âˆ’2 ğœƒğ‘ğœ–ğ‘¡âˆ’ğ‘
ğ‘¤â„ ğ‘’ğ‘Ÿğ‘’ğœƒğ‘˜
ğ‘¦ğ‘¡
ğœ–ğ‘¡
=ğ‘šğ‘œğ‘‘ğ‘’ğ‘™Â ğ‘ğ‘œğ‘’ğ‘“ğ‘“ğ‘–ğ‘ğ‘–ğ‘’ğ‘›ğ‘¡Â ğ‘ğ‘¡Â ğ‘™ğ‘ğ‘”Â ğ‘˜=ğ‘œğ‘ğ‘ ğ‘’ğ‘Ÿğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›Â ğ‘ğ‘¡Â ğ‘¡ğ‘–ğ‘šğ‘’Â ğ‘¡=ğ‘–ğ‘›ğ‘›ğ‘œğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›Â ğ‘œğ‘ŸÂ ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘ŸÂ ğ‘ğ‘¡Â ğ‘¡ğ‘–ğ‘šğ‘’Â ğ‘¡;Â âˆ¼ ğ™½(0, )ğœ2
=1ğœŒ0
â‰ 0ğœŒğ‘˜ =ğ‘ğ‘˜>0",434,semantic
5ebc8783-500a-4fd3-91c6-e7647de5d904,Forecasting And Time Series Analysis.pdf,CSCI_83,57,"The Moving Average Model
How can we understand the MA model? Model matrix of MA(2) model
MA model is a linear model! But, the value of  dependents on 
The s are unobservable!",174,semantic
5255ce4d-5c22-47dc-946c-3278137b3a7a,Forecasting And Time Series Analysis.pdf,CSCI_83,57,"So, ï¬tting requires nonlinear iteratively rewieighted least squares
ğ´=
â¡
â£
â¢â¢â¢â¢â¢â¢â¢â¢â¢
,Â  ,Â ğ‘¦ğ‘¡ ğœ–1âˆ’1 ğœ–ğ‘¡âˆ’2
,Â  ,Â ğ‘¦ğ‘¡âˆ’1 ğœ–1âˆ’2 ğœ–ğ‘¡âˆ’3
,Â  ,Â ğ‘¦ğ‘¡âˆ’2 ğœ–1âˆ’3 ğœ–ğ‘¡âˆ’4
â‹® ,Â Â Â Â Â â‹® ,Â Â Â Â Â â‹® ,Â Â Â Â  ,Â Â Â Â ğ‘¦2 ğœ–1 ğœ–0
,Â Â Â Â  Â Â Â 0ğ‘¦1 ğœ–0
,Â Â Â Â 0,Â Â Â Â 0ğ‘¦0
â¤
â¦
â¥â¥â¥â¥â¥â¥â¥â¥â¥
ğœ–ğ‘¡ [ , ,..., ]ğœ–ğ‘¡âˆ’1ğœ–ğ‘¡âˆ’2 ğœ–ğ‘¡âˆ’ğ‘
ğœ–ğ‘˜",256,semantic
14d884df-98f0-48b3-9171-3740aecf2986,Forecasting And Time Series Analysis.pdf,CSCI_83,58,"The Moving Average Model
Example of an MA(1) model with coefï¬cients 
The time series looks fairly random
The ACF has 1 statistically signiï¬cant nonzero lag value
(1,âˆ’0.75)",171,semantic
1d6d5b84-51da-4eb1-82e5-26616148afdc,Forecasting And Time Series Analysis.pdf,CSCI_83,60,"The Moving Average Model
Example model summary for MA(1) model:
The MA coefï¬cient is statistically signiï¬cant
Notice that true value is within the conï¬dence interval
Conï¬dence interval is wide",192,semantic
aeb10f31-bf45-47c8-87d6-fc0d2c217997,Forecasting And Time Series Analysis.pdf,CSCI_83,61,"Autoregressive Moving Average Model
We can combine AR and MA terms to create the autoregressive moving average (ARMA) model of order :
Fit ARMA model by solving a nonlinear equatioin:
Can write as polynomial equation in terms of coefï¬cient vectors , :
Model is linear in coefï¬cients
ARMA model assumes stationary time series
(ğ‘,ğ‘)
= + ,â€¦,+ + + + +â€¦+ğ‘¦ğ‘¡ ğœ™1ğ‘¦ğ‘¡âˆ’1 ğœ™2ğ‘¦ğ‘¡âˆ’2 ğœ™ğ‘ğ‘¦ğ‘¡âˆ’ğ‘ ğœ–ğ‘¡ ğœƒ1ğœ–ğ‘¡âˆ’1 ğœƒ2ğœ–ğ‘¡âˆ’2 ğœƒğ‘ğœ–ğ‘¡âˆ’ğ‘
âˆ’ âˆ’ ,â€¦,âˆ’ = + + +â€¦+ğ‘¦ğ‘¡ ğœ™1ğ‘¦ğ‘¡âˆ’1 ğœ™2ğ‘¦ğ‘¡âˆ’2 ğœ™ğ‘ğ‘¦ğ‘¡âˆ’ğ‘ ğœ–ğ‘¡ ğœƒ1ğœ–ğ‘¡âˆ’1 ğœƒ2ğœ–ğ‘¡âˆ’2 ğœƒğ‘ğœ–ğ‘¡âˆ’ğ‘
Î¦=[1, , ,â€¦, ğ‘¦]ğœ™1ğœ™2 ğœ™ğ‘ Î˜=[1, , ,â€¦, ]ğœƒ1ğœƒ2 ğœƒğ‘
(1âˆ’Î¦)ğ‘Œ=Î˜ğœ–",515,semantic
ed17bc92-df3d-4d13-8baf-3ee8228eab24,Forecasting And Time Series Analysis.pdf,CSCI_83,62,"The ARIMA Model
The integrative model addresses certain non-stationary components of a time series
Random walks
Trends
Based on difference operator
Typically ï¬rst order difference
Can be higher order
Is deterministic, no model coefï¬cient to estimate",249,semantic
595549c1-716d-4d28-a9ae-7c834b61fce7,Forecasting And Time Series Analysis.pdf,CSCI_83,63,"The ARIMA Model
The autoregressive integrative moving average (ARIMA) model includes AR, integrative and MA terms
The order of an ARIMA is speciï¬ed as (p,d,q)
p is the AR order
d is the order of differencing
q is the MA order
The integrative term helps transforms trend and random walks to stationary process
Does not account for seasonal effect
For difference values, , formulate as:âˆ‡ ğ‘¦ğ‘¡
(1âˆ’Î¦)âˆ‡ ğ‘Œ=Î˜ğœ–",400,semantic
81a33db1-5ee9-4832-be84-7bd266a1d7a4,Forecasting And Time Series Analysis.pdf,CSCI_83,64,"Seasonal Models
Several possible seasonal models
Seasonal effects can be periodic or single event (e.g. holiday, game day, etc.)
Linear regression model to ï¬nd effect for each time step in period
STL decomposition
SARIMAX, the S term
Each model requires:
Known period of the cycle or time of seasonal event
Additive or logarithmic transformation",345,semantic
37036c31-96d8-476c-aa92-1f32d2a65ac4,Forecasting And Time Series Analysis.pdf,CSCI_83,65,"SARIMAX Model
The SARIMAX model adds seasonal and exogenous terms
ARIMA terms are same, (p,d,q)
Seasonal terms:
ARIMA model, order (P,D,Q,S)
Must specify period, S
Order of SARIMAX model is speciï¬ed as (p,d,q)(P,D,Q,S)
See Statsmodels State Space User Guide for more details and examples",287,semantic
b173f862-03f3-4c36-9042-cacb1179264d,Forecasting And Time Series Analysis.pdf,CSCI_83,66,"SARIMAX Model
The SARIMAX model (with no exogenous variables) is formulated
 and  are the AR polynomials non-seasonal and seasonal terms
 and  are the MA polynomials non-seasonal and seasonal terms
 and  are the non-seasonal and seasonal differencing operators
 is the trend term
(ğ‘Œ) ( ) =ğ´(ğ‘¡)+(ğ‘Œ) ( )ğœ™ğ‘ ğœ™ÌƒÂ ğ‘ƒğ‘Œâˆ— âˆ‡ ğ‘‘âˆ‡ ğ·ğ‘¦ğ‘¡ ğœƒğ‘ ğœƒÌƒÂ ğ‘„ğ‘Œâˆ— 
(ğ‘Œ)ğœ™ğ‘ ( )ğœ™ÌƒÂ ğ‘ƒğ‘Œâˆ— 
(ğ‘Œ)ğœƒğ‘ ( )ğœƒÌƒÂ ğ‘„ğ‘Œâˆ— 
âˆ‡ ğ‘‘ âˆ‡ ğ·
ğ´(ğ‘¡)",377,semantic
8bd97dfa-0e27-4448-ab5d-182347eb69e3,Forecasting And Time Series Analysis.pdf,CSCI_83,67,"SARIMAX Model
SARIMAX model can include exogenous variables, , leading to a new system of equationsâ€:
Time series model for latent variable, 
 acts as the intercept term for the regression model for 
The coefï¬cient vector, , contains the effect sizes for the exogenous variables
ğ‘¥
= +ğ‘¦ğ‘¡ ğ›½ğ‘¡ğ‘¥ğ‘¡ ğœ‡ğ‘¡
(ğ‘Œ) ( ) =ğ´(ğ‘¡)+(ğ‘Œ) ( )ğœ™ğ‘ ğœ™ÌƒÂ ğ‘ƒğ‘Œâˆ— âˆ‡ ğ‘‘âˆ‡ ğ·ğœ‡ğ‘¡ ğœƒğ‘ ğœƒÌƒÂ ğ‘ƒğ‘Œâˆ— 
ğœ‡ğ‘¡
ğœ‡ğ‘¡ ğ‘¥ğ‘¡
ğ›½",356,semantic
d808d2e3-58ec-4e10-a625-226c1a923be8,Forecasting And Time Series Analysis.pdf,CSCI_83,68,"Evaluating and Comparing Time Series Models
How can we evaluate time series models? RMSE; compare forecast to actual values
Could use log-likelihood; 
Use score function 
But, score will decrease with model complexity
Need to adjust for number of model parameters
We always prefer simpler models; fewer parameters to learn
Akaki Information Criteria (AIC)
Bayes Information Criteria (BIC)
ğ‘™ğ‘œğ‘”(ğ‘(ğ‘‹|ğœƒ))
ğœƒ=ğ‘šğ‘œğ‘‘ğ‘’ğ‘™Â ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ 
=âˆ’2Â ğ‘™ğ‘œğ‘”(ğ‘™ğ‘–ğ‘˜ğ‘’ğ‘™ğ‘–â„ ğ‘œğ‘œğ‘‘)=âˆ’2Â ğ‘™ğ‘œğ‘”(ğ‘(ğ‘‹|ğœƒ))",455,semantic
ba2e5886-5bd5-4f33-a063-7f6679d434e1,Forecasting And Time Series Analysis.pdf,CSCI_83,69,"Evaluating and Comparing Time Series Models
Akaki Information criteria, AIC
AIC penalizes the score function for the complexity of the model by 
Model with lowest AIC is best
ğ´ğ¼ğ¶=2Â ğ‘˜âˆ’2Â ğ‘™ğ‘›()ğ¿Ì‚Â ğ‘¤â„ ğ‘’ğ‘Ÿğ‘’=ğ‘¡â„ ğ‘’Â ğ‘™ğ‘–ğ‘˜ğ‘’ğ‘™ğ‘–â„ ğ‘œğ‘œğ‘‘Â ğ‘”ğ‘–ğ‘£ğ‘’ğ‘›Â ğ‘¡â„ ğ‘’Â ğ‘“ğ‘–ğ‘¡ğ‘¡ğ‘’ğ‘‘Â ğ‘šğ‘œğ‘‘ğ‘’ğ‘™Â ğ‘ğ‘ğ‘Ÿğ‘šğ‘ğ‘¡ğ‘’ğ‘Ÿğ‘ Â =ğ‘(ğ‘¥| )ğ¿Ì‚Â  ğœƒÌ‚Â  ğœƒÌ‚Â ğ‘¥=ğ‘œğ‘ğ‘ ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘‘Â ğ‘‘ğ‘ğ‘¡ğ‘ğ‘˜=ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘ŸÂ ğ‘œğ‘“Â ğ‘šğ‘œğ‘‘ğ‘’ğ‘™Â ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ 
2Â ğ‘˜",315,semantic
411ef8ae-49c9-4d94-8729-8f1c80feecd4,Forecasting And Time Series Analysis.pdf,CSCI_83,70,"Evaluating and Comparing Time Series Models
Bayes Information criteria, BIC
BIC penalizes the score function for the complexity of the model, 
BIC adjusts for number of samples used to learn the  model parameters
Model with lowest BIC is best
BIC is often preferred to AIC for time series models
ğµğ¼ğ¶=ğ‘™ğ‘›(ğ‘›)Â ğ‘˜âˆ’2Â ğ‘™ğ‘›()ğ¿Ì‚Â ğ‘¤â„ ğ‘’ğ‘Ÿğ‘’=ğ‘¡â„ ğ‘’Â ğ‘™ğ‘–ğ‘˜ğ‘’ğ‘™ğ‘–â„ ğ‘œğ‘œğ‘‘Â ğ‘”ğ‘–ğ‘£ğ‘’ğ‘›Â ğ‘¡â„ ğ‘’Â ğ‘“ğ‘–ğ‘¡ğ‘¡ğ‘’ğ‘‘Â ğ‘šğ‘œğ‘‘ğ‘’ğ‘™Â ğ‘ğ‘ğ‘Ÿğ‘šğ‘ğ‘¡ğ‘’ğ‘Ÿğ‘ Â =ğ‘(ğ‘¥| )ğ¿Ì‚Â  ğœƒÌ‚Â  ğœƒÌ‚Â ğ‘¥=ğ‘œğ‘ğ‘ ğ‘’ğ‘Ÿğ‘£ğ‘’ğ‘‘Â ğ‘‘ğ‘ğ‘¡ğ‘ğ‘˜=ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘ŸÂ ğ‘œğ‘“Â ğ‘šğ‘œğ‘‘ğ‘’ğ‘™Â ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ ğ‘›=ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘ŸÂ ğ‘œğ‘“Â ğ‘œğ‘ğ‘ ğ‘’ğ‘Ÿğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ 
ğ‘˜
ğ‘˜",464,semantic
42529490-565c-4c61-ad5c-5a094455ec67,Forecasting And Time Series Analysis.pdf,CSCI_83,71,"Evaluating and Comparing Time Series Models
Can compare and select models using BIC or AIC
Backwards step-wise model selection
1. Start with initial order of the model; e.g. 2.",176,semantic
0e5e5c1f-7c14-403c-ba24-5fe2d9c72b5a,Forecasting And Time Series Analysis.pdf,CSCI_83,71,"Fit (learn) the model parameters
3. compute the BIC, and if reduced consider this a better model
4. Reduce the order of one of the model components
5. Repeat steps 2, 3 and 4 until no further improvement
Tips for comparing models:
BIC and AIC are approximations; small changes (3rd or 4th decimal) are not important
If close tie for best model pick the simpler (lower order) case
Often best to consider integrative terms,  and , separately
(ğ‘,ğ‘‘,ğ‘)(ğ‘ƒ,ğ·,ğ‘„,ğ‘†)
ğ‘‘ ğ·",460,semantic
86709bea-b4f8-4c84-9c64-d694a611d5a8,Forecasting And Time Series Analysis.pdf,CSCI_83,72,"SARIMAX Example
Example: 3 time series of Australian production",63,semantic
5b3ce8eb-2299-464f-9bc9-1d69de1cd880,Forecasting And Time Series Analysis.pdf,CSCI_83,74,"SARIMAX Example
Use the SARIMAX model to ï¬nd the best ARIMA ï¬t of log(electric production)
Log_electric = CBE.elec_log[:'1989-12-31']
best_model = pm.auto_arima(Log_electric, start_p=1 , start_q=1 ,                             max_p=3 , max_q=3 , m=1 2 ,                             start_P=0 , seasonal=True,
                             d=1 , D=1 , trace=True,                             information_criterion = 'bic',                             error_action='ignore',  # don't want to know if an order does not work                             suppress_warnings=True,  # don't want convergence warnings
                             stepwise=True)  # set to stepwise",670,semantic
a1a941aa-b79a-44d0-9b71-f48045c46e09,Forecasting And Time Series Analysis.pdf,CSCI_83,75,"SARIMAX Example
Example of SARIMAX model of order (0.1.1)(0,1,2,12) for monthly electric production series
Model selected by backwards step-wise method
First order model integrative term and MA(1)
First order model integrative term and MA(1) for period 12 seasonality",267,semantic
6e0464a1-14eb-4538-ba3c-8159b7b731ed,Forecasting And Time Series Analysis.pdf,CSCI_83,76,"SARIMAX Example
Predictions for the last 12 months of the time series
prediction = pd.Series(best_model.predict(n_periods=1 2 ), 
                       index = pd.date_range(start = '1990-01-31', end = '1990-12-31', freq = 'M'))",229,semantic
fbea2386-4bc2-45de-9e36-a12b851ee468,Forecasting And Time Series Analysis.pdf,CSCI_83,78,"SARIMAX Example
Residuals of the predictions
residuals = CBE.elec_log['1990-01-31':] - prediction
fig, ax = plt.subplots(nrows=1 , ncols=1 , figsize=(4 , 4 ))_=ss.probplot(residuals, plot = ax);
plt.show()",205,semantic
8c5a4dc2-04a2-4b03-911c-0c5a07f57e1e,Forecasting And Time Series Analysis.pdf,CSCI_83,79,"Summary
Fundamental elements of time series
Fundamental components which cannot be predicted
White noise
Random walks
Autocorrelation and partial autocorrelation
Trend
Seasonal components
Stationarity properties; Dicky Fuller test",230,semantic
d58bb260-881d-4b75-9a07-e4c15c7e635f,Forecasting And Time Series Analysis.pdf,CSCI_83,80,"Summary
Time series models must account for serial correlation
Exponential models; e.g. Holt-Winters
Second order accounts for trend
Third order accounts for trend and seasonal",176,semantic
a585324c-308c-4ad3-93f4-8e656c5a4e78,Forecasting And Time Series Analysis.pdf,CSCI_83,81,"Summary
Time series models must account for serial correlation
e.g. ARIMA and SARIMAX
AR components for serial correlation of values
MA components for serial correlation of errors
Integrative components for random walk and trend, I
Seasonal, (P,D,Q,S)
Exogenous variables, X",274,semantic
2a755141-dcd7-4a82-b637-b43713c767a1,Forecasting And Time Series Analysis.pdf,CSCI_83,82,"Summary
Evaluation and model comparison
RMSE
AIC and BIC, penalize score function for model complexity
Use BIC (or AIC) to perform backwards step-wise model selection",166,semantic
c05f547a-9b98-4b01-b736-261970be2266,Sampling and Simulation.pdf,CSCI_83,0,"Sampling and Simulation
Steve Elston
09/22/2022",47,semantic
f9edb31a-3384-4c7c-bc8b-61b6126ab85b,Sampling and Simulation.pdf,CSCI_83,1,"Review
Axioms of probability; for discrete distribution
Expectation
0â‰¤ğ‘ƒ(ğ´)â‰¤1
ğ‘ƒ(ğ‘†)= ğ‘ƒ()=1âˆ‘âˆˆ ğ´ğ‘ğ‘–
ğ‘ğ‘–
ğ‘ƒ(ğ´Â âˆª ğµ)=ğ‘ƒ(ğ´)+ğ‘ƒ(ğµ)ğ‘–ğ‘“Â ğ´âŠ¥ ğµ
E[ğ—]= Â ğ‘()âˆ‘ğ‘–=1
ğ‘›
ğ‘¥ğ‘– ğ‘¥ğ‘–",146,semantic
3ec5f980-cbd9-48e8-8221-ef8be08832f0,Sampling and Simulation.pdf,CSCI_83,2,"Review
The Categorical distribution - Discrete multi-variate distribution
For outcome  we one hot encode the results as:
For a single trial the probabilities of the  possible outcomes are expressed:
Probability mass function as:
Multivariate Normal distribution, parameterized by n-dimensional vector of locations,  and  x  dimensional covariance matrix
ğ‘–
=(0,0,â€¦,1,â€¦,0)ğğ¢
ğ‘˜
Î =( , ,â€¦, )ğœ‹1ğœ‹2 ğœ‹ğ‘˜
ğ‘“(|Î )=ğ‘¥ğ‘– ğœ‹ğ‘–
ğœ‡âƒ—Â  ğ‘› ğ‘›
ğ‘“()= ğ‘’ğ‘¥ğ‘( (âˆ’ ğšº(âˆ’))ğ±âƒ—Â  1(2ğœ‹|ğšº|)ğ‘˜âˆš 12ğ±âƒ—Â ğœ‡âƒ—Â )ğ‘‡ ğ±âƒ—Â ğœ‡âƒ—",463,semantic
174be24a-d82d-4cb8-a9f5-dfbd55e22e3a,Sampling and Simulation.pdf,CSCI_83,3,"Review
Conditional probability
One random variable depends on another
But not commutable
Mutually exclusivity
Independence
ğ‘ƒ(ğ´|ğµ)=ğ‘ƒ(ğ´)â‡ğ‘ƒ(ğµ|ğ´)=ğ‘ƒ(ğµ)
ğ‘ƒ(ğ´|ğµ)=ğ‘ƒ(ğ´)+ğ‘ƒ(ğµ)
ğ‘ƒ(ğ´|ğµ)=ğ‘ƒ(ğ´)",175,semantic
6d1bdd97-b388-4177-b5c5-e3954c8dda3f,Sampling and Simulation.pdf,CSCI_83,4,"Review
Bayesâ€™ theorem
Marginal distribution
For continuous distributon
For discrete distribution
ğ‘ƒ(ğ´|ğµ)=ğ‘ƒ(ğµ|ğ´)ğ‘ƒ(ğ´)ğ‘ƒ(ğµ)
ğ‘()= ğ‘(, ,â€¦, )Â ğ‘‘ğœƒ2,â€¦,ğ‘‘ğœƒ1 âˆ«,â€¦,ğœƒ2 ğœƒğ‘›
ğœƒ1ğœƒ2 ğœƒğ‘› ğœƒğ‘›
ğ‘()= ğ‘(, ,â€¦, )ğœƒ1 âˆ‘,â€¦,ğœƒ2 ğœƒğ‘›
ğœƒ1ğœƒ2 ğœƒğ‘›",199,semantic
288e0326-2eec-40c5-9fe1-279c57d48e34,Sampling and Simulation.pdf,CSCI_83,5,"Introduction
Sampling is a fundamental process in the collection and analysis of data
Sampling is important because we almost never have data on a whole population
Sampling must be randomized to preclude biases
As sample size increases, the standard error decreases by the law of large numbers
Key points to keep in mind:
Understanding sampling is essential to ensure data is representative of the entire population
Use inferences on the sample to say something about the population
The sample must be randomly drawn from the population
Sampling from distribution is the building block of simulation
We will take up the topic of resampling later",645,semantic
741c2a0f-b376-45b7-96c0-cac4ce97ede0,Sampling and Simulation.pdf,CSCI_83,6,"Sampling Example
Use Case Sample Population
A/B Testing The users we show either web sites A or B All possible users, past present and future
World Cup Soccer 32 teams which qualify in one season All national teams in past, present and future years
Average height of data science students Students in a data science class All students taking data science classes world wide
Tolerances of a manufactured part Samples taken from production lines All parts manufactured in the past, present andfuture
Numbers of a species in a habitat Population counts from sampled habitats All possible habitats in the past, present and future
In several cases it is not only impractical, but impossible to collect data from the entire population
We nearly always work with samples, rather than the entire population.",799,semantic
03393fcd-7172-459b-a9b8-817b34c2366b,Sampling and Simulation.pdf,CSCI_83,7,"Importance of Random Sampling
All statistical methods rely on the use of randomized unbiased samples
Failure to randomized samples violates many key assumptions of statistical models
An understanding of proper use of sampling methods is essential to statistical inference
Most commonly used machine learning algorithms assume that training data are unbiased and independent and identically distributed(iid)
These conditions are only met if training data sample is randomized
Otherwise, the training data will be biased and not represent the underlying process distribution",572,semantic
22dab002-90d4-4e41-861a-d6041066b060,Sampling and Simulation.pdf,CSCI_83,8,"Sampling Distributions
Sampling of a population is done from an unknown population distribution, 
Any statistic, , we compute for the generating process is based on a sample, 
The statistic is an approximation, of a population parameter
For example, the mean of the population is 
But, sample estimate is 
If we continue to take random samples from the population and compute estimates of a statistic, we generate a sampling distribution
Hypothetical concept of the sampling distribution is a foundation of frequentist statistics
Example, if we continue generating samples and computing the sample means,  for the ith sample
Frequentist statistics built on the idea of randomly resampling the population distribution and recomputing a statistic
In the frequentist world, statistical inferences are performed on the sampling distribution
Sampling process must not bias the estimates of the statistic
 
ğ‘   Ì‚Â 
ğœ‡
ğ‘¥Â¯
ğ‘¥Â¯ğ‘–",915,semantic
5462dd3e-2b2b-4b86-b92f-f5cdc3265034,Sampling and Simulation.pdf,CSCI_83,9,"Sampling Distributions
Sampling of a population is done from an unknown population distribution, 
Any statistic, , we compute for the generating process is an approximation for the population, 
Sampling distribution of unknown population parameter
 
ğ‘  ğ‘ () Ì‚",257,semantic
917ba4e4-1a1c-4ef1-b25a-b0c5fb30ee93,Sampling and Simulation.pdf,CSCI_83,10,"Sampling and the Law of Large Numbers
The weak law of large numbers is a theorem that states that statistics of independent random samples converge to the populationvalues as more samples are used
Example, for a population distribution, , the sample mean is:
Then by the weak law of large numbers:
This result is reassuring, the larger the sample the more the statistic converges to the population parameter
 (ğœ‡,ğœ)
ğ¿ğ‘’ğ‘¡Â  =ğ‘‹Â¯ 1ğ‘›âˆ‘ğ‘–=1
ğ‘›
ğ‘‹ğ‘–
â†’ğ¸(ğ‘‹)=ğœ‡ğ‘‹Â¯ ğ‘ğ‘ ğ‘›â†’âˆ",451,semantic
5326052d-cae8-47dd-8dc6-434486c4ecff,Sampling and Simulation.pdf,CSCI_83,11,"Sampling and the Law of Large Numbers
The law of large numbers is foundational to statistics
We rely on the law of large numbers whenever we work with samples
Assume that larger samples are more representatives of the population we are sampling
Is foundation of sampling theory, plus modern computational methods; simulation, bootstrap resampling, and Monte Carlo methods
If the real world did not follow this theorem, then much of statistics (along with much of science and technology) would have to be rethought",513,semantic
1ba6a164-d152-4333-8e12-9757cee123fe,Sampling and Simulation.pdf,CSCI_83,12,"Sampling and the Law of Large Numbers
The weak law of large numbers has a long history
Jacob Bernoulli posthumously published the ï¬rst proof for the Binomial distribution in 1713
Law of large numbers is sometimes referred to as Bernoulliâ€™s theorem
A more general proof was published by Poisson in 1837.",302,semantic
8a49d283-daf2-4d4e-95c9-545d7efc4aaa,Sampling and Simulation.pdf,CSCI_83,13,"Sampling and the Law of Large Numbers
A simple example
The mean of fair coin ï¬‚ips (0,1) = (T,H) converges to the expected value with more ï¬‚ips
The mean converges to the expected value of 0.5 for 
## 5  0.80## 50  0.36## 500  0.48## 5000  0.49
ğ‘›=5,50,500,5000",258,semantic
01ea0583-b8db-4422-aa35-3bf74a215b16,Sampling and Simulation.pdf,CSCI_83,14,"Sampling and the Law of Large Numbers
A simple example; mean of fair coin ï¬‚ips (0,1) = (T,H) converges to the expected value with more ï¬‚ips
Convergance of mean estimates for fair coin",183,semantic
a865e0c5-4d42-4f91-928f-87cb1c9be91d,Sampling and Simulation.pdf,CSCI_83,15,"the Central Limit Theorem (CLT)
Law of large number is almost too obvious, but the CLT is more tricky! Law of large number applied to any statistic, but the CLT applies only to the mean
Let  be a random variable representing the population
 is allowed to have any distribution (not limited to normal), and let  be your true population mean and  the true population standard deviation
Given sample size  thee sampling distribution of is
ğ‘‹
ğ‘‹ ğœ‡ ğœ
ğ‘› ğ‘‹Â¯
âˆ¼ ğ‘(ğœ‡, )ğ‘‹Â¯ ğœğ‘›âˆš",463,semantic
e9269e0c-42af-4d7c-8c9e-e6ec77ca6ebd,Sampling and Simulation.pdf,CSCI_83,16,"importance of CLT
CLT is a sort of guarantee
Sampling distribution of mean estimates do not depend on the population the sample was drawn from
Standard deviation  of the sampling distribution of  converges as 
Only depends on the populationâ€™s mean and variance, and on the sample size
CLT is the basis for hypothesis testing
ğ‘  ğ‘¥Â¯ 1/ğ‘›âˆš",334,semantic
1e68a6a4-7b36-4f62-a961-207eb7ec2573,Sampling and Simulation.pdf,CSCI_83,17,"Example of CLT
Start with a mixture of Normal distributions",59,semantic
f1506131-4806-4188-9a13-b1a19864441b,Sampling and Simulation.pdf,CSCI_83,18,"Example CLT
Sample distribution of the mean of mixture of Normals is Normally distributed! Repetitively random sample the population, 
Compute the mean estimate,  for each sample
ğ‘ ğ‘–ğ‘§ğ‘’=50
ğ‘¥Â¯
x_means = np.array([        nr.choice(x, size=5 0 , replace=True).mean()        for i in range(5 0 0 )])
breaks = np.linspace(x_means.min(), x_means.max(), num=4 0 )fig, ax = plt.subplots(1 ,2 , figsize=(1 2 , 5 ) ) _ = ax[0 ].hist(x_means, bins=breaks)
_ = sm.qqplot(x_means, line='s', ax=ax[1 ])",487,semantic
1d959f33-da9f-443a-a4b4-144eefbc1acf,Sampling and Simulation.pdf,CSCI_83,19,"Example CLT
Sample distribution of the mean of mixture of Normals is Normally distributed!",90,semantic
c4f75ebc-f8ca-4ff0-9fac-afbd4f840b3a,Sampling and Simulation.pdf,CSCI_83,20,"Standard Error and Convergence for a Normal Distribution
As we sampled from a Normal distribution, the sample means converges to the population mean
What can we say about the expected error of the mean estimate as the number of samples increases? Population has standard deviation 
This measure is known as the standard error of the sample mean
By the CLT the standard error is deï¬ned:
Standard error decreases as the square root of 
Example, if you wish to halve the error, you will need to sample four times as many values. For the mean estimate, , deï¬ne the uncertainty in terms of conï¬dence intervals
For 95% conï¬dence interval:
ğœ
ğ‘ ğ‘’=Â±ğœ(ğ‘›)âˆš
ğ‘›
ğ‘¥Â¯
ğ¶ =Â±1.96Â ğ‘ ğ‘’ğ¼95 ğ‘¥Â¯",667,semantic
0063b04f-ccf5-4cd0-82d5-f0a4f12fe946,Sampling and Simulation.pdf,CSCI_83,21,"Convergence and Standard Errors for a Normal Distribution
Mean estimates for realizations of standard Normal distribution with 95% conï¬dence intervals
Convergance of mean estimates with standard errors",201,semantic
efab3e19-b268-4276-a664-f8de7d3c93be,Sampling and Simulation.pdf,CSCI_83,22,"Sampling Strategies
There are a great number of possible sampling methods. Some of the most commonly used methods
Bernoulli sampling, a foundation of random sampling
Stratiï¬ed sampling, when groups with different characteristics must be sampled
Cluster sampling, to reduce cost of sampling
Systematic sampling and convenience sampling, a slippery slope",352,semantic
00cdbe76-2be1-4486-a637-677521d60410,Sampling and Simulation.pdf,CSCI_83,23,"Bernoulli Sampling
Bernoulli sampling is a widely used foundational random sampling strategy
Bernoulli sampling has the following properties:
A single random sample of the population is created
A particular value in the population is sampled based on the outcome of a Bernoulli trial with ï¬xed probability of success, 
Example, a company sells a product by weight
To ensure the quality of a packaging process so few packages are underweight
Impractical to empty and weight the contents of every package
Bernoulli randomly sampled packages from the production line and weigh contents
Statistical inferences are made from sample
ğ‘",628,semantic
32232391-086e-4c52-ad07-3729e3c6d2a0,Sampling and Simulation.pdf,CSCI_83,24,"Bernoulli Sampling
An example with synthetic data. - Generate population of 10000 samples from the standard Normal distribution- The realizations are randomly divided into 4 groups with - The probability of a sample being in a group is not uniform, and sums to 1.0. ##         var  group## 0  1.469248      1## 1 -1.150144      2## 2  2.519226      2## 3 -0.082478      2## 4 -0.033601      0## 5 -1.636656      0## 6 -0.412092      2## 7  1.784949      1## 8  0.042383      2## 9 -0.619732      2
ğ‘=[0.1,0.3,0.4,0.2]
nr.seed(3 4 5 )population_size = 1 0 0 0 0 
data = pd.DataFrame({""var"":nr.normal(size = population_size),                      ""group"":nr.choice(range(4 ), size= population_size, p = [0 . 1 ,0 . 3 ,0 .",719,semantic
e60e7b0b-48a0-413e-a6f2-3c0e818f9db8,Sampling and Simulation.pdf,CSCI_83,24,"4 ,0 . 2 ])})data.head(1 0 )",28,semantic
ea391c62-db7e-4d8c-a818-b99ff93d7dc9,Sampling and Simulation.pdf,CSCI_83,25,"Bernoulli Sampling
The population of 10000 samples from the standard Normal distribution
The mean of each group should be close to 0.0:1. The sample is divided between 4 groups2. Probability of sample from given group, 3. Summary statistics are computed for each group
##        Count      Mean        SE  Upper_CI  Lower_CI## group                                               ## 0        102 -0.030849  0.102082  0.169231 -0.230929## 1        297 -0.000648  0.056728  0.110538 -0.111834## 2        399  0.003840  0.050075  0.101986 -0.094306## 3        200 -0.014848  0.071609  0.125506 -0.155202
ğ‘=0.1
def count_mean(dat, p=1 . 0 ):    import numpy as np    import pandas as pd    groups = dat.groupby('group') # Create the groups
    n_samples = np.int64(p * groups.size())    se = np.sqrt(np.divide(groups.aggregate(np.var).loc[:, 'var'], n_samples))    means = groups.aggregate(np.mean).loc[:, 'var']
    ## Create a data frame with the counts and the means of the groups    return pd.DataFrame({'Count': n_samples,                         'Mean': means,                        'SE': se,
                        'Upper_CI': np.add(means, 1 . 9 6  * se),                        'Lower_CI': np.add(means, -1 . 9 6  * se)})p = 0 .",1234,semantic
3e4ee82c-c808-48a1-babd-a4e5cda29d08,Sampling and Simulation.pdf,CSCI_83,25,"1 
count_mean(data, p)",22,semantic
a531f9c0-172b-47b7-b134-3f5927e8f8f5,Sampling and Simulation.pdf,CSCI_83,26,"Sampling Grouped Data
Group data is quite common in application
A few examples include:
1. Pooling opinion by county and income group, where income groups and counties have signiï¬cant differences in population
2. Testing a drug which may have different effectiveness by sex and ethnic group
3.",293,semantic
8c479f67-e682-4a46-b035-988b43d40112,Sampling and Simulation.pdf,CSCI_83,26,Spectral characteristics of stars by type,41,semantic
4a298bb2-fccf-4ce9-b617-14d3f78cb465,Sampling and Simulation.pdf,CSCI_83,27,"Stratiï¬ed Sampling
What is a sampling strategy for grouped or stratiï¬ed data? Stratiï¬ed sampling strategies are used when data are organized in strata
Simple Idea: independently sample an equal numbers of cases from each strata
The simplest version of stratiï¬ed sampling creates an equal-size Bernoulli sample from each strata
In many cases, nested samples are required
For example, a top level sample can be grouped by zip code, a geographic strata
Within each zip code, people are then sampled by income bracket strata
Equal sized Bernoulli samples are collected at the lowest level",584,semantic
a3844c0b-ebba-482d-a1b1-93e981f74992,Sampling and Simulation.pdf,CSCI_83,28,"Example
Bernoulli sample 100 from each group and compute summary statistics
##        Count      Mean        SE  Upper_CI  Lower_CI## group                                               ## 0        100 -0.041753  0.097958  0.150244 -0.233750## 1        100 -0.159626  0.092409  0.021495 -0.340748## 2        100 -0.072279  0.097027  0.117893 -0.262451## 3        100  0.129402  0.102523  0.330347 -0.071543
def stratify(dat, p):
    groups = dat.groupby('group') # Create the groups    nums = min(groups.size()) # Find the size of the smallest group    num = int(p * dat.shape[0 ]) # Compute the desired number of samples per group
    if num <= nums:         ## If sufficient group size, sample each group. ## We drop the unneeded index level and return,         ## which leaves a data frame with just the original row index. return groups.apply(lambda x: x.sample(n=num)).droplevel('group')    else: # Oops.",909,semantic
ece56499-9ea2-4eb3-83be-43366632f975,Sampling and Simulation.pdf,CSCI_83,28,"p is to large and our groups cannot accommodate the choice of p. pmax = nums / dat.shape[0 ]
        print('The maximum value of p = ' + str(pmax))
p = 0 . 0 1 stratified = stratify(data, p)
count_mean(stratified)",213,semantic
ec471eaf-f474-4517-8f36-3476d537425c,Sampling and Simulation.pdf,CSCI_83,29,"Cluster Sampling
When sampling is expensive, a strategy is required to reduce the cost
Examples of expensive to collect data:
Surveys of customers at a chain of stores
Door to door survey of homeowners
Sampling wildlife populations in a dispersed habitat
Population can be divided into randomly selected clusters:- Deï¬ne the clusters for the population- Randomly select the required number of clusters- Sample from selected clusters- Optionally, stratify the sample within each cluster",485,semantic
669d3f86-304c-4fe4-85a3-19b07d4784b0,Sampling and Simulation.pdf,CSCI_83,30,"Cluster Sampling
As an example, select a few store locations and Bernoulli sample customers at these locations. ##        Count      Mean        SE  Upper_CI  Lower_CI## group                                               ## 0         97  0.023343  0.103481  0.226166 -0.179479## 1         97  0.049290  0.104858  0.254812 -0.156232## 2         98  0.056362  0.109000  0.270002 -0.157278## 3        102 -0.146424  0.105291  0.059947 -0.352795## 4        111 -0.025847  0.100979  0.172072 -0.223765## 5        100  0.079969  0.090989  0.258308 -0.098369## 6        104 -0.064531  0.101523  0.134454 -0.263515## 7        111 -0.057377  0.092665  0.124247 -0.239000## 8         87  0.038115  0.103077  0.240147 -0.163916## 9         93  0.033201  0.100668  0.230510 -0.164109
## First compute the clusters
num_clusters = 1 0 num_vals = 1 0 0 0 ## Create a data frame with randomly sampled cluster numbers
clusters = pd.DataFrame({'group': range(num_clusters)}).sample(n = num_vals, replace = True)## Add a column to the data frame with Normally distributed valuesclusters.loc[:, 'var'] = nr.normal(size = num_vals)",1111,semantic
7d7250af-5fae-4af7-a2ac-f0ce34533a31,Sampling and Simulation.pdf,CSCI_83,31,"Cluster Sampling
Randomly select 3 clusters
The sampled clusters
## cluster sampled are:
## 7## 5## 4
Display summary statistics
##        Count      Mean        SE  Upper_CI  Lower_CI## group                                               ## 4        111 -0.025847  0.100979  0.172072 -0.223765## 5        100  0.079969  0.090989  0.258308 -0.098369## 7        111 -0.057377  0.092665  0.124247 -0.239000
## Randomly sample the group numbers, making sure we sample from 
## unique values of the group numbers. clusters_samples = nr.choice(clusters.loc[:, 'group'].unique(),                              size = 3 , replace = False)
## Now sample all rows with the selected cluster numbersclus_samples = clusters.loc[clusters.loc[:, 'group'].isin(clusters_samples), :]",766,semantic
3b9f91c5-e41b-477e-88a1-db8f9b19c9eb,Sampling and Simulation.pdf,CSCI_83,32,"Systematic Sampling
Convenience and systematic sampling are a slippery slope toward biased inferences
Systematic sampling lacks randomization
Convenience sampling selects the cases that are easiest to obtain
Commonly cited example known as database sampling
Example, the ï¬rst N rows resulting from a database query
Example, every k-th case of the population",357,semantic
ea29c70b-561f-4301-a418-0c0d53d329d5,Sampling and Simulation.pdf,CSCI_83,33,"A Few More Thoughts on Sampling
There are many practical aspects of sampling. Random sampling is essential to the underlying assumptions of statistical inference
Whenever you are planning to sample data, make sure you have a clear sampling plan
Know the number of clusters, strata, samples in advance
Donâ€™t just stop sampling when your desired result is achieved: e.g. error measure!",383,semantic
8d6ff321-f699-4cbe-8c1c-4b9b83399dd3,Sampling and Simulation.pdf,CSCI_83,34,"Introduction to Simulation
Simulation enables data scientists to study the behavior of stochastic processes with complex probability distributions
Most real-world processes have complex behavior, resulting in complex distributions of output values
Simulation is a practical approach to understanding these complex processes
Two main purposes of simulation can be summarized as:
Testing models: If data simulated from the model do not resemble the original data, something is likely wrong
Understand processes with complex probability distributions: In these cases, simulation provides a powerful and ï¬‚exible computational technique to understand behavior",654,semantic
52f50bb9-faff-4e52-beb9-06cdf8d08490,Sampling and Simulation.pdf,CSCI_83,35,"Introduction to Simulation
As cheap computational power has become ubiquitous, simulation has become a widely used technique
Simulations compute a large number of cases, or realizations
The computing cost of each realization must be low in any practical simulation
Realizations are drawn from complex probability distributions of the process model
In many cases, realizations are computed using conditional probability distributions
The ï¬nal or posterior distribution of the process is comprised of these realizations",517,semantic
8739a737-4a26-4e4d-9c7e-00070399ef9d,Sampling and Simulation.pdf,CSCI_83,36,"Representation as a Directed Acyclic Graphical Model
When creating a simulation with multiple conditionally dependent variables it is useful to draw a directed graph; a directed acyclic graphical model orDAG
The graph is a communications device showing which variables are independent and which are conditionally dependent on others with the shapes usedrepresenting the type of nodes
Probability distributions of the variables are shown as ellipses
Distributions have parameters which must be estimated
Decision variables are deterministic and are shown as rectangles
Decisions are determined by variables
Setting decision variables can be performed either manually or automatically
Utility nodes, proï¬t in this case, are shown as diamonds
Nodes represent a utility function given the dependencies in the graph
Utility calculations are deterministic given the input values
Directed edges show the dependency structure of the distributions
Arrows point to child nodes which are dependent on parent nodes
Child node conditional on parent nodes",1041,semantic
96001fc0-e91a-4134-aa46-0e219bbadfa2,Sampling and Simulation.pdf,CSCI_83,37,"Sandwich Shop Simulation
The sandwich shop simulation can be represented by a DAG
Directed graph of the distributions for proï¬t simulation",138,semantic
c94d5653-5317-43f4-987b-f21ba4bc89cd,Sampling and Simulation.pdf,CSCI_83,38,"Sandwich Shop Simulation
Interpreting the DAG
The DAG is a shorthand description of the simulation model
Leaves of the DAG are independent distributions
Parameters must be known or estimated
Can be useful to vary the parameters
Child distributions are conditional on their parents
Parameters must be known or estimated
Resulting distribution can be quite complex
Decision variables deterministicly change the model parameters
Utility node uses a ï¬xed deterministic formula to compute the value for each realization of the simulaiton",532,semantic
932f3c8e-6970-45f4-b602-0eaa1253e2b0,Sampling and Simulation.pdf,CSCI_83,39,"Tips on Building Simulations
Creating, testing and debugging simulation software can be tricky given the stochastic nature of simulation
Build your simulation as a series of small, easily tested, chunks
Test each small functional unit individually, including at least testing some typical cases, as well as boundary or extreme cases
Test your overall simulation each time you add a new functional component - avoid big bang integration! Simulations are inherently stochastic, set a seed before you begin tests so they are repeatable",532,semantic
216ba496-cdec-422b-b26e-d60fb92cb537,Sampling and Simulation.pdf,CSCI_83,40,"Summary
Sampling is a fundamental process in the collection and analysis of data
Sampling is important because we almost never have data on a whole population
Sampling must be randomized to preclude biases
As sample size increases the standard error of a statistic computed from the sample decreases by the law of large numbers
Key points to keep in mind:
Understanding sampling is essential to ensure data is representative of the entire population
Use inferences on the sample to say something about the population
The sample must be randomly drawn from the population
Sampling from distribution is the building block of simulation",633,semantic
08aa94ef-5ab1-4bac-a8a5-685c230f7c0d,Introduction to Bayesian Models.pdf,CSCI_83,0,"Introduction to Bayesian Models
Steve Elston
10/13/2022",55,semantic
bc05bc85-7bab-4151-a730-c93fd536102c,Introduction to Bayesian Models.pdf,CSCI_83,1,"Review
The concept of likelihood and maximum likelihood estimation (MLE) have been at the core of much of statistical modeling for about 100 years
In 21st Century, likelihood and MLE ideas continue to be foundational
The principles we can easily visualize in a few dimensions apply in high dimenstions
An understanding of these principles helps you understand modern large scale methods
Understanding the concept of likelihood and the use of MLE methods is key to understanding many parametric statistical methods
Likelihood is a measure of how well a model ï¬ts data
MLE is a generic methods for parameter estimation
MLE used widely for machine learning models, including some deep learning models",697,semantic
17973849-711c-493e-9060-67a84d7c368f,Introduction to Bayesian Models.pdf,CSCI_83,2,"Review
Statistical inference seeks to characterize the uncertainty in statistical point estimates
Statistics are estimates of population parameters
Inferences using statistics must consider the uncertainty in the estimates
Conï¬dence intervals quantify uncertainty in statistical estimates
Two-sided conï¬dence intervals: express conï¬dence that a value is within some range around the point estimate
One-sided conï¬dence intervals: express conï¬dence that the point estimate is greater or less than some range of values",515,semantic
cc352652-2c74-4b80-ab6c-e2dc642d92ab,Introduction to Bayesian Models.pdf,CSCI_83,3,"Review
Nonparametric bootstrap estimation is widely useful and requires minimal assumption
Bootstrap distribution is comprised of values of the statistic computed from bootstrap resamples of the original observations (data sample)
Computing bootstrap distribution requires no assumptions about population distribution! Bootstrap resampling substitutes computer power for paper and pencil statistician power
Bootstrap resampling estimates the bootstrap distribution of a statistic
Compute mostly likely point estimate of the statistic, or bootstrap estimate
The bootstrap conï¬dence interval is computed from the bootstrap distribution",633,semantic
85b56351-1f16-4893-8ee6-09c0519a8aa3,Introduction to Bayesian Models.pdf,CSCI_83,4,"Review
There are several variations of the basic nonparametric bootstrap algorithm
One sample bootstrap
Inference on single statistic
Two sample bootstrap
Inference on different statistic
Special cases
Correlation coefï¬cients - part of your assignment",251,semantic
88af1919-50f6-440e-9d4b-9a05c9bbbb81,Introduction to Bayesian Models.pdf,CSCI_83,5,"Review
Re-sampling methods are general and powerful but, there is no magic involved! There are pitfalls! If a sample is biased, the re-sampled statistic estimate based on that sample will be biased
Results can be no better than the sample you start with
Example; the bootstrap estimate of mean is the unbiased sample estimate, , not the population parameter, 
The sample variance and Cis can be no better than the sample distribution allows
Be suspicious of overly optimistic conï¬dence intervals
CIs can be optimistically biased
Are computationally intensive, but often highly parallelizable
ğ‘¥Â¯ ğœ‡",596,semantic
bf354843-8eb2-436f-9d17-5ab4b805cf7a,Introduction to Bayesian Models.pdf,CSCI_83,6,"Introduction Baysian Models
Despite the long history, Bayesian models have not been used extensively until recently
Two traditions in statistics
Frequentist we have been working with previously
Bayesian statistics
Limited use is a result of several difï¬culties
Rarely taught for much of the 20th Century
The need to specify a prior distribution has proved a formidable intellectual obstacle
Modern Bayesian methods are often computationally intensive and have become practical only with cheap computing
Recent emergence of improved software and algorithms has resulted in wide and practical access to Bayesian methods",617,semantic
968220ec-2b25-4f3e-b917-7ac3fa03f11e,Introduction to Bayesian Models.pdf,CSCI_83,7,"Introduction
Bayesian analysis is a contrast to frequentist methods
The objective of Bayesian analysis is to compute a posterior distribution
Contrasts with frequentist statistics is to compute a point estimate and conï¬dence interval from a sample
Bayesian models allow expressing prior information in the form of a prior distribution
Selection of prior distributions can be performed in a number of ways
The posterior distribution is said to quantify our current belief
Update beliefs based on additional data or evidence
A critical difference with frequentist models which must be computed from a complete sample
Inference can be performed on the posterior distribution by ï¬nding the maximum a postiori (MAP) value and a credible interval",740,semantic
0614b5c0-36d6-41f9-958f-a9ab8d5d092c,Introduction to Bayesian Models.pdf,CSCI_83,8,"Bayesian Model Use Case
Bayesian methods made global headlines with the successful location of the missing Air France Flight 447
Aircraft had disappeared in little traveled area of the South Atlantic Ocean
Conventional location methods had failed to locate the wreckage; potential search area too large
Bayesian methods rapidly narrowed the prospective search area
Used â€˜prior informationâ€™ on aircraft heading and time of sattelite transmisison
Posterior distribution of locations of Air France 447",498,semantic
659a260a-8d90-4161-b34a-91d36d2abde4,Introduction to Bayesian Models.pdf,CSCI_83,9,"Bayesian Model Use Case
Kratzke, Stone and Frost developed an optimal search missing planner using Baysian model
Search areas concentrate on high posterior probability regions
Model accounts for current, wind, etc. Screen shot from USCG search planner",251,semantic
fc3e880b-1c1b-4f88-ad87-33e0b2f05fe5,Introduction to Bayesian Models.pdf,CSCI_83,10,"Bayesian vs. Frequentist Views
With greater computational power and general acceptance, Bayes methods are now widely used
Among pragmatists
Some problems are better handled by frequentist methods
Some problems with Bayesian methods
Bayes models allow us to express prior information
Models that fall between these extremes are also in common use
Methods include the so-called empirical Bayes methods.",400,semantic
a42c5a05-7133-4522-ad50-d6751efd9638,Introduction to Bayesian Models.pdf,CSCI_83,11,"Bayesian vs. Frequentist Views
Can compare the contrasting frequentist and Bayesian approaches
Comparison of frequentist and Bayes methods",138,semantic
4638013a-76ac-419e-8b91-0b11ae7f86d5,Introduction to Bayesian Models.pdf,CSCI_83,12,"Review of Bayes Theorem
Bayesâ€™ Theorem is fundamental to Bayesian data analysis. Start with:
We can also write:
Eliminating 
And ï¬nally, Bayes theorem! ğ‘ƒ(ğ´âˆ©ğµ)=ğ‘ƒ(ğ´|ğµ)ğ‘ƒ(ğµ)
ğ‘ƒ(ğ´âˆ©ğµ)=ğ‘ƒ(ğµ|ğ´)ğ‘ƒ(ğ´)
ğ‘ƒ(ğ´âˆ©ğµ):
ğ‘ƒ(ğµ)ğ‘ƒ(ğ´|ğµ)=ğ‘ƒ(ğ´)ğ‘ƒ(ğµ|ğ´)
ğ‘ƒ(ğ´|ğµ)=ğ‘ƒ(ğµ|ğ´)ğ‘ƒ(ğ´)ğ‘ƒ(ğµ)",239,semantic
b551b943-823a-441d-a5df-9de5b722ae22,Introduction to Bayesian Models.pdf,CSCI_83,13,"Bayes Theorem
Bayes Theorem!",28,semantic
bc62031d-44c6-4cc0-a6a1-680db70c8106,Introduction to Bayesian Models.pdf,CSCI_83,14,"Marginal Distributions
In many cases we are interested in the marginal distribution
Example, it is often the case that only one or a few parameters of a joint distribution will be of interest
In other words, we are interested in the marginal distribution of these parameters
The denominator of Bayes theorem, , can be computed as a marginal distribution
Consider a multivariate probability density function with  variables, 
Marginal distribution is the distribution of one variable with the others integrated out. Integrate over all other variables  the result is the marginal distribution, :
- But computing this integral is not easy! ğ‘ƒ(ğ‘‘ğ‘ğ‘¡ğ‘)
ğ‘› ğ‘(, ,â€¦, )ğœƒ1 ğœƒ2 ğœƒğ‘›
{ ,â€¦, }ğœƒ2 ğœƒğ‘› ğ‘( )ğœƒ1
ğ‘()= ğ‘(, ,â€¦, )Â ğ‘‘ğœƒ2,â€¦,ğ‘‘ğœƒ1 âˆ«,â€¦,ğœƒ2 ğœƒğ‘›
ğœƒ1 ğœƒ2 ğœƒğ‘› ğœƒğ‘›",731,semantic
8488b818-392d-4773-aa8b-49fe940a7257,Introduction to Bayesian Models.pdf,CSCI_83,15,"Marginal Distributions
For discrete distributions compute the marginal by summation
Or, for discrete samples of a continuous distribution
Example, need to know (un-normalized) posterior distribution of parameter , a marginal distribution:
Now we have the marginal distribution of 
Or, we need to ï¬nd the denominator for Bayes theorem to normalize our posterior distribution, a marginal distribution:
We can compute  from samples without ever directly computing the marginal
ğœƒ
ğ‘(ğœƒ)= ğ‘(ğœƒ|ğ—)Â ğ‘(ğ—)âˆ‘ğ‘¥âˆˆ ğ—
ğœƒ
ğ‘(ğ—)= ğ‘(ğ—|ğœƒ)ğ‘(ğœƒ)âˆ‘ğœƒâˆˆ Î˜
ğ‘(ğ—)",527,semantic
fc8333a2-d48a-4998-a25e-57cce7e03dda,Introduction to Bayesian Models.pdf,CSCI_83,16,"Interpreting Bayes Theorem
How can you interpret Bayesâ€™ Theorem? For model parameter estimation problem:
Or, Bayesâ€™ theorem in terms of model parameters:
Summarized as:
ğ‘ƒğ‘œğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘œğ‘ŸÂ ğ·ğ‘–ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›=ğ¿ğ‘–ğ‘˜ğ‘’ğ‘™ğ‘–â„ ğ‘œğ‘œğ‘‘âˆ™ğ‘ƒğ‘Ÿğ‘–ğ‘œğ‘ŸÂ ğ·ğ‘–ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›ğ¸ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘›ğ‘ğ‘’
ğ‘ğ‘œğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘œğ‘ŸÂ ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ â”‚ğ‘‘ğ‘ğ‘¡ğ‘)=ğ¿ğ‘–ğ‘˜ğ‘’ğ‘™ğ‘–â„ ğ‘œğ‘œğ‘‘(ğ‘‘ğ‘ğ‘¡ğ‘|ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ )Â ğ‘ƒğ‘Ÿğ‘–ğ‘œğ‘Ÿ(ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ )ğ‘ƒ(ğ‘‘ğ‘ğ‘¡ğ‘)
ğ‘ƒ(ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ â”‚ğ‘‘ğ‘ğ‘¡ğ‘)=ğ‘ƒ(ğ‘‘ğ‘ğ‘¡ğ‘|ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ )Â ğ‘ƒ(ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ )ğ‘ƒ(ğ‘‘ğ‘ğ‘¡ğ‘)",383,semantic
04ed8614-b7d2-4a37-8534-59bffd783aea,Introduction to Bayesian Models.pdf,CSCI_83,17,"Interpreting Bayes Theorem
What do these terms actually mean? 1. Posterior distribution of the parameters given the evidence or data, the goal of Bayesian analysis
2.",166,semantic
4e55f368-2d15-4bd7-a39a-21bbd6a1a32c,Introduction to Bayesian Models.pdf,CSCI_83,17,"Prior distribution is chosen to express information available about the model parameters apriori
3. Likelihood is the conditional distribution of the data given the model parameters
4. Probabiltiy of Data or evidence is the distribution of the data and normalizes the posterior
Relationships can apply to the parameters in a model; partial slopes, intercept, error distributions, lasso constants, etc",400,semantic
032548a8-e43c-434f-98dc-c7fce710f22b,Introduction to Bayesian Models.pdf,CSCI_83,18,"Applying Bayes Theorem
We need a tractable formulation of Bayes Theorem for computational problems
We must avoid directly summing all of the possibilities to compute the denominator, 
In many cases, computing this denominator directly is intractable
Some interesting facts about conditional probabilities:
Where, , and the marginal distribution, , can be written:
ğ‘ƒ(ğµ)
ğ‘ƒ(ğµâˆ©ğ´)=ğ‘ƒ(ğµ|ğ´)ğ‘ƒ(ğ´)ğ´ğ‘›ğ‘‘ğ‘ƒ(ğµ)=ğ‘ƒ(ğµâˆ©ğ´)+ğ‘ƒ(ğµâˆ© )ğ´Â¯
=ğ‘›ğ‘œğ‘¡Â ğ´ğ´Â¯ ğ‘ƒ(ğµ)
ğ‘ƒ(ğµ)=ğ‘ƒ(ğµ|ğ´)ğ‘ƒ(ğ´)+ğ‘ƒ(ğµâ”‚)ğ‘ƒ()ğ´Â¯ ğ´Â¯",453,semantic
0be28a1a-f2a8-4c39-82cd-bc21864e3f3b,Introduction to Bayesian Models.pdf,CSCI_83,19,"Applying Bayes Theorem
Using the foregoing relations we can rewrite Bayes Theorem as:
Computing the denominator requires summing all cases in the subsets  and 
This is a bit of a mess! And, the siguation is worse if there are multiple alternative hypotheses! Fortunately, we can often avoid computing this denominator by force
Write Bayes Theorem as:
Ignoring the normalization constant :
ğ‘ƒ(ğ´|ğµ)= ğ‘ƒ(ğ´)ğ‘ƒ(ğµ|ğ´)ğ‘ƒ(ğµâ”‚ğ´)ğ‘ƒ(ğ´)+ğ‘ƒ(ğµâ”‚)ğ‘ƒ()ğ´Â¯ ğ´Â¯
ğ´ ğ‘›ğ‘œğ‘¡Â ğ´
ğ‘ƒ(ğ´â”‚ğµ)=ğ‘˜âˆ™ğ‘ƒ(ğµ|ğ´)ğ‘ƒ(ğ´)
ğ‘˜
ğ‘ƒ(ğ´â”‚ğµ)âˆ ğ‘ƒ(ğµ|ğ´)ğ‘ƒ(ğ´)",480,semantic
e7fa043b-dd9c-417a-b6d6-a3ee41977115,Introduction to Bayesian Models.pdf,CSCI_83,20,"Interpreting Bayes Theorem
Denominator must account for all possible outcomes, or alternative hypotheses, :
Computing this denominator is a formidable problem! Can be inï¬nite number of alternative hypotheses; e.g. continuous random variable
â„ â€²
ğ‘ƒğ‘œğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘œğ‘Ÿ(â„ ğ‘¦ğ‘ğ‘œğ‘¡â„ ğ‘’ğ‘ ğ‘–ğ‘ Â |Â ğ‘’ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘›ğ‘ğ‘’)=ğ¿ğ‘–ğ‘˜ğ‘’ğ‘™ğ‘–â„ ğ‘œğ‘œğ‘‘(ğ‘’ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘›ğ‘ğ‘’Â |Â â„ ğ‘¦ğ‘ğ‘œğ‘¡â„ ğ‘’ğ‘ ğ‘–ğ‘ )Â ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ(â„ ğ‘¦ğ‘ğ‘œğ‘¡â„ ğ‘’ğ‘ ğ‘–ğ‘ )ğ¿ğ‘–ğ‘˜ğ‘’ğ‘™ğ‘–â„ ğ‘œğ‘œğ‘‘(ğ‘’ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘›ğ‘ğ‘’Â |Â  )Â ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ()âˆ‘âˆˆ Â ğ´ğ‘™ğ‘™Â ğ‘ğ‘œğ‘ ğ‘ ğ‘–ğ‘ğ‘™ğ‘’Â â„ ğ‘¦ğ‘ğ‘œğ‘¡â„ ğ‘’ğ‘ ğ‘’ğ‘ â„ â€² â„ â€² â„ â€²",409,semantic
12890d9b-e58c-4fc7-9dac-e310ba2a7ef0,Introduction to Bayesian Models.pdf,CSCI_83,21,"Simpliï¬ed Relationship for Bayes Theorem
How to we interpret the foregoing relationship? Consider the following relationship:
We can ï¬nd an un-normalized function proportional to the posterior distribution
Sum over  to ï¬nd the marginal distribution 
Approach can transform an intractable computation into a simple summation
ğ‘ƒğ‘œğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘œğ‘ŸÂ ğ·ğ‘–ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›âˆ ğ¿ğ‘–ğ‘˜ğ‘’ğ‘™ğ‘–â„ ğ‘œğ‘œğ‘‘âˆ™ğ‘ƒğ‘Ÿğ‘–ğ‘œğ‘ŸÂ ğ·ğ‘–ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›ğ‘‚ğ‘Ÿğ‘ƒ(ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ â”‚ğ‘‘ğ‘ğ‘¡ğ‘)âˆ ğ‘ƒ(ğ‘‘ğ‘ğ‘¡ğ‘|ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ )ğ‘ƒ(ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ )
ğ‘ƒ(ğ‘‘ğ‘ğ‘¡ğ‘|ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ )ğ‘ƒ(ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ ) ğ‘ƒ(ğµ)",468,semantic
1f53fa84-19d9-42b7-a602-a309aa4e7bc6,Introduction to Bayesian Models.pdf,CSCI_83,22,"Creating Bayes models
The goal of a Bayesian analysis is computing and performing inference on the posterior distribution of the model parameters
The general steps are as follows:
1. Identify data relevant to the research question
2. Deï¬ne a sampling plan for the data.",269,semantic
4f932a2c-b2f4-4dfb-b58c-e5c47d422551,Introduction to Bayesian Models.pdf,CSCI_83,22,"Data need not be collected in a single batch
3. Deï¬ne the model and the likelihood function; e.g. regression model with Normal likelihood
4. Specify a prior distribution of the model parameters
5. Use the Bayesian inference formula to compute posterior distribution of the model parameters
6. Update the posterior as data is observed
7. Inference on the posterior can be performed; compute credible intervals
8. Optionally, simulate data values from realizations of the posterior distribution. These values are predictions from the model.",538,semantic
d4033e28-849c-4eb1-80f9-005a0a0604e5,Introduction to Bayesian Models.pdf,CSCI_83,23,"Updating Bayesian Models
An advantage of Bayesain model is that it can be updated as new observations are made
In contrast, for frequentist models data must be collected completely in advance
We update our belief by adding new evidence
The posterior of a Bayesian model with no evidence is the prior
The previous posterior serves as a prior for model updates",358,semantic
506fed69-91aa-47d4-af5c-eae4716c519e,Introduction to Bayesian Models.pdf,CSCI_83,24,"How can you choose a prior? The choice of the prior is a difï¬cult, and potentially vexing, problem when performing Bayesian analysis
The need to choose a prior has often been cited as a reason why Bayesian models are impractical
General guidance is that a prior must be convincing to a skeptical audience
Often tend to use vague or less informative priors in practice",367,semantic
d6bc0d44-d7dc-425a-a83d-9f098d2afb80,Introduction to Bayesian Models.pdf,CSCI_83,25,"How can you choose a prior? Some possible approaches to prior selection include:
Use prior empirical information
Apply domain knowledge to determine a reasonable distribution
Use information from prior work
Example, the viable range of parameter values could be computed from physical principles
Example, it could be well know that there is price range for some asset
If there is poor prior knowledge for the problem a non-informative prior can be used
One possibility is a Uniform distribution. But be careful! since a uniform prior is informative because of limits on the values!",581,semantic
61ee3fa6-4d39-44c3-914b-0126f6ffa046,Introduction to Bayesian Models.pdf,CSCI_83,25,Other options include the Jefferysâ€™ prior.,42,semantic
501c98f8-a82c-4a66-a6a1-b23af52955bf,Introduction to Bayesian Models.pdf,CSCI_83,26,"How can you choose a prior? How to use prior empirical information to estimate the parameters of the prior distribution
Deriving a prior distribution in this manner is sometimes called empirical Bayes
Has become more practical with large modern data sets
Method somewhere between Bayesian and frequentist
Empirical Bayes approach is often applied in practice
Some Bayesian theoreticians do not consider this a Bayesian approach at all! Example, need a prior distribution of home prices per square foot by location
Use pooled information to compute distribution of prices for all locations
Use the prior with speciï¬c evidence by locations to compute posteriors by location
Is example of hierarchical model
Typically, a less informative prior distribution is used than the actual empirical distribution so the model is not overly constrained",839,semantic
06403051-48fd-4262-9cd7-2a03bf712a97,Introduction to Bayesian Models.pdf,CSCI_83,27,"Conjugate Prior Distributions
An analytically and computationally simple choice for a prior distribution family is a conjugate prior
When a likelihood function is multiplied by its conjugate distribution the posterior distribution will be in the same family as the conjugate prior
Attractive idea for cases where the conjugate distribution exists
Analytic results can be computed
The posterior is a known distribution
But there are many practical cases where a conjugate prior is not used
We will address more general methods later",531,semantic
760b73bf-f644-4d82-8903-e410798b4f90,Introduction to Bayesian Models.pdf,CSCI_83,28,"Conjugate Prior Distributions
Most commonly used distributions have conjugates, with a few examples:
Likelihood Conjugate
Binomial Beta
Bernoulli Beta
Poisson Gamma
Categorical Dirichlet
Normal - mean Normal
Normal - variance, Inverse Gamma
Normal - inverse variance, Gamma
ğœ’2
ğœ",278,semantic
047af5bf-38fa-45c7-8097-a56834bd1cdf,Introduction to Bayesian Models.pdf,CSCI_83,29,"Example using Conjugate Distribution
We are interested in analyzing the incidence of distracted drivers
Randomly sample the behavior of 10 drivers at an intersection and determine if they exhibit distracted driving or not
Data are Binomially distributed, a driver is distracted or not, with likelihood:
Binomial likelihood has one parameter we need to estimate, , the probability of success
ğ‘ƒ(ğ‘˜)=()â‹… (1âˆ’ğœƒğ‘›ğ‘˜ ğœƒğ‘˜ )ğ‘›âˆ’ğ‘˜
ğœƒ",416,semantic
5c910689-823c-4144-9c4d-0e24fa29450b,Introduction to Bayesian Models.pdf,CSCI_83,30,"Working with Conjugate Distribution
Our process for example is:
1.",66,semantic
f5eaa91c-fcef-47db-a914-1ef0da58136f,Introduction to Bayesian Models.pdf,CSCI_83,30,"Use the conjugate prior, the Beta distribution with parameters  and  (or a,b)
2. Using the data sample, compute the likelihood
3. Compute the posterior distribution of distracted driving
4. Add more evidence (data) and update the posterior distribution. ğ›¼ ğ›½",257,semantic
b4a491bb-a1c0-4350-8051-db219c324780,Introduction to Bayesian Models.pdf,CSCI_83,31,"Example using Conjugate Distribution
What are the properties of the Beta distribution? Beta distribution for different parameter values",135,semantic
41c30761-67ef-40bf-b94f-1a41850fee66,Introduction to Bayesian Models.pdf,CSCI_83,32,"Example using Conjugate Distribution
Consider the product of a Binomial likelihood and a Beta prior
Deï¬ne the evidence as  trials with  successes
Prior is a Beta distribution with parameters  and , or the vector 
From Bayes Theorem the distribution of the posterior:
ğ‘› ğ‘§
ğ‘ ğ‘ ğœƒ=(ğ‘,ğ‘)
ğ‘ğ‘œğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘œğ‘Ÿ(ğœƒ|ğ‘§,ğ‘›)
ğ‘(ğœƒ|ğ‘§,ğ‘›)
=ğ‘™ğ‘–ğ‘˜ğ‘’ğ‘™ğ‘–â„ ğ‘œğ‘œğ‘‘(ğ‘§,ğ‘›|ğœƒ)Â ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ(ğœƒ)ğ‘‘ğ‘ğ‘¡ğ‘Â ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘§,ğ‘›)
=ğµğ‘–ğ‘›ğ‘œğ‘šğ‘–ğ‘ğ‘™(ğ‘§,ğ‘›|ğœƒ)Â ğµğ‘’ğ‘¡ğ‘(ğœƒ)ğ‘(ğ‘§,ğ‘›)=ğµğ‘’ğ‘¡ğ‘(ğ‘§+ğ‘,Â ğ‘›âˆ’ğ‘§+ğ‘)",407,semantic
47aeedd0-643a-42cd-aeaa-1c3d5b19201c,Introduction to Bayesian Models.pdf,CSCI_83,33,"Example using Conjugate Distribution
There are some useful insights you can gain from this relationship for (discrete) integer counts:
Posterior distribution is in the Beta family, as a result of conjugacy
Parameters  and  are determined by the prior and the evidence
Parameters of the prior can be interpreted as pseudo counts of successes,  and failures, 
Be careful when creating a prior to add 1 to the successes and failures
The larger the total pseudo counts, , the stronger the prior information
-Evidence is also in the form (actual) counts of successes,  and failure, - The more evidence the greater the inï¬‚uence on the posterior distribution- Large amount of evidence will overwhelm the prior- With large amount of evidence, posterior converges to frequentist model
ğ‘ğ‘œğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘œğ‘Ÿ(ğœƒ|ğ‘§,ğ‘›)=ğµğ‘’ğ‘¡ğ‘(ğ‘§+ğ‘,Â ğ‘›âˆ’ğ‘§+ğ‘)
ğ‘ ğ‘
ğ‘=ğ‘ğ‘ ğ‘’ğ‘¢ğ‘‘ğ‘œÂ ğ‘ ğ‘¢ğ‘ğ‘ğ‘’ğ‘ ğ‘ +1 ğ‘=ğ‘ğ‘ ğ‘’ğ‘¢ğ‘‘ğ‘œÂ ğ‘“ğ‘ğ‘–ğ‘™ğ‘¢ğ‘Ÿğ‘’+1
ğ‘+ğ‘
ğ‘§ ğ‘›âˆ’ğ‘§",861,semantic
ab619937-84c7-473f-a378-9a76c89cbf35,Introduction to Bayesian Models.pdf,CSCI_83,34,"Example using Conjugate Distribution
Consider example with:- Prior pseudo counts , successes  and failures, - Evidence, successes  and failures, - Posterior is 
Prior, likelihood and posterior for distracted driving
[1,9] ğ‘=1+1 ğ‘=9+1=10 =30ğµğ‘’ğ‘¡ğ‘(10+2,Â 40âˆ’10+10)=ğµğ‘’ğ‘¡ğ‘(12,Â 40)",273,semantic
b262b7e3-f33f-4484-81cd-3e71990f4212,Introduction to Bayesian Models.pdf,CSCI_83,35,"Sampling the Posterior
How can we ï¬nd an estimate of the poster distribution? 1. We can sample from the analytic solution - if we have a conjugate
2.",149,semantic
ebda028a-14f6-46c7-a279-7582ec1f812e,Introduction to Bayesian Models.pdf,CSCI_83,35,"We can sample the likelihood and prior, take the product and normalize - for any posterior
3. Grid sample or Markov chain Monte Carlo (MCMC) sample",147,semantic
f4eca257-9e3a-46e2-a3e2-a64869618399,Introduction to Bayesian Models.pdf,CSCI_83,36,"Sampling the Posterior
Grid sampling is a naive approach
Compute the probability at each point on a regular gird
Sample over range of interesting values for variables
Posterior if conjugate prior
Prior and likelihood
In principle can work for any number of dimensions
In 1-dimension is just regularly spaced points on a line
Poor scaling to higher dimensions
Sampling grid for bivariate distribution",399,semantic
220d8ec9-d8fb-4421-a28c-18dfce22462b,Introduction to Bayesian Models.pdf,CSCI_83,37,"Sampling the Posterior
Algorithm for grid sampling to compute posterior from likelihood and prior
Procedure CreateGrid(variables, lower_limits, upper_limits): 
    # Build the sampling grid     return sampling_grid       
Procedure SampleLikelihood(sampling_value, observation_values):        return likelihood_function(sampling_value, observation_values)    Procedure Prior(sampling_values, prior_parameter_value):    
    return prior_density_function(sampling_value, prior_parameter_values)        ComputePosterior(variables, lower_limits, upper_limits):    
    # Initialize the sampling grid    Grid = CreateGrid(variables, lower_limits, upper_limits)        # Initialize array to hold sampled posterior values       
    array posterior[range(Grid)]
    # Compute posterior at each sampling value in the grid      for sampling_value in range(lower_limits, upper_limits):   
        likelihood = SampleLikelihood(sampling_value, observation_values)        prior = Prior(sampling_values, prior_parameter_value)           posterior[sampling_value] = likelihood * prior
    # Normalize the posterior           probability_data = sum(posterior[range(Grid)])    posterior = posterior[range(Grid)]/probability_data 
    return posterior",1235,semantic
dea429d1-2c16-4ea8-9769-8de6377d7248,Introduction to Bayesian Models.pdf,CSCI_83,38,"Credible Intervals
How can we specify the uncertainty for a Bayesian parameter estimate? For frequentist analysis we use conï¬dence intervals, but not entirely appropriate
Conï¬dence intervals are based on a sampling distribution
The upper and lower conï¬dence intervals quantiles of the sampling distribution
Bayesian analysis has no sampling distribution uses a prior distribution and likelihood
For Bayesian analysis inference performed on posterior distribution
We use a concept known as the credible interval
A credible interval is an interval on the Bayesian posterior distribution with the highest  proportion of posterior probabilityğ›¼",639,semantic
cf9692d5-4db6-418b-8a32-6cd0f0aed685,Introduction to Bayesian Models.pdf,CSCI_83,39,"Credible Intervals
How can we specify the uncertainty for a Bayesian parameter estimate? Example, the  credible interval encompasses the 90% of the posterior distribution with the highest density
The credible interval is sometime called the highest density interval (HDI), or highest posterior density interval (HPDI)
These names make sense, since we seek the the densest posterior interval containing  probability
For symmetric distributions the credible interval can be numerically the same as the conï¬dence interval
In general, these two quantities can be quite different
ğ›¼=0.90
ğ›¼",583,semantic
a0f6211f-aa8c-40c5-bba7-945735709e00,Introduction to Bayesian Models.pdf,CSCI_83,40,"Credible Intervals
What are the 95% credible intervals for ? Probability of distract drivers for next 10 cars
ğµğ‘’ğ‘¡ğ‘(12,Â 40)",122,semantic
c554fafb-1605-4d56-818c-984da49a35b2,Introduction to Bayesian Models.pdf,CSCI_83,41,"Credible Intervals are not Conï¬dence Intervals
How are credible intervals different from the more familiar conï¬dence intervals? Conï¬dence intervials and credible intervals are conceptually quite different
A conï¬dence interval is a purely frequentest concept- Is an interval on the sampling distribution where repeated samples of a statistic are expected with probability - Cannot interpret a conï¬dence interval as an interval on a probability distribution of the value of a statistic! Credible interval is an interval on a posterior distribution of the statistic- Credible interval is exactly what the misinterpretation of the conï¬dence interval tries to be- Credible interval is the interval with highest  probability for the statistic being estimated
For symmetric posterior distributions, the credible interval will be numerically the same as the conï¬dence interval- This need not be the case in general
=ğ›¼
ğ›¼",911,semantic
0903d84f-438a-4af5-ac2b-103dbe69329f,Introduction to Bayesian Models.pdf,CSCI_83,42,"Credible Intervals are not Conï¬dence Intervals
Compare conï¬dence interval and credible interval for the case of 10 observations
Credible intervals cross the density function at exactly the same density
Conï¬dence intervals have the same CDF in the tails beyond the interval
Difference between credible and conï¬dence intervals",324,semantic
2303ed96-5f1f-46db-9b4c-4559b7032a68,Introduction to Bayesian Models.pdf,CSCI_83,43,"Simulating from the posterior distribution: predictions
What else can we do with a Bayesian posterior distribution beyond credible intervals? Perform simulations and make predictions
Predictions are computed by simulating from the posterior distribution
Results of these simulations are useful for several purposes, including:
Predicting posterior values
Model checking by comparing simulation results agree (or not) with observations",434,semantic
63a14af2-2c11-4456-a9fd-8f339d9a3f67,Introduction to Bayesian Models.pdf,CSCI_83,44,"Simulating from the posterior distribution: predictions
Example; What are the probabilities of distracted drivers for the next 10 cars with posterior, ? Probability of distract drivers for next 10 cars
ğµğ‘’ğ‘¡ğ‘(12,Â 40)",214,semantic
fffc35d2-b684-45a7-99e8-129d255874f8,Introduction to Bayesian Models.pdf,CSCI_83,45,"Summary
Bayesian analysis is a contrast to frequentist methods
The objective of Bayesian analysis is to compute a posterior distribution
Contrasts with frequentist statistics is to compute a point estimate and conï¬dence interval from a sample
Bayesian models allow expressing prior information in the form of a prior distribution
Selection of prior distributions can be performed in a number of ways
The posterior distribution is said to quantify our current belief
We update beliefs based on additional data or evidence
A critical difference with frequentist models which must be computed from a complete sample
Inference can be performed on the posterior distribution by ï¬nding the maximum a postiori (MAP) value and a credible interval
Predictions are made by simulating from the posterior distribution a",807,semantic
229845e7-5a5a-4ad9-9d6c-465d5c7ea4cd,Introduction to Bayesian Models.pdf,CSCI_83,46,"Summary
Bayesian analysis is in contrast to frequentist methods
The objective of Bayesian analysis is to compute a posterior distribution
Frequentist statistics seeks to compute a point estimate and conï¬dence interval from a sample
Bayesian models allow expressing prior information in the form of a prior distribution
Selection of prior distributions can be performed in a number of ways
The posterior distribution is said to quantify our current belief
We update beliefs based on additional data or evidence
A critical difference with frequentist models which must be computed from a complete sample
Inference can be performed on the posterior distribution by ï¬nding the maximum a postiori (MAP) value and a credible interval",727,semantic
3165b24f-be1a-4000-b3c4-70dbfe6775e7,Parameter Estimation and Likelihood.pdf,CSCI_83,0,"Parameter Estimation and Likelihood
Steve Elston
09/29/2022",59,semantic
f6779f9d-2f1c-4faa-ba18-2fd09201b3bb,Parameter Estimation and Likelihood.pdf,CSCI_83,1,"Review
Sampling is a fundamental process in the collection and analysis of data
Sampling is important because we almost never have data on a whole population
Sampling must be randomized to preclude biases
As sample size increases the standard error of a statistic computed from the sample decreases by the law of large numbers
Key points to keep in mind:
Understanding sampling is essential to ensure data is representative of the entire population
Use inferences on the sample to say something about the population
The sample must be randomly drawn from the population
Sampling from distribution is the building block of simulation",632,semantic
6ce20741-3218-49a1-bf9f-4a8dd4142a04,Parameter Estimation and Likelihood.pdf,CSCI_83,2,"Review
Sampling of a population is done from an unknown population distribution, 
Any statistic we compute for the generating process is an approximation for the population, 
Sampling distribution of unknown population parameter
 
ğ‘ () Ì‚",236,semantic
fb073469-aef6-4b64-90d0-a14792a46390,Parameter Estimation and Likelihood.pdf,CSCI_83,3,"Review
The law of large numbers is a theorem that states that statistics of independent random samples converge to the population valuesas more samples are used
The law of large numbers is foundational to statistics and sampling theory
Assume that larger samples are more representatives of the population we are sampling
If the real world did not follow this theorem much of statistics, to say nothing of science and technology, would fail badly.",447,semantic
d881b353-191a-4e11-b595-77f9ac7e6243,Parameter Estimation and Likelihood.pdf,CSCI_83,4,"Introduction
The concept of likelihood and maximum likelihood estimation (MLE) have been at the core of much of statistical modeling for about 100 years
In 21st Century, likelihood and MLE ideas continue to be foundational
Understanding the concept of likelihood and the use of MLE methods is key to understanding many parametric statistical methods
Likelihood is a measure of how like a parametric model is to generate the observed data
MLE is a generic methods for parameter estimation
MLE used widely for machine learning models, including some deep learning models",568,semantic
16745d58-a49a-4b95-8e53-55b172f7368f,Parameter Estimation and Likelihood.pdf,CSCI_83,5,"Likelihood and Density Functions
Likelihood is a measure of how like a parametric model is to generate the observed data
Start with a data sample, 
Likelihood of sample from a generating process with a parametric density function, 
 can be either a probability density function (PDF), for continuous distributions, or a probability mass function (PMF), for discrete distributions
The distribution parameter vector, , is ï¬xed
Now, for each observation, , in , the density is just 
ğ—=[ , ,â€¦, ]ğ‘¥1ğ‘¥2 ğ‘¥ğ‘›
ğ‘“(ğ—|ğœƒ)
ğ‘“(ğ—Â |Â ğœƒ)
ğœƒ
ğ‘¥ğ‘– ğ—= , ,â€¦,ğ‘¥1ğ‘¥2 ğ‘¥ğ‘› ğ‘“(|Â ğœƒ)ğ‘¥ğ‘–",544,semantic
7a78812a-f0c3-4864-874d-91682f37e262,Parameter Estimation and Likelihood.pdf,CSCI_83,6,"Likelihood
Likelihood is a measure of how like a parametric model is to generate the observed data
Likelihood of sample from a generating process with a parametric probability density, 
For the set of observations, , the likelihood is the product of the densities:
In most practical cases, we work with the log likelihood, for observations, , the log likelihood is expressed:
ğ‘“(ğ—|ğœƒ)
ğ—=[ , ,â€¦, ]ğ‘¥1ğ‘¥2 ğ‘¥ğ‘›
 (ğ—|Â ğœƒ)= ğ‘“(|ğœƒ)âˆğ‘–=1
ğ‘›
ğ‘¥ğ‘–
ğ—= , ,â€¦,ğ‘¥1ğ‘¥2 ğ‘¥ğ‘›
ğ‘™(ğ—|Â ğœƒ)=ğ‘™ğ‘œğ‘”( (ğ—|Â ğœƒ))= ğ‘™ğ‘œğ‘”(ğ‘“(Â |Â ğœƒ))âˆ‘ğ‘–=1
ğ‘›
ğ‘¥ğ‘–",485,semantic
c7bf3ffd-65a4-44fa-91fb-b1b22cd856d2,Parameter Estimation and Likelihood.pdf,CSCI_83,7,"Likelihood
Likelihood is a measure of how likely a parametric model is to generate the observed data sample
In most practical cases, we work with the log likelihood, for observations, , the log likelihood is expressed:
Use log-likelihood to work with the sum of log probabilities rather than the product
If the probabilities are small, the sum is numerically stable
The product of many small numbers is a very small number, which can lead to numerical underï¬‚ow even for 64 or 128 bit ï¬‚oating point arithmetic
ğ—= , ,â€¦,ğ‘¥1ğ‘¥2 ğ‘¥ğ‘›
ğ‘™(ğ—|Â ğœƒ)=ğ‘™ğ‘œğ‘”( (ğ—|Â ğœƒ))= ğ‘™ğ‘œğ‘”(ğ‘“(Â |Â ğœƒ))âˆ‘ğ‘–=1
ğ‘›
ğ‘¥ğ‘–",568,semantic
1ae3731b-513d-4dad-91b8-a862b500bc61,Parameter Estimation and Likelihood.pdf,CSCI_83,8,"Likelihood Example
Likelihood is a measure of how likely a parametric model is to generate the observed data sample
Binomial likelihood for sample sizes  with 
Notice the variability in the likelihood curve for smaller samples
Likelihood has stronger curvature for larger samples - less uncertainty for maximum
Binomial likelihood at different sample sizes
[25,50,100] ğ‘=0.5",374,semantic
8a62a87e-8ba9-4fa5-b76a-1f011ca72bbf,Parameter Estimation and Likelihood.pdf,CSCI_83,9,"Example: The Normal likelihood
The univariate Normal probability density function with parameter vector  for a single observation, :
For n observations, , the likelihood is the product of the densities:
The log-likelihood (log of above) is a lot easiter to deal with:
- The log-likelihood is a function of the parameters, 
ğœƒ=(ğœ‡,ğœ) ğ‘¥ğ‘–
ğ‘“(ğ‘¥Â |Â ğœ‡, )=âˆ’ ğ‘’ğ‘¥ğ‘[âˆ’ ( âˆ’ğœ‡]ğœ2 1(2ğœ‹ğœ2)1/2 12ğœ2ğ‘¥ğ‘– )2
ğ—= , ,â€¦,ğ‘¥1ğ‘¥2 ğ‘¥ğ‘›
ğ‘™(ğ—|Â ğœƒ)=âˆ’ ğ‘’ğ‘¥ğ‘[âˆ’ ( âˆ’ğœ‡]ğ‘›(2ğœ‹ğœ2)1/2 12ğœ2âˆğ‘–=1
ğ‘›
ğ‘¥ğ‘– )2
ğ‘™(ğ—Â |Â ğœ‡,ğœ)=âˆ’ğ‘™ğ‘œğ‘”(2ğœ‹ )âˆ’ ( âˆ’ğœ‡ğ‘›2 ğœ2 12ğœ2âˆ‘ğ‘–=1
ğ‘›
ğ‘¥ğ‘– )2
(ğœ‡,ğœ)",501,semantic
7e966bb1-6d1e-45a0-b292-0381fee3413d,Parameter Estimation and Likelihood.pdf,CSCI_83,10,"Example: The Normal likelihood
An example to illustrate the foregoing concepts
Plot the likelihood for 5, 10 and 20 samples from a standard Normal distribution
Vary the parameter , and assume the parameter  is ï¬xed and known. The steps are:
A random sample is drawn from a standard Normal distribution
For the random sample the log-likelihood is computed at each location parameter value
Notice that as the number of observations increases so does the curvature of the likelihood. Normal likelihood at different sample sizes
ğœ‡ ğœ",528,semantic
4cf31d1a-95d8-489b-9457-572656157c1a,Parameter Estimation and Likelihood.pdf,CSCI_83,12,"Example: Binomial Likelihood
Example of log-likelihood for the Binomial distribution
Binomial distribution models discrete events
Range of the single parameter, , restricted to the range 
Binomial distribution has the following probability mass function (PMF) for  successes in  trials:
Log-likelihood is easily found:
Binomial log-likelihood has a strong dependence on both the sample size,  and the number of successes, 
ğœ‹ 0â‰¤ğœ‹â‰¤1
ğ‘˜ ğ‘›
ğ‘“(ğ‘˜,ğ‘›Â |Â ğœ‹)=()(1âˆ’ğœ‹ğ‘›ğ‘¦ğœ‹ğ‘˜ )ğ‘›âˆ’ğ‘˜
ğ‘™(ğ‘˜,ğ‘›Â |Â ğœ‹)=ğ‘™ğ‘œğ‘”()+ğ‘˜Â ğ‘™ğ‘œğ‘”(ğœ‹)+(ğ‘›âˆ’ğ‘˜)Â ğ‘™ğ‘œğ‘”(1âˆ’ğœ‹)ğ‘›ğ‘˜
ğ‘› ğ‘˜",508,semantic
e366bf6f-22f1-489a-b139-0bc1825090ab,Parameter Estimation and Likelihood.pdf,CSCI_83,13,"The Maximum Likehihood Estimator
Maximum likelihood estimator (MLE) is a foundational tool for much of statistical inference and machine learning
Given a log-likelihood function, ï¬nd the model parameters which maximize it
Further, knowing the distribution allows us to quantify the uncertainty of the MLE parameter estimates
The model parameter estimates found by MLE is Normal for large samples, a remarkable property
The MLE is a point estimator
An estimate of a single parameter value, or point value, with the highest likelihood",532,semantic
8346ae2f-4d39-41ef-be4a-0dc22a997113,Parameter Estimation and Likelihood.pdf,CSCI_83,14,"The Maximum Likehihood Estimator
The maximum likelihood for the model parameters is achieved when two conditions are met:
Interpret these two conditions:
First derivative of log-likelihood function, or slope, is 0 at either maximum or minimum points
In general,  is a vector of model parameters
Partial derivatives of log-likelihood are a vector - the gradient with respect to the model parameters
Gradient of the log-likelihood are known as the score function
The second derivatives of the log-likelihood indicates the curvature
Maximum has negative curvature
Minimum has positive curvature
=0âˆ‚Â ğ‘™(ğ—Â |Â ğœƒ))âˆ‚ğœƒ
<0Â ğ‘™(ğ—Â |Â ğœƒ))âˆ‚2
âˆ‚ğœƒ2
ğœƒâƒ—",629,semantic
ee7e43c7-1138-4f00-992e-0373f607c3ee,Parameter Estimation and Likelihood.pdf,CSCI_83,15,"Fisher information and properties of MLE
The maximum likelihood estimator has useful and desirable properties
Start with a matrix of second partial derivatives of the log-likelihood function
Matrix is the observed information matrix of the model, . Useful properties of the information matrix
The more negative the values of the second partial derivatives, the greater the curvature of the log-likelihood
log-likelihood likelihood function with more negative values has limbs
Narrow peak implies that the information on parameter values is high
The matrix is symmetric, or information is symmetric around the maximum likelihood point
 ()ğœƒâƒ—Â 
 ()=ğœƒâƒ—Â 
â›
â
âœâœâœâœâœâœâœ
ğ‘™(ğ—Â |Â ğœƒ)âˆ‚2
âˆ‚ğœƒ21
ğ‘™(ğ—Â |Â ğœƒ)âˆ‚2
âˆ‚Â âˆ‚ğœƒ1 ğœƒ2
â‹® 
ğ‘™(ğ—Â |Â ğœƒ)âˆ‚2
âˆ‚Â âˆ‚ğœƒğ‘› ğœƒ1
ğ‘™(ğ—Â |Â ğœƒ)âˆ‚2
âˆ‚Â âˆ‚ğœƒ2 ğœƒ1
ğ‘™(ğ—Â |Â ğœƒ)âˆ‚2
âˆ‚ğœƒ22
â‹® 
ğ‘™(ğ—Â |Â ğœƒ)âˆ‚2
âˆ‚Â âˆ‚ğœƒğ‘› ğœƒ2
â€¦
â€¦
â‹® 
â€¦
ğ‘™(ğ—Â |Â ğœƒ)âˆ‚2
âˆ‚Â âˆ‚ğœƒğ‘› ğœƒ1
ğ‘™(ğ—Â |Â ğœƒ)âˆ‚2
âˆ‚Â âˆ‚ğœƒ2 ğœƒğ‘›
â‹® 
ğ‘™(ğ—Â |Â ğœƒ)âˆ‚2
âˆ‚ğœƒ2ğ‘›
â
â 
âŸâŸâŸâŸâŸâŸâŸ",858,semantic
cdb64b1e-abb0-4c1a-896b-cbeef6393356,Parameter Estimation and Likelihood.pdf,CSCI_83,16,"Fisher information and properties of MLE
Can one consider the information of the MLE before sampling data or performing an experiment? Can use the expected information or Fisher information
Fisher information is the expectation over the second derivative of the observed information
 ()=âˆ’ğ„{ ()}=âˆ’ğ„{ }ğœƒâƒ—Â  ğœƒâƒ—Â  Â ğ‘™(ğ—Â |Â ğœƒ)âˆ‚2
âˆ‚ğœƒ2",323,semantic
37caca8f-91f3-4766-a33e-2e86288b93a3,Parameter Estimation and Likelihood.pdf,CSCI_83,17,"Fisher information and properties of MLE
Fisher information leads to an important relationship
The MLE parameter estimate  is a Normally distributed random variable
Arises from the Taylor expansion of the maximum likelihood estimator
or
: ï¬rst partial derivative given observations 
: second partial derivative given observations 
ğœƒÌ‚Â 
0= = + (âˆ’ğœƒ)âˆ‚Â ğ‘™(ğ—Â |Â ğœƒ)âˆ‚ğœƒ âˆ‚Â ğ‘™(ğœƒ)ğ—
âˆ‚ğœƒ Â ğ‘™(âˆ‚2 ğœƒÌ‚Â )ğ—
âˆ‚ğœƒ2 ğœƒÌ‚Â 
0=(ğœƒ +( (âˆ’ğœƒ)ğ‘™â€² )ğ— ğ‘™â€³ğœƒÌ‚Â )ğ—ğœƒÌ‚Â 
(ğœƒğ‘™â€² )ğ— ğ—
(ğ‘™â€³ğœƒÌ‚Â )ğ— ğ—",439,semantic
974021fb-9292-472a-adbf-d3e818842aae,Parameter Estimation and Likelihood.pdf,CSCI_83,18,"Fisher information and properties of MLE
Continuing with the simpliï¬ed notation, and solving for ;
Fisher information relates to the score function as the inverse variance
For large sample, , take the expectation over all samples, . Assuming ï¬rst and second derivatives exist and continuous, then by the Central LimitTheorem:
ğœƒÌ‚Â 
=ğœƒ+ğœƒÌ‚Â  (ğœƒ /ğ‘›ğ‘™â€² )ğ—
âˆ’( /ğ‘›ğ‘™â€³ğœƒÌ‚Â )ğ—
Â Â  (0,1/ )âˆ‚Â ğ‘™(ğœƒ)ğ—
âˆ‚ğœƒ âˆ¼ Ë™  ğœƒ
ğ‘›â†’âˆ ğ—
 (ğœƒ, )ğœƒÌ‚Â âˆ¼ Ë™ 1ğ‘› (ğœƒ)",414,semantic
1b6a0991-fa70-47b5-8b09-0b53d5373cf8,Parameter Estimation and Likelihood.pdf,CSCI_83,19,"Fisher information and properties of MLE
Relationship shows several important properties
The maximum likelihood estimate of model parameters, , is Normally distributed
The larger the Fisher information, the lower the variance of the parameter estimate
Greater curvature of the log likelihood function gives more certain the parameter estimates
The variance of the parameter estimate is inversely proportional to the number of samples, .  (0,1/ )âˆ‚Â ğ‘™(ğœƒ)âˆ‚ğœƒ âˆ¼ Ë™  ğœƒ
ğœƒÌ‚Â 
ğ‘›",466,semantic
d30e1b02-d245-4aca-8664-dfd0d28380aa,Parameter Estimation and Likelihood.pdf,CSCI_83,20,"Example of MLE for Normal distribution
MLE for the Normal distribution
Find derivatives of the log-likelihood function with respect to the model parameters,  and 
Solving above for the estimate of the mean, 
ğœ‡ ğœ2
( )=( )=()
âˆ‚ğ‘™âˆ‚ğœ‡âˆ‚ğ‘™âˆ‚ğœ2
( âˆ’ğœ‡)1ğœ2 âˆ‘ğ‘—ğ‘¥ğ‘—
âˆ’ + ( âˆ’ğœ‡ğ‘›2ğœ2 12ğœ4 âˆ‘ğ‘—ğ‘¥ğ‘— )2
00
ğ‘¥Â¯
ğ‘›( âˆ’ğœ‡)=0âˆ‘ğ‘—=1
ğ‘¥ğ‘—
â†’ğœ‡:Â ğ‘ğ‘¡Â ğ‘ğ‘œğ‘›ğ‘£ğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘›ğ‘ğ‘’ğ‘¥Â¯
=ğ‘¥Â¯ 1ğ‘›âˆ‘ğ‘—=1
ğ‘¥ğ‘—",330,semantic
91b2b7dd-f6d7-45a2-ada3-8a662c1e3264,Parameter Estimation and Likelihood.pdf,CSCI_83,21,"Example of MLE for Normal distribution
MLE for the Normal distribution
Find the maximum likelihood estimate of 
( )=( )=()
âˆ‚ğ‘™âˆ‚ğœ‡âˆ‚ğ‘™âˆ‚ğœ2
( âˆ’ğœ‡)1ğœ2 âˆ‘ğ‘—ğ‘¥ğ‘—
âˆ’ + ( âˆ’ğœ‡ğ‘›2ğœ2 12ğœ4 âˆ‘ğ‘—ğ‘¥ğ‘— )2
00
ğœ2
( âˆ’ğœ‡ =ğ‘›1ğ‘ 2âˆ‘ğ‘—=1
ğ‘¥ğ‘— )2
â†’ğœ‡:Â ğ‘ğ‘¡Â ğ‘ğ‘œğ‘›ğ‘£ğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘›ğ‘ğ‘’ğ‘¥Â¯
= ( âˆ’ğ‘ 2 1ğ‘›âˆ‘ğ‘—=1
ğ‘›
ğ‘¥ğ‘— ğ‘¥Â¯)2",245,semantic
73d9720b-1c74-464b-9e2e-0f6b716988ae,Parameter Estimation and Likelihood.pdf,CSCI_83,22,"Example of MLE for Normal distribution
The simpliï¬cation results from the fact that  and  in the limit of a large sample from the law of large numbers. There are some aspects of these relationships which make the MLE method attractive:
The curvature of the MLE for both parameters increases with the number of samples 
The peak of the log-likelihood function becomes better deï¬ned as  increases. The maximum likelihood estimates of the parameters,  and  are independent. The off-diagonal terms are .",499,semantic
97ced58c-8b33-42a5-a047-7e880cd393c0,Parameter Estimation and Likelihood.pdf,CSCI_83,22," ()ğœƒâƒ—Â =âˆ’ğ„â›
â
âœâœ
ğ‘™âˆ‚2
âˆ‚ğœ‡2
ğ‘™âˆ‚2
âˆ‚ğœ‡Â âˆ‚ğœ2
ğ‘™âˆ‚2
âˆ‚ğœ‡Â âˆ‚ğœ2
ğ‘™âˆ‚2
âˆ‚(ğœ2)2
â
â 
âŸâŸ
=âˆ’ğ„( )
âˆ’ğ‘›ğœ2
âˆ’(âˆ’ğœ‡)ğ‘›ğœ4 ğ‘¥Â¯
âˆ’(âˆ’ğœ‡)ğ‘›ğœ4 ğ‘¥Â¯
âˆ’ + ( âˆ’ğœ‡ğ‘›2ğœ4 1ğœ6 âˆ‘ğ‘—ğ‘¥ğ‘— )2
=( )
ğ‘›ğœ2
0
0
ğ‘›2ğœ4
â†’ğ„()=ğœ‡ğ‘¥Â¯ ğ±ğ£ â†’ğ„{( âˆ’ğœ‡}=ğ‘ 2 ğ‘¥ğ‘— )2 ğœ2
ğ‘›
ğ‘›
ğœ‡ ğœ2 0",186,semantic
d7d59354-98ae-4881-8ec1-d293ff07947b,Parameter Estimation and Likelihood.pdf,CSCI_83,23,"Fisher information and properties of MLE
Example of Fisher Information for Normal distribution
As sample size increases, Fisher Information decreases
Variance of MLE decreases with decreasing Fisher Information
Fisher Information for Normal distribution at different sample sizes",279,semantic
2534ace2-0cb2-4d67-9f3f-0766eef04a3c,Parameter Estimation and Likelihood.pdf,CSCI_83,25,"Finding Solutions Without a Closed Form
How do we generalize the MLE method beyond cases with a closed form solution
For example, logistic regression has a nonlinear log likelihood function
An approximate solution can be found by numerical optimization methods or root ï¬nding methods
Two widely used methods
Gradient descent, a ï¬rst-order method
Newtonâ€™s method, quadratic approximation",386,semantic
51572586-f670-4ecb-9412-488c4b07ceb6,Parameter Estimation and Likelihood.pdf,CSCI_83,26,"Gradient descent methods
The gradient descent method ï¬nds the maximum of the log-likelihood function by following the gradient â€˜uphillâ€™
Start with the gradient of the log-likelihood function with respect to the parameters, 
The gradient is the vector of partial derivatives with respect to each of the parameters
Given a current parameter estimate vector at step n , , the improved parameter estimate vector, , is found:
ğœƒ
ğ‘”ğ‘Ÿğ‘ğ‘‘(ğ‘™())= Â ğ‘™()=ğœƒâƒ—Â  âˆ‡ ğœƒ ğœƒâƒ—Â 
â›
â
âœâœâœâœâœâœ
âˆ‚ğ‘™(ğ—Â |Â ğœƒ)âˆ‚ğœƒ1
âˆ‚ğ‘™(ğ—Â |Â ğœƒ)âˆ‚ğœƒ2
â‹® âˆ‚ğ‘™(ğ—Â |Â ğœƒ)âˆ‚ğœƒğ‘›
â
â 
âŸâŸâŸâŸâŸâŸ
ğœƒÌ‚Â ğ‘› ğœƒÌ‚Â ğ‘›+1
= +ğ›¾Â  Â ğ‘™()ğœƒÌ‚Â ğ‘›+1 ğœƒÌ‚Â ğ‘› âˆ‡ ğœƒ ğœƒÌ‚",554,semantic
510dbd3d-90dc-4cee-a72f-e474b737a348,Parameter Estimation and Likelihood.pdf,CSCI_83,27,"Gradient descent methods
The gradient descent method ï¬nds the maximum of the log-likelihood function by following the gradient â€˜uphillâ€™
Example: contour plot of Normal log-likelihood
Gradient descent following negative (minus) gradient to maximum of log-likelihood
Maximum gradient direction is perpendicular to coutour lines - steepest uphill path
Notice difference between gradients (and information) between locaiton and scale!",430,semantic
a746dde1-c247-4ca4-b94a-7b269adacc21,Parameter Estimation and Likelihood.pdf,CSCI_83,28,Contours of Normal Log-Likelihood,33,semantic
5344fa23-91ab-4516-a7c7-fbca99003203,Parameter Estimation and Likelihood.pdf,CSCI_83,29,"Gradient descent methods
The gradient descent method ï¬nds the maximum of the log-likelihood function by following the gradient â€˜uphillâ€™
Given a current parameter estimate vector at step n , , the improved parameter estimate vector, , is found:
The hyperparameter  is the learning rate or step size
Determining a learning rate can have a signiï¬cant effect on the performance of the gradient
This hyperparameter can be chosen manually, often with by a search of the hyperparameter space
Using a ï¬xed  is far from optimal
The gradient changes toward the maximum point the optimal step size changes
More sophisticated algorithms use an adaptive method to determine an optimal step at each step
Adaptive algorithm ï¬nds step size dynamically using a line search procedure
Algorithm converges when the norm of the gradient is approximately 0
This is the stopping condition
Express this condition as
ğœƒÌ‚Â ğ‘› ğœƒÌ‚Â ğ‘›+1
= +ğ›¾Â  Â ğ‘™()ğœƒÌ‚Â ğ‘›+1 ğœƒÌ‚Â ğ‘› âˆ‡ ğœƒ ğœƒÌ‚Â 
ğ›¾
ğ›¾
|| Â ğ‘™()||â‰¤ğ‘¡ğ‘œğ‘™ğ‘™ğ‘’ğ‘Ÿğ‘ğ‘›ğ‘ğ‘’âˆ‡ ğœƒ ğœƒâƒ—",964,semantic
cf623280-6be1-408c-b881-a0f83e15e2e6,Parameter Estimation and Likelihood.pdf,CSCI_83,30,"Gradient descent methods
Gradient with respect to model parameters is computed for each dimension of the model parameter vector
proceedure compute_expected_grad(parameters, cases):     for each dimension:      grad[dimension] = grad(parameters, cases)  return grad
The forgoing procedure can be vectorized or parallelized",321,semantic
5f1c7ae3-a010-4c46-b788-e30a97507d7c,Parameter Estimation and Likelihood.pdf,CSCI_83,31,"Gradient descent methods
The gradient descent method ï¬nds the maximum of the log-likelihood function by following the gradient â€˜uphillâ€™
The gradient is always perpendicular to the contours
Gradient descent on countour plot of log-likelihood",240,semantic
c2ea15f3-e96a-414c-a189-f9c91bdc1209,Parameter Estimation and Likelihood.pdf,CSCI_83,32,"Stochastic gadient descent (SGD)
The simple gradient descent algorithm and Newtonâ€™s method have limited scalability
Basic algorithms require computing and summing the entire gradient vector
Calculation must be done as a single batch in memory, or batch gradient descent
Computing the full gradient at each step limits scalability
Stochastic gradient decent (SGD) algorithm computes the approximate expected gradient using a mini-batch
The mini-batch is a limited-size Bernoulli sample from the full set of cases
These gradient approximations are inherently noisy or stochastic, giving rise to the methodâ€™s name
Using mini-batches greatly increases scalability
While gradient estimates are less accurate, these estimates can be computed very quickly
SGD is highly scalable and the workhorse of many large-scale statistical methods
Mini-batch optimization is often referred to as online optimization since updates as cases arrive.",928,semantic
cf89f6e8-3015-4d69-b6b5-783a6bdff017,Parameter Estimation and Likelihood.pdf,CSCI_83,33,"Stochastic gadient descent (SGD)
The basic idea of stochastic optimization is using a Bernoulli random sample of the data to estimate the expected update of the model weights
where,  is the expected value of the gradient given the Bernoulli sample of the data . Choosing batch size can require some tuning
If the batch is too small, the gradient estimate will be and, hardware resources may not be fully utilized
Large batches require signiï¬cant memory and slow the calculation
Empirically, SGD has good convergence properties
This behavior arises since stochastic gradient samples provide a better exploration of the loss function space
For very large data sets, the SGD algorithm often converges before the ï¬rst pass through the data is completed
= +ğ›¾Â  [ ğ½()]ğœƒğ‘¡+1 ğœƒğ‘¡ ğ¸ğ‘‘ğ‘ğ‘¡ğ‘ğ‘Ì‚Â  âˆ‡ ğœƒ ğœƒğ‘¡
[]ğ¸ğ‘‘ğ‘ğ‘¡ğ‘ğ‘Ì‚Â  ğ‘‘ğ‘ğ‘¡ğ‘ğ‘Ì‚",802,semantic
f00e284d-2a1b-4291-8582-8cf262eabeca,Parameter Estimation and Likelihood.pdf,CSCI_83,34,"Stochastic gadient descent (SGD)
The pseudo code for the SGD algorithm is:
define learning_rateproceedure update_weights(weights, grad):     weights = weights + learning_rate * grad   return weights
Random_sort(cases)          while(grad > stopping_criteria):          mini-batch = sample_next_n(cases)         grad = compute_expected_grad(weights, mini_batch)          weights = update_weights(weights, grad)`   
If the sampling continues for more than one cycle through the cases, the samples are biased
In practice, this small bias does not seem to mater much",562,semantic
0ed38f31-d6b5-4a38-9e6a-6352cd1250a2,Parameter Estimation and Likelihood.pdf,CSCI_83,35,"Newtonâ€™s method
Newtonâ€™s method, and related methods, employ a quadratic approximation to optimization. For MLE, Newtonâ€™s method uses both the ï¬rst andsecond derivatives of the log-likelihood function. Consider a nonlinear log-likelihood function, . We use a Taylor expansion to ï¬nd the tangent point of the log-likelihood, .",325,semantic
545abfdb-c7f0-402b-a4a3-96625361bac7,Parameter Estimation and Likelihood.pdf,CSCI_83,35,"The Taylorexpansion is:
Setting this expansion to 0, we have:
It is simple to solve for :
ğ‘™(ğœƒÂ |ğ—) = +ğ›¿ğœƒğœƒğ‘› ğœƒğ‘˜
ğ‘™( +ğ›¿ğœƒ)=ğ‘™( )+( )ğ›¿ğœƒ+ ( )ğ›¿ğœƒğœƒğ‘˜ ğœƒğ‘˜ ğ‘™â€² ğœƒğ‘˜ 12ğ‘™â€³ğœƒğ‘˜
0
0
= (ğ‘™( )+( )ğ›¿ğœƒ+ ( )ğ›¿ )ğ‘‘ğ‘‘Â ğ›¿ğœƒ ğœƒğ‘˜ ğ‘™â€² ğœƒğ‘˜ 12ğ‘™â€³ğœƒğ‘˜ ğœƒ2
=( )+( )ğ›¿ğœƒğ‘™â€² ğœƒğ‘˜ ğ‘™â€³ğœƒğ‘˜
ğ›¿ğœƒ
ğ›¿ğœƒ= ( )ğ‘™â€² ğœƒğ‘˜
( )ğ‘™â€³ğœƒğ‘˜",247,semantic
d5bf1127-f5c9-45f2-a43a-f9a863f502f2,Parameter Estimation and Likelihood.pdf,CSCI_83,36,"Newtonâ€™s method
At each step in the iteration, the update  is given by the following relationship:
 is a learning rate of step size. Newtonâ€™s method has a quadratic form
The quadratic form is not just a mathematical curiosity
Newtonâ€™s method exhibits convergence quadratic in the number of iterations
Compared to the approximate linear convergence for gradient descent methods
ğ‘¥ğ‘›+1
ğœƒğ‘›+1= +ğ›¾ğœƒğ‘› ( )ğ‘™â€² ğœƒğ‘˜
( )ğ‘™â€³ğœƒğ‘˜
= +ğ›¾Â ğ›¿ğœƒğœƒğ‘›
ğ›¾",421,semantic
bbce18d1-da6b-4e06-bb8a-cb1793d2d085,Parameter Estimation and Likelihood.pdf,CSCI_83,37,"Newtonâ€™s method
Newtonâ€™s method in higher dimensions
Use the gradient,  for the ï¬rst derivatives of the likelihood
Second derivative is represented as a matrix, , known as the Hessian
The quadratic update is:
Requires computing the inverse Hessian matrix with practical difï¬culties
The inverse of the Hessian may not exist as this matrix may be singular
With large number of model parameters, the Hessian will have high dimensionality and computing inverse is computationally intensive
Computing the full gradient and Hessian requires summing over all observations
For large scale problems quasi-Newton methods are used
Use an approximation to avoid computing the full inverse Hessian
Limited-memory Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno (L-BFGS) algorithm the most widely used quasi-Newton method
âˆ‡ ğ‘™()ğœƒâƒ—Â 
ğ‘™()âˆ‡ 2ğœƒâƒ—Â 
= +ğ›¾| ğ‘™() ğ‘™()ğ‘¥ğ‘›+1 ğ‘¥ğ‘› âˆ‡ 2ğœƒ ğœƒâƒ—Â |âˆ’1âˆ‡ ğœƒ ğœƒâƒ—",850,semantic
0790ea36-dff2-4c9c-b2ec-971a9bd60fc0,Parameter Estimation and Likelihood.pdf,CSCI_83,38,"Limitations of MLE
The maximum likelihood estimator has a number of important limitations, including
Incorrect model for complex distributions
Parameter near limits
High dimensional problems
Correlated features",210,semantic
60676866-3a26-44fe-bf32-cc9989125dd0,Parameter Estimation and Likelihood.pdf,CSCI_83,39,"Incorrect model and complex distributions
In many real-world problems the distribution are not simple
Problematic for maximum likelihood methods
Consider a likelihood function that is only approximately correct
Population being modeled has a different distribution
Outliers in the form of erroneous samples
Example; maximum likelihood estimator for population with a mixture of simple distributions
Mixture has multiple modes
One mode is the maximum
Mode found by any gradient-based algorithm is dependent on the starting point
MLE algorithm will ï¬nd the nearest local maximum
No guarantee of ï¬nding the actual maximum likelihood point",635,semantic
09765e5a-9850-4cc9-be6a-8685e4b2f8de,Parameter Estimation and Likelihood.pdf,CSCI_83,40,"Incorrect model and complex distributions
Example; univariate Normal maximum likelihood estimator for mixture of 3 Normal distributions",135,semantic
62050c95-3bfc-40a3-9a7d-7e6a7b86cf56,Parameter Estimation and Likelihood.pdf,CSCI_83,41,"Parameter near limits
For many distributions, parameter values have limits
The log-likelihood function may have an extremely high gradient near the limit
Results can be poorly determined parameter estimates and slow convergence
Fisher information drops rapidly near these limits, indicating poorly determined gradient
Examples:
Variance estiamtes near 0; variables with low variance
Binomial parameter estimates near  and ; case with either very few successes or failureğ‘=0 ğ‘=1",477,semantic
16c45ae8-801d-43b5-809d-1ea200646a88,Parameter Estimation and Likelihood.pdf,CSCI_83,42,"Parameter near limits
Example; Binomial likelihood and Fisher information with Extreme gradients and low information near limits,  and 
ğ‘=0.5ğ‘=0 ğ‘=1",148,semantic
38977815-d11e-40b6-9c32-902a338a72f3,Parameter Estimation and Likelihood.pdf,CSCI_83,43,"High dimensional problems
The MLE method often ï¬nds poor solutions to problems in high dimensions
High dimensions means a large numbers of parameters
The likelihood function has corresponding high dimensionality; one dimension for each parameter
For high dimensional problems, it is often the case that the gradient and Hessian are not well determined
Uncertainty in the variables can lead to considerable uncertainty in determining the gradient in high dimensions
MLE algorithms may not converge or converge to results with a large uncertainty
Even with few parameters, MLE methods can have convergence problems
Example; difï¬culties ï¬tting the variance parameter while ï¬tting the location parameter for a univariate Normal distribution
We will discuss this problem later in the course",785,semantic
d6773d38-2239-45fb-95d8-8c7c18fccd7e,Parameter Estimation and Likelihood.pdf,CSCI_83,44,"Correlated features
Theory assumes that the variables used for MLE are independent
In reality, never truly the case
Some variables have a high correlation to each other
MLE algorithm can breakdown since gradients will not be well determined
We will discuss this problem later in the course",289,semantic
ca87c474-3e16-4bd7-8f4d-c4f24c9347e1,Parameter Estimation and Likelihood.pdf,CSCI_83,45,"Summary
Likelihood is a measure of how well a parametric model ï¬ts a data sample
In most practical cases, we work with the log likelibood
 (ğ—|Â ğœƒ)= ğ‘“(|ğœƒ)âˆğ‘–=1
ğ‘›
ğ‘¥ğ‘–
ğ‘™(Â |Â ğ—)=ğ‘™ğ‘œğ‘”( (Â |Â ğ—))= ğ‘™ğ‘œğ‘”(ğ‘“(Â |Â ))ğœƒâƒ—Â  ğœƒâƒ—Â  âˆ‘ğ‘—
ğ‘¥ğ‘— ğœƒâƒ—",211,semantic
b2f4dc96-c8b0-4873-810b-ccd37bef4d40,Parameter Estimation and Likelihood.pdf,CSCI_83,46,"Summary
Matrix is the observed information matrix of the model, . The more negative the values of the second partial derivatives, the greater the curvature of the log-likehihood
Fisher information or expected information is the expectation over the second derivative of the observed information
Fisher information relates to the score function as its variance
For large sample, , take the expectation over :
The maximum likelihood estimate of model parameters, , is Normally distributed
The larger the Fisher information, the lower the variance of the parameter estimate
Greater curvature of the log likelihood function gives more certain the parameter estimates
The variance of the parameter estimate is inversely proportional to the number of samples, 
 ()ğœƒâƒ—Â 
 ()=âˆ’ğ„{ ()}=âˆ’ğ„{ }ğœƒâƒ—Â  ğœƒâƒ—Â  Â ğ‘™(ğ—|ğœƒ)âˆ‚2
âˆ‚ğœƒ2
Â Â  (0,1/ )âˆ‚Â ğ‘™(ğœƒ)âˆ‚ğœƒ âˆ¼ Ë™  ğœƒ
ğ‘›â†’âˆ ğ—
 (ğœƒ, )ğœƒÌ‚Â âˆ¼ Ë™ 1ğ‘› (ğœƒ)
ğœƒÌ‚Â 
ğ‘›",858,semantic
ec9bad19-ad52-4e1d-960c-8eb0e8bf9b1c,Parameter Estimation and Likelihood.pdf,CSCI_83,47,"Summary
The gradient descent method ï¬nds the maximum of the log-likelihood function by following the gradient â€˜uphillâ€™
The maximum likelihood for the model parameters is achieved when two conditions are met:
Gradient of the log-likelihood is known as the score function
=0âˆ‚Â ğ‘™(ğ—Â |Â ğœƒ))âˆ‚ğœƒ
<0Â ğ‘™(ğ—Â |Â ğœƒ))âˆ‚2
âˆ‚ğœƒ2
ğ‘”ğ‘Ÿğ‘ğ‘‘(ğ‘™())= Â ğ‘™()=ğœƒâƒ—Â  âˆ‡ ğœƒ ğœƒâƒ—Â 
â›
â
âœâœâœâœâœâœ
âˆ‚ğ‘™(ğ—Â |Â ğœƒ)âˆ‚ğœƒ1
âˆ‚ğ‘™(ğ—Â |Â ğœƒ)âˆ‚ğœƒ2
â‹® âˆ‚ğ‘™(ğ—Â |Â ğœƒ)âˆ‚ğœƒğ‘›
â
â 
âŸâŸâŸâŸâŸâŸ",395,semantic
1e5165ca-1315-45f3-a02d-6d7dd0e1aa8d,Parameter Estimation and Likelihood.pdf,CSCI_83,48,"Summary
The gradient descent method ï¬nds the maximum of the log-likelihood function by following the gradient â€˜uphillâ€™
Given a current parameter estimate vector at step n , , the improved parameter estimate vector, , is found:
The hyperparameter  is the learning rate or step size
Stochastic optimization uses a Bernoulli random sample of the data to estimate the expected update of the model weights
Where,  is the expected value of the gradient given the Bernoulli sample of the data, . Empirically, SGD has good convergence properties
ğœƒÌ‚Â ğ‘› ğœƒÌ‚Â ğ‘›+1
= +ğ›¾Â  Â ğ‘™()ğœƒÌ‚Â ğ‘›+1 ğœƒÌ‚Â ğ‘› âˆ‡ ğœƒ ğœƒÌ‚Â 
ğ›¾
= +ğ›¾Â  [ ğ½()]ğœƒğ‘¡+1 ğœƒğ‘¡ ğ¸ğ‘‘ğ‘ğ‘¡ğ‘ğ‘Ì‚Â  âˆ‡ ğœƒ ğœƒğ‘¡
[]ğ¸ğ‘‘ğ‘ğ‘¡ğ‘ğ‘Ì‚Â  ğ‘‘ğ‘ğ‘¡ğ‘ğ‘Ì‚",635,semantic
00d7f549-3400-4665-b282-f9ef5d76cd77,Parameter Estimation and Likelihood.pdf,CSCI_83,49,"Summary
Newtonâ€™s method, and related methods, employ a quadratic approximation to optimization. Newtonâ€™s method exhibits convergence quadratic in the number of iterations
Compared to the approximate linear convergence for gradient descent methods
The quadratic update is:
 is a learning rate of step size
Requires computing the inverse Hessian matrix with practical difï¬culties
The inverse of the Hessian may not exist as this matrix may be singular
With large number of model parameters, the Hessian will have high dimensionality and computing inverse is computationally intensive
For large scale problems quasi-Newton methods are used
Use an approximation to avoid computing the full inverse Hessian
Limited-memory Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno (L-BFGS) algorithm the most widely used quasi-Newton method
= +ğ›¾| ğ‘™() ğ‘™()ğ‘¥ğ‘›+1 ğ‘¥ğ‘› âˆ‡ 2ğœƒ ğœƒâƒ—Â |âˆ’1âˆ‡ ğœƒ ğœƒâƒ—Â 
ğ›¾",851,semantic
475a0986-ca8f-4658-b207-06eb2eb8cac4,Parameter Estimation and Likelihood.pdf,CSCI_83,50,"Summary
The maximum likelihood estimator has a number of important limitations, including
Incorrect model and complex distributions
Parameter near limits
High dimensional problems
Correlated features",199,semantic
349f5bde-b65b-4b22-975f-8a498463c362,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,0,"When One Thing Depends on Another; Conditional Probability
Steve Elston
09/15/2022",82,semantic
62d81b10-ecfe-40ab-97c5-b35eb43c50b4,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,1,"Introduction
Many real-world random variables depend on other random variables
Statistical models of complex processes invariably require the use of conditional probability distributions
Conditional probability is the probability that event A occurs given that event B has occurred
Write the conditional probability of A given B as:
Example: Model of the probability of contracting the infectious disease, depends on other variables
In more technical terms, the probability of contracting the disease is conditional on other random variables. Age, contact with people carrying the disease, immunity, etc. ğ‘ƒ(ğ´|ğµ)",611,semantic
325cb9dc-33fd-4e1f-929c-1ce10fe00990,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,2,"Properties of Conditional Probability
Example:
Example of conditional probability of discrete events; credit, Wikipedia commons
Sample space is the space of all possible events in the set 
Sample space is divided into several subspaces or subsets, ,  and 
Intersection is where the two sets overlap occur in both  and 
ğ‘†
ğ´ğµ ğ¶
ğ´ ğµ",329,semantic
dfc235cb-fc59-4197-ad9a-81612e31c690,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,3,"Properties of Conditional Probability
Example:
Example of conditional probability of discrete events; credit, Wikipedia commons
Intersection is where the two sets overlap occur in both  and 
First, ï¬nd the relationship between conditional probability and the intersection between the sets, 
The probability of the intersection is the product of two probabilities:
1. since B must be true to be in this intersection. 2.",418,semantic
2b3e4f6d-9f8f-4565-937f-3d8056d533f7,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,3,"since A must also occur when B is occurring
The result is:
ğ´ ğµ
ğ‘ƒ(ğ´âˆ© ğµ)
ğ‘ƒ(ğµ)
ğ‘ƒ(ğ´|ğµ)
ğ‘ƒ(ğ´âˆ© ğµ)=ğ‘ƒ(ğ´|ğµ)ğ‘ƒ(ğµ)",101,semantic
014d82d3-75f5-494d-9c13-4ead1d710467,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,4,"Properties of Conditional Probability
Rearranging terms we get the following:
We could have, just as well, written the last equation as:
Now, the probability of an identical event in the same intersection:
Factorization of a probability function is a key tool: notice that the factorization of a conditional probability distribution in not unique
ğ‘ƒ(ğ´âˆ© ğµ)=ğ‘ƒ(ğ´|ğµ)ğ‘ƒ(ğµ)
ğ‘ƒ(ğ´|ğµ)=ğ‘ƒ(ğ´âˆ© ğµ)ğ‘ƒ(ğµ)
= = =
210410
24 12
ğ‘ƒ(ğµâˆ© ğ´)=ğ‘ƒ(ğµ|ğ´)ğ‘ƒ(ğ´)
ğ‘ƒ(ğ´âˆ© ğµ)=ğ‘ƒ(ğ´|ğµ)ğ‘ƒ(ğµ)=ğ‘ƒ(ğµ|ğ´)ğ‘ƒ(ğ´)=ğ‘ƒ(ğµâˆ© ğ´)",460,semantic
0ce2f933-d9b4-4225-b86c-dd1a5e21caeb,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,5,"Set Operations and Probability
Set operations are applied to probability problems
1.",84,semantic
6110b77a-8cc7-4334-bb22-b509894cb301,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,5,"Intersection:
2. Union: is the sum of the probabilities of the sets minus the intersection between the sets:
3. Negation: Example, compute the probability of an event being in subset  but not in :
Example: We can apply De Morganâ€™s Laws:
ğ‘ƒ(ğ´âˆ© ğµ)=ğ‘ƒ(ğ´|ğµ)ğ‘ƒ(ğµ)
ğ‘ƒ(ğ´âˆª ğµ)=ğ‘ƒ(ğ´)+ğ‘ƒ(ğµ)âˆ’ğ‘ƒ(ğ´âˆ© ğµ)
ğ´ ğµ
ğ‘ƒ(ğ´Â ğ‘ğ‘›ğ‘‘Â Â¬ğµ)=ğ‘ƒ(ğ´)âˆ’ğ‘ƒ(ğµâˆ© ğ´)
ğ‘ƒ(Â¬(ğ´âˆª ğµ))ğ‘ƒ(Â¬(ğ´âˆ© ğµ))=ğ‘ƒ(Â¬ğ´Â âˆ© Â¬ğµ)=ğ‘ƒ(Â¬ğ´Â âˆª Â¬ğµ)",353,semantic
cc919f8c-9d9c-4fae-849f-1ebd6b3d72b8,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,6,"Independence and Mutual Exclusivity
The factorization of probability distributions can be simpliï¬ed if events are either independent or mutually exclusive
At ï¬rst glance, these concepts may seem similar
Are quite different
Very different implications
Independence of sets  and  means the occurrence of an event in , does not have any dependency on an event in 
Mutual exclusivity means events cannot occur in both the sets  and 
ğ´ ğµ ğ´ ğµ
ğ´ ğµ",440,semantic
7b270e28-4f1c-46ce-bd15-55a567fa432c,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,7,"Independence
Express independence of random variables, , mathematically:
But independence of A given B does not imply independence of B given A:
In other words, we need to pay attention to if A is independent of B or B is independent of A- One or the other could be true - Both could be true
ğ´âŠ¥ ğµ
ğ‘ƒ(ğ´Â âˆ© ğµ)ğ‘ƒ(ğ´Â âˆª ğµ)ğ‘ƒ(ğ´|ğµ)ğ‘ƒ(ğ´|Â¬ğµ)
=ğ‘ƒ(ğ´|ğµ)ğ‘ƒ(ğµ)=ğ‘ƒ(ğ´)ğ‘ƒ(ğµ)=ğ‘ƒ(ğ´)+ğ‘ƒ(ğµ)âˆ’ğ‘ƒ(ğ´)ğ‘ƒ(ğµ)=ğ‘ƒ(ğ´)=ğ‘ƒ(ğ´)
ğ‘ƒ(ğ´|ğµ)=ğ‘ƒ(ğ´)â‡ğ‘ƒ(ğµ|ğ´)=ğ‘ƒ(ğµ)",400,semantic
234a14bb-8194-4976-bfcd-aa01cc06c108,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,8,"Mutual Exclusivity
If the intersection between events is an empty set:
Then, events in A are mutually exclusive of events in B:
And,  mutually exclusive of B, implies B is mutually exclusive of A
ğ´âˆ© ğµ=âˆ…
ğ‘ƒ(ğ´âˆª ğµ)ğ‘ƒ(ğ´|ğµ)
ğ‘ƒ(ğ´|Â¬ğµ)
=ğ‘ƒ(ğ´)+ğ‘ƒ(ğµ)=0
= ğ‘ƒ(ğ´)1âˆ’ğ‘ƒ(ğµ)
ğ´",252,semantic
dbb08a49-ffc1-4cf4-b205-ed1ec81c3321,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,9,"Conditional Distributions and Bayesâ€™ Theorem
Bayesâ€™ theorem, also known as Bayesâ€™ rule, is a powerful tool to think about and analyze conditional probabilities
We can derive Bayes Theorem starting with the following relationships:
Now:
Which leads to:
Which is Bayesâ€™ theorem! ğ‘ƒ(ğ´âˆ© ğµ)=ğ‘ƒ(ğ´|ğµ)ğ‘ƒ(ğµ)ğ‘ƒ(ğµâˆ© ğ´)=ğ‘ƒ(ğµ|ğ´)ğ‘ƒ(ğ´)
ğ‘ƒ(ğ´âˆ© ğµ)=ğ‘ƒ(ğµâˆ© ğ´)
ğ‘ƒ(ğ´|ğµ)ğ‘ƒ(ğµ)
ğ‘ƒ(ğ´|ğµ)
=ğ‘ƒ(ğµ|ğ´)ğ‘ƒ(ğ´)
=ğ‘ƒ(ğµ|ğ´)ğ‘ƒ(ğ´)ğ‘ƒ(ğµ)",375,semantic
3f148c5a-d4c7-4a0e-82ea-2bbf2ee0909a,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,10,"Interpreting Bayes Theorem
How can we interpret Bayesâ€™ theorem in a useful way? Consider an example using Bayes Theorem for an hypothesis test given some data or evidence
We must make an assertion of our prior probability that the hypothesis is true
We also must choose a likelihood function of the evidence given the hypothesis
ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ(â„ ğ‘¦ğ‘ğ‘œğ‘¡â„ ğ‘’ğ‘ ğ‘–ğ‘ )
ğ¿ğ‘–ğ‘˜ğ‘’ğ‘™ğ‘–â„ ğ‘œğ‘œğ‘‘(ğ‘’ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘›ğ‘ğ‘’Â |Â â„ ğ‘¦ğ‘ğ‘œğ‘¡â„ ğ‘’ğ‘ ğ‘–ğ‘ )",385,semantic
cf1de5fc-8f7c-411c-b7e6-67c14c7ff4b3,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,11,"Interpreting Bayes Theorem
Now, we can think of Bayesâ€™ theorem in the following terms:
We discuss selection of prior probability distributions and likelihood functions in subsequent lectures
The denominator  or partition function is problematic
Required to normalize the posterior distribution to range 
ğ‘ƒğ‘œğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘œğ‘Ÿ(â„ ğ‘¦ğ‘ğ‘œğ‘¡â„ ğ‘’ğ‘ ğ‘–ğ‘ Â |Â ğ‘’ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘›ğ‘ğ‘’)=ğ¿ğ‘–ğ‘˜ğ‘’ğ‘™ğ‘–â„ ğ‘œğ‘œğ‘‘(ğ‘’ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘›ğ‘ğ‘’Â |Â â„ ğ‘¦ğ‘ğ‘œğ‘¡â„ ğ‘’ğ‘ ğ‘–ğ‘ )Â ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ(â„ ğ‘¦ğ‘ğ‘œğ‘¡â„ ğ‘’ğ‘ ğ‘–ğ‘ )ğ‘ƒ(ğ‘’ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘›ğ‘ğ‘’)
ğ‘ƒ(ğ‘’ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘›ğ‘ğ‘’)
0â‰¤ğ‘ƒğ‘œğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘œğ‘Ÿ(â„ ğ‘¦ğ‘ğ‘œğ‘¡â„ ğ‘’ğ‘ ğ‘–ğ‘ Â |Â ğ‘’ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘›ğ‘ğ‘’)â‰¤1",457,semantic
5ae9bf6b-f543-4017-b392-e225ee9b9d2e,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,12,"Interpreting Bayes Theorem
Denominator must account for all possible outcomes, or alternative hypotheses, :
This is a formidable problem! â„ â€²
ğ‘ƒğ‘œğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘œğ‘Ÿ(â„ ğ‘¦ğ‘ğ‘œğ‘¡â„ ğ‘’ğ‘ ğ‘–ğ‘ Â |Â ğ‘’ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘›ğ‘ğ‘’)=ğ¿ğ‘–ğ‘˜ğ‘’ğ‘™ğ‘–â„ ğ‘œğ‘œğ‘‘(ğ‘’ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘›ğ‘ğ‘’Â |Â â„ ğ‘¦ğ‘ğ‘œğ‘¡â„ ğ‘’ğ‘ ğ‘–ğ‘ )Â ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ(â„ ğ‘¦ğ‘ğ‘œğ‘¡â„ ğ‘’ğ‘ ğ‘–ğ‘ )ğ¿ğ‘–ğ‘˜ğ‘’ğ‘™ğ‘–â„ ğ‘œğ‘œğ‘‘(ğ‘’ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘›ğ‘ğ‘’Â |Â  )Â ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ()âˆ‘âˆˆ Â ğ´ğ‘™ğ‘™Â ğ‘ğ‘œğ‘ ğ‘ ğ‘–ğ‘ğ‘™ğ‘’Â â„ ğ‘¦ğ‘ğ‘œğ‘¡â„ ğ‘’ğ‘ ğ‘’ğ‘ â„ â€² â„ â€² â„ â€²",306,semantic
2859b95c-05d0-4216-8159-c2ebfe04ba62,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,13,"Bayes Theorem Example
Hemophilia is a serious genetic condition expressed on any X chromosome
Women have two X chromosomes and are unlikely to exhibit hemophilia
One X chromosome inherited from each parent
Must inherit hemophilia from both parents
Men have one X chromosome and one Y chromosome
Inherit Y chromosome from the father
Inherit X chromosome, and possibly hemophilia, from the mother
Say a woman has a brother who exhibits hemophilia
X chromosome expression is  - brother has hemophilia with 
Womanâ€™s father does not exhibit hemophilia,  - father has hemophilia with 
Our prior probability that she carries the genetic marker for hemophilia  - Womanâ€™s X chromosome has P = 0.5 that it is from mother, who carried the marker with 
ğœƒ=1.0 ğ‘ƒ=1.0
ğœƒ=0 ğ‘ƒ=0.0
ğœƒ=0.5 ğ‘ƒ=1.0",774,semantic
b0cd05a2-1ffa-4149-a9c4-d74a489bfca1,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,14,"Bayes Theorem Example
As evidence the woman has two sons (not identical twins) with no expression of hemophilia
What is the likelihood for the two sons  not having hemophilia? Two possible cases here
Case where woman caries one X chromosome with hemophilia expression, , and probability of passing to son = 0.5
Case where woman does not carry an X chromosome with hemophilia expression, 
Note: we are neglecting the possibility of a mutations in one of the sons
ğ‘‹=( , )ğ‘¥1ğ‘¥2
ğœƒ=1
ğœƒ=0
ğ‘( =0, =0|ğœƒ=1)=0.5âˆ— 0.5=0.25ğ‘¥1 ğ‘¥2
ğ‘( =0, =0|ğœƒ=0)=1.0âˆ— 1.0=1.0ğ‘¥1 ğ‘¥2",548,semantic
156d661d-712c-439d-a6f0-63034b4fec5c,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,15,"Bayes Theorem Example
Use Bayes theorem to compute probability woman carries an X chromosome with hemophilia expression, 
Where:
The evidence of two sons without hemophilia causes us to update our belief that the probability of the woman carrying the disease
ğœƒ=1
ğ‘(ğœƒ=1|ğ‘‹)= ğ‘(ğ‘‹|ğœƒ=1)ğ‘(ğœƒ=1)ğ‘(ğ‘‹|ğœƒ=1)ğ‘(ğœƒ=1)+ğ‘(ğ‘‹|ğœƒ=0)ğ‘(ğœƒ=0)
= =0.200.25âˆ— 0.50.25âˆ— 0.5+1.0âˆ— 0.5
ğ¿ğ‘–ğ‘˜ğ‘’ğ‘™ğ‘–â„ ğ‘œğ‘œğ‘‘ğ‘ƒğ‘Ÿğ‘–ğ‘œğ‘ŸÂ ğ‘ğ‘Ÿğ‘œğ‘ğ‘ğ‘ğ‘–ğ‘™ğ‘–ğ‘¡ğ‘¦ğ‘ƒğ‘Ÿğ‘œğ‘ğ‘ğ‘ğ‘–ğ‘™ğ‘–ğ‘¡ğ‘¦Â ğ‘œğ‘“Â â„ ğ‘¦ğ‘ğ‘œğ‘¡â„ ğ‘’ğ‘ ğ‘–ğ‘ Â ğ‘›ğ‘œÂ ğ‘šğ‘ğ‘Ÿğ‘˜ğ‘’ğ‘Ÿ
=ğ‘(ğ‘‹|ğœƒ=1)=0.50.5=0.25=ğ‘(ğœƒ=1)=0.5=ğ‘(ğœƒ=0)=1âˆ’ğ‘(ğœƒ=1)=0.5",470,semantic
e90977e3-ebdb-43f0-92cf-bdb706c686c3,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,16,"Marginal Distributions
In many cases we are interested in the marginal distribution
Example, it is often the case that only one or a few parameters of a joint distribution will be of interest
In other words, we are interested in the marginal distribution of these parameters
The denominator of Bayes theorem, , can be computed as a marginal distribution
Consider a multivariate probability density function with  variables, 
Marginal distribution is the distribution of one variable with the others integrated out. Integrate over all other variables  the result is the marginal distribution, :
For discrete distribution the above is a summation
ğ‘ƒ(ğ‘‘ğ‘ğ‘¡ğ‘)
ğ‘› ğ‘(, ,â€¦, )ğœƒ1ğœƒ2 ğœƒğ‘›
{,â€¦, }ğœƒ2 ğœƒğ‘› ğ‘()ğœƒ1
ğ‘()= ğ‘(, ,â€¦, )Â ğ‘‘ğœƒ2,â€¦,ğ‘‘ğœƒ1 âˆ«,â€¦,ğœƒ2 ğœƒğ‘›
ğœƒ1ğœƒ2 ğœƒğ‘› ğœƒğ‘›
ğ‘()= ğ‘(, ,â€¦, )ğœƒ1 âˆ‘,â€¦,ğœƒ2 ğœƒğ‘›
ğœƒ1ğœƒ2 ğœƒğ‘›",770,semantic
9d0d69b7-d9e3-4568-b636-a4579bca556d,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,17,"Example: Marginal Distribution
Marginal distributions of multivariate Normal with  and 
Marginal distributions displayed on margins of scatter plot
## For x mean = 0.014143391305686877  variance = 0.9422459154327474## For y mean = -0.030639573892598113  variance = 0.9834781862781161
ğœ‡=[0,0] ğœ=[ ]1.00.0 0.01.0",310,semantic
aab53b5d-56c8-4773-a2e2-55444bbe0a3c,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,18,"Example: Marginal Distribution
Marginal distributions of multivariate Normal with  and 
Marginal distributions displayed on margins of scatter plot
## For x mean = -0.03097095972118977  variance = 0.9515116745292175## For y mean = -0.0618435252254139  variance = 1.0206970889966356
ğœ‡=[0,0] ğœ=[ ]1.00.5 0.51.0",308,semantic
d7c1f405-97e5-43a2-9e27-caf4cd55b50a,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,19,"Example: Marginal Distribution
Marginal distributions of multivariate Normal with  and 
Marginal distributions displayed on margins of scatter plot
## For x mean = -0.03902993462783489  variance = 0.9970254226866206## For y mean = -0.06192143862563938  variance = 0.9812738103164559
ğœ‡=[0,0] ğœ=[ ]1.00.9 0.91.0",309,semantic
daa0fba2-4fb2-4cfb-8c66-2583c0f090a7,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,20,"Conditional Probability Example
A simple and widely used example of using conditional probabilities to work out the chance of having a rare disease. Sickle Cell Anemia is a serious, but fairly rare, disease
The probability that a given patient, drawn at random from the population of all people in the United States, has the disease is. We can describe the possible events in diagnosing this condition as:
 a patient has the disease. a patient does not have the disease.",470,semantic
aa07a437-9a12-46a5-86b2-1f57a3752344,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,20,"patient tests positive. a patient tests negative. ğ‘ƒ(ğ‘†)= =0.000312513200
ğ‘†â‡’
â‡’ğ‘†â€²
âŠ• â‡’
âˆ’â‡’",85,semantic
f4fbeb71-7d15-4124-b9f7-ec657941c0ae,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,21,"Conditional Probability Example
What if a medical company claims that it has developed a test that is 99% accurate? We can write:
On the surface, it seems that a 99% reliable test is rather good
On average, 99 people out of 100 who have the disease will be identiï¬ed and treated
But, dig into the conditional probabilities and make sure! ğ‘ƒ(ğ‘†|âŠ• )=0.99
ğ‘ƒ( |âˆ’)=0.99ğ‘†â€²",364,semantic
b7494453-5813-489a-819a-596a182d842d,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,22,"Conditional Probability Example
From the root the directed acyclic graphical model (DAG) deï¬nes a conditional dependency structure
Goal: Evaluate the medical test as a decision rule for treatment
Summarize the conditional probabilities for these outcomes:
: Conditional probability the test correctly identiï¬es patient with disease
: Conditional probability of a negative test for a patient with the disease; Type II Error or False Negative
: Conditional probability that a patient with no disease tests positive; Type I Error or False Positive
: Conditional probability of a negative test for a patient with no disease
Graph showing dependency of conditional distribution
ğ‘ƒ(âŠ• |ğ‘†)
ğ‘ƒ(âˆ’|ğ‘†)
ğ‘ƒ(âŠ• | )ğ‘†â€²
ğ‘ƒ(âˆ’| )ğ‘†â€²",706,semantic
51eb3fe1-eb02-4511-8e0b-fa2d015afdd3,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,23,"Conditional Probability Example
Four possible outcomes shown using a confusion matrix or truth table
Table shows conditional probabilities of each outcome
Positive TestNegative Test
DiseaseTrue Positive RateFalse Negative Rate
No DiseaseFalse Positive RateTrue Negative Rate
Tip: Make sure the numbers in your confusion matrix sum to 1.0",337,semantic
88a791f2-bc68-4de1-b81b-e8fa20154487,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,24,"Summary
Conditional probability
One random variable depends on another
But not commutable
Mutually exclusivity
Independence
ğ‘ƒ(ğ´|ğµ)=ğ‘ƒ(ğ´)â‡ğ‘ƒ(ğµ|ğ´)=ğ‘ƒ(ğµ)
ğ‘ƒ(ğ´|ğµ)=ğ‘ƒ(ğ´)+ğ‘ƒ(ğµ)
ğ‘ƒ(ğ´|ğµ)=ğ‘ƒ(ğ´)",176,semantic
46ec78b0-97b9-46a2-80d4-f2435cb3eb0a,When One Thing Depends on Another; Conditional Probability.pdf,CSCI_83,25,"Summary
Bayesâ€™ theorem
Marginal distribution
ğ‘ƒ(ğ´|ğµ)=ğ‘ƒ(ğµ|ğ´)ğ‘ƒ(ğ´)ğ‘ƒ(ğµ)
ğ‘()= ğ‘(, ,â€¦, )Â ğ‘‘ğœƒ2,â€¦,ğ‘‘ğœƒ1 âˆ«,â€¦,ğœƒ2 ğœƒğ‘›
ğœƒ1ğœƒ2 ğœƒğ‘› ğœƒğ‘›
ğ‘()= ğ‘(, ,â€¦, )ğœƒ1 âˆ‘,â€¦,ğœƒ2 ğœƒğ‘›
ğœƒ1ğœƒ2 ğœƒğ‘›",147,semantic
d9417461-20f5-4d62-ac52-a9f0263e8aae,Bayes MCMC Models.pdf,CSCI_83,0,"Bayes MCMC Models
Steve Elston
10/17/2022",41,semantic
c077bfac-7806-4523-a56d-27fcff21b8e4,Bayes MCMC Models.pdf,CSCI_83,1,"Review
Bayesian analysis is a contrast to frequentist methods
The objective of Bayesian analysis is to compute a posterior distribution
Contrasts with frequentist statistics with objective to compute a point estimate and conï¬dence interval from a sample
Bayesian models allow expressing prior information as a prior distribution
The posterior distribution is said to quantify our current belief
Belief is based on the posterior distribution
We update beliefs based on additional data or evidence
A critical difference with frequentist models which must be computed from a complete sample
Inference can be performed on the posterior distribution by ï¬nding the maximum a postiori (MAP) value and a credible interval
Predictions are made by simulating from the posterior distribution",780,semantic
5e9ed4cd-ceb3-43c9-9d9c-8fcbbfef8d47,Bayes MCMC Models.pdf,CSCI_83,2,"Review
Two functions must be deï¬ned to compute the posterior distribution
The likelihood for the model being used
The likelihood function includes the parameters for the model
Example; for Binomial likelihood is the probability of success
Example; for Normal likelihood is the mean,  and variance. The prior distribution encodes the information we have in advance about the model parameters
For simple cases is the conjugate distribution
The posterior distribution is in the conjugate family
Example, for binomial likelihood, the conjugate is the 
Example, for Normal likelihood, the normal distribution is the conjugate for the mean, 
ğœ‡ ğœ2
ğµğ‘’ğ‘¡ğ‘(ğ›¼,ğ›½)
ğœ‡",652,semantic
4dd23382-c943-4911-9cda-69b35027676e,Bayes MCMC Models.pdf,CSCI_83,3,"Introduction
How can we extend Bayes models to more complex problems? For simple problems we can use a conjugate prior and posterior
Unlikely posterior distribution will be a simple conjugate
Need to perform sampling to compute approximation of complex posterior
We need highly efï¬cient sampling methods for complex problems",324,semantic
bdcdb1a7-bcfb-4cfb-a54d-62fe37ec5b7d,Bayes MCMC Models.pdf,CSCI_83,4,"Grid Sampling Cannot Scale! Real-world Bayes models have large numbers of parameters, into the millions
Naive approach is simple grid sampling
Sample across dimensions of the parameter space
Consider this thought experiment, sampling dimension 100 times:
1-parameter model:  samples
2-parameter model:  samples
3-parameter model:  samples
100-parameter model:  samples
Computational complexity of grid sampling has exponential scaling with dimensionality
Need a better approach! 100
=100001002
=1003 105
=100100 10102",517,semantic
0b14d29f-4f61-4c90-88dc-c8b6eb1f9c4c,Bayes MCMC Models.pdf,CSCI_83,5,"Scaling Bayesian models
How can we scale Bayesian models to 1000s of parameters? Variational methods
Based on Variational calculus
A very effective and efï¬cient method for some problems
But, proves difï¬cult us use as a general solution
Details beyond our scope here
Markov Chain Monte Carlo (MCMC)
A simple and reliable method for Bayesian inference
Our focus here",364,semantic
692e8a10-bbbb-43db-8d9c-53e760a0de9b,Bayes MCMC Models.pdf,CSCI_83,6,"Introduction to Markov Chain Monte Carlo
Large-scale Bayesian models need highly efï¬cient sampling methods
Markov chain Monte Carlo (MCMC) sampling is efï¬cient and scalable
Rather than sampling on a grid MCMC methods sample distributions efï¬ciently
Sample higher density regions of posterior with higher probability
Requires effort to understand how the algorithm works
Must carefully evaluate how well algorithm converged
What to do when things go wrong",454,semantic
f7c5f9ba-9766-4cf0-9e63-e5f77fade621,Bayes MCMC Models.pdf,CSCI_83,7,"What is a Markov process? MCMC sampling uses a Markov processes sampling chain
A Markov process is a stochastic process that transitions from a current state, , to some next state, , with probability 
No dependency on past states
Summarize properties of a Markov process:
Probability of state transition is parameterized by a matrix of probabilities, , of dim N X N for N possible states
 only depends on the current state, 
Transition can be to current state. ğ‘¥ğ‘¡ ğ‘¥ğ‘¡+1 Î 
Î 
Î  ğ‘¥ğ‘¡",477,semantic
0f949dac-ea07-4ef0-9083-51e59bbfc83b,Bayes MCMC Models.pdf,CSCI_83,8,"What is a Markov process? Since Markov transition process depends only on the current state a Markov process is memoryless
We can express the sequence of a Markov transition processes as:
The Markov process is memoryless
Transition probability only depends on the current state, 
No dependency on any previous states, . ğ‘( | =, , ,â€¦, )=ğ‘( | )ğ‘‹ğ‘¡+1ğ‘‹ğ‘¡ ğ‘¥ğ‘¡ğ‘¥ğ‘¡âˆ’1 ğ‘¥ğ‘¡âˆ’2 ğ‘¥0 ğ‘‹ğ‘¡+1ğ‘¥ğ‘¡
ğ‘¥ğ‘¡
{ , ,â€¦, }ğ‘¥ğ‘¡âˆ’1 ğ‘¥ğ‘¡âˆ’2 ğ‘¥0",395,semantic
682e1d13-199a-4f52-8563-01703a796400,Bayes MCMC Models.pdf,CSCI_83,9,"What is a Markov process? For system with  possible states we can write the transition probability matrix, :ğ‘ Î 
Î =
â¡
â£
â¢â¢â¢â¢
ğœ‹1,1
ğœ‹2,1
â€¦ğœ‹ğ‘,ğ‘–
ğœ‹1,2
ğœ‹2,2
â€¦ğœ‹ğ‘,2
â€¦â€¦â€¦â€¦
ğœ‹1,ğ‘
ğœ‹2,ğ‘
â€¦ğœ‹ğ‘,ğ‘
â¤
â¦
â¥â¥â¥â¥
ğ‘¤â„ ğ‘’ğ‘Ÿğ‘’=ğ‘ğ‘Ÿğ‘œğ‘ğ‘ğ‘ğ‘–ğ‘™ğ‘–ğ‘¡ğ‘¦Â ğ‘œğ‘“Â ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›Â ğ‘“ğ‘Ÿğ‘œğ‘šÂ ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’Â ğ‘—Â ğ‘¡ğ‘œÂ ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’Â ğ‘–ğœ‹ğ‘–,ğ‘—
ğ‘ğ‘›ğ‘‘=ğ‘ğ‘Ÿğ‘œğ‘ğ‘ğ‘ğ‘–ğ‘™ğ‘–ğ‘¡ğ‘¦Â ğ‘œğ‘“Â ğ‘ ğ‘¡ğ‘ğ‘¦ğ‘–ğ‘›ğ‘”Â ğ‘–ğ‘›Â ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’Â ğ‘–ğœ‹ğ‘–,ğ‘–
ğ‘“ğ‘¢ğ‘Ÿğ‘¡â„ ğ‘’ğ‘Ÿâ‰  Â ğ‘–ğ‘›Â ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿğ‘ğ‘™ğœ‹ğ‘–,ğ‘— ğœ‹ğ‘—,ğ‘–",319,semantic
b07b4615-67f8-498f-bd3f-c75fe0a1c215,Bayes MCMC Models.pdf,CSCI_83,10,"Example of a Markov Process
To make the foregoing more concrete letâ€™s construct a simple example. We will start with a system of 3 states, . The transition matrix is:
Some key points to notice
The probabilities of transition from a state is given in a column
The probabilities in each column must add to 1.0
The probabilities of a transition to the same state are along the diagonal of the matrix
Some transitions not possible have a probability of 0.0
Example, ; cannot transition from state 2 to 1
Example, ; cannot remain in state 3
{, , }ğ‘¥1ğ‘¥2ğ‘¥3
Î = =â¡
â£
â¢â¢â¢
ğœ‹1,1
ğœ‹2,1
ğœ‹3,1
ğœ‹1,2
ğœ‹2,2
ğœ‹3,2
ğœ‹1,3
ğœ‹2,3
ğœ‹3,3
â¤
â¦
â¥â¥â¥
â¡
â£
â¢â¢
0.50.20.3
0.00.30.7
0.60.40.0
â¤
â¦
â¥â¥
=0ğœ‹2,1
=0ğœ‹3,3",671,semantic
807ec47d-302c-48be-a0a5-63e4ab717465,Bayes MCMC Models.pdf,CSCI_83,11,"Example of a Markov Process
Letâ€™s apply a probability matrix to a set of possible states
The state vector represents being in the ï¬rst state at time step ; 
After a state transition, we compute the probability of being in each of the three possible states at the next time step, :
ğ‘¡ =[1,0,0]ğ‘¥ğ‘¡â†’
ğ‘¡+1
=Î Â  = =ğ‘¥âƒ—Â ğ‘¡+1 ğ‘¥âƒ—Â ğ‘¡
â¡
â£
â¢â¢
0.50.20.3
0.00.30.7
0.60.40.0
â¤
â¦
â¥â¥
â¡
â£
â¢â¢
100
â¤
â¦
â¥â¥
â¡
â£
â¢â¢
0.50.20.3
â¤
â¦
â¥â¥",403,semantic
914d3199-c5c9-49b3-9904-8adebf934989,Bayes MCMC Models.pdf,CSCI_83,12,"From Markov process to Markov chain
The foregoing is a single step of a Markov process
What happens when there is a series of transitions? A sequence of such transitions is known as a Markov chain
There are two major behaviors observed with Markov Chains
Episodic Markov chains have a terminal state
The terminal state can only transition to itself
Once the system is in the terminal state, we say that the episode has ended
Episodic processes are not of direct interest here
Continuous Markov chains have no terminal state
Continue indeï¬nitely, at least in principle
Continuous Markov chains sample probability distribution
Are ideal for estimating Bayesian posterior distributions",682,semantic
aa18a0f0-b978-4f74-9141-60e72ca26fba,Bayes MCMC Models.pdf,CSCI_83,13,"From Markov process to Markov chain
Markov chain comprises a number of state transitions
Chain of  state transitions, 
Each transition has the probabilities given by the state transition matrix, 
To estimate the probabilities of being in the states we use a special case known as a stationary Markov chain
We will skip the technical mathematical details here
Over a large number of time steps the number of times the states are visited is proportional to the state probabilities
ğ‘› {, , ,â€¦, }ğ‘¡1ğ‘¡2ğ‘¡3 ğ‘¡ğ‘›
Î ",502,semantic
f642e098-19f5-45e6-8f77-f1eb3e874885,Bayes MCMC Models.pdf,CSCI_83,14,"From Markov process to Markov chain
Start with initial state,  for a continuous Markov chain:
We can ï¬nd the probabilities of the states without knowing the values of the transition matrix, ! As long as we can repeatedly sample the stochastic Markov process, we can estimate the state probabilities
This is the key to Markov Chain Monte Carlo sampling
ğ‘¥âƒ—Â 0
Î Â Î Â Î Â â€¦Î Â  = Â ğ‘¥âƒ—Â ğ‘¡ Î ğ‘›ğ‘¥âƒ—Â ğ‘¡âˆ’â†’âˆ’âˆ’ğ‘›â†’âˆğ‘(ğ‘¥)â†’
Î ",395,semantic
5cfa39b8-6873-4b80-9ec5-c7fcaaa3de2e,Bayes MCMC Models.pdf,CSCI_83,15,"MCMC and the Metropolis-Hastings Algorithm
The ï¬rst MCMC sampling algorithm developed is the Metropolis-Hastings (M-H) algorithm; often referred to as Metropolis algorithm or the M-Halgorithm. The M-H algorithm uses the following steps to estimate the posterior density:
Pick a starting point in the parameter space
Sample the posterior distribution according to the model, the product of the likelihood  and prior, 
Choose a nearby point in parameter space randomly and evaluate the posterior at this point
Use the following decision rule to accept or reject the new sample:
If the likelihood, , of the new point is greater than the current point, accept new point
If the likelihood of the new point is less than your current point, only accept with probability according to the ratio:
. If the sample is accepted, compute the posterior density at the new sample point
Repeat sampling steps many times, until convergence
ğ‘ƒ(ğ‘‘ğ‘ğ‘¡ğ‘|ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ ) ğ‘ƒ(ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ )
ğ‘( ğ‘‘ğ‘ğ‘¡ğ‘|ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ ) 
ğ´ğ‘ğ‘ğ‘’ğ‘ğ‘¡ğ‘ğ‘›ğ‘ğ‘’Â ğ‘ğ‘Ÿğ‘œğ‘ğ‘ğ‘ğ‘–ğ‘™ğ‘–ğ‘¡ğ‘¦Â = ğ‘(ğ‘‘ğ‘ğ‘¡ğ‘|ğ‘›ğ‘’ğ‘¤Â ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ )ğ‘(ğ‘‘ğ‘ğ‘¡ğ‘|ğ‘ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘œğ‘¢ğ‘ Â ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ )",1050,semantic
d25cced2-c6d3-49c0-9ebf-bb5a7e7c498f,Bayes MCMC Models.pdf,CSCI_83,16,"MCMC and the Metropolis-Hastings Algorithm
Eventually, the M-H algorithms converges to the posterior distribution
M-H random sampling algorithm is far more sample efï¬cient than naive grid sampling
Consider that the M-H algorithm probabilistically samples the parameter space
Preferentially sample high density areas
Not every point on a grid
Important properties of the Metropolis-Hastings MCMC algorithm include:
The M-H algorithm is guaranteed to eventually converge to the underlying distribution, but convergence can be quite slow
High serial correlation from one sample to the next in chain gives slow convergence",618,semantic
f5fb7f8c-4080-4fb7-867e-6bb1a03fcae6,Bayes MCMC Models.pdf,CSCI_83,17,"MCMC and the Metropolis-Hastings Algorithm
Poor convergence arises from low sample efï¬ciency
Algorithm must be â€˜tunedâ€™ to ensure sample efï¬ciency
Tuning ï¬nds a good dispersion parameter value for the state sampling distribution
Dispersion parameter determines the size of the jumps the algorithm makes
Example, for Normal distribution pick the variance 
 is too small, the chain only slowly searches
 is too big, chain has are large jumps which slows convergence
ğœ2
ğœ2
ğœ2",471,semantic
5f441804-5ba2-4fdd-8125-0e29f94f033a,Bayes MCMC Models.pdf,CSCI_83,18,"M-H algorithm example
Letâ€™s try a simple example, ï¬nd an estimate of the posterior density of a bivariate Normal distribution
Random samples from a bivariate Normal distribution
ğœ‡=[ ].5.5
ğ¶ğ‘œğ‘£ğ‘ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘ğ‘’=[ ]1,.6.6,1",210,semantic
a15529cd-b0b5-4852-8536-d53b3fc5e5c4,Bayes MCMC Models.pdf,CSCI_83,19,"M-H algorithm example
And, the marginal distributions of the variables
Marginal distributions of bivariate Normal samples",121,semantic
b11f300b-27a6-4a96-91a1-64b2b54e7f96,Bayes MCMC Models.pdf,CSCI_83,20,"M-H algorithm example
Now, we are ready to sample these data using the M-H MCMC algorithm
The algorithm is
1. Compute the bi-variate Normal distribution likelihood
2. Initialize the chain
3.",190,semantic
a9528ca5-0c69-4a1e-9c2a-6359d02260cf,Bayes MCMC Models.pdf,CSCI_83,20,"Initialize some hyperparameters statistics
4. Sample the likelihood of the data using the M-H algorithm. MCMC samples from bivariate Normal distribution",152,semantic
202d7b6d-5650-4c7c-a871-d640a7ab2f98,Bayes MCMC Models.pdf,CSCI_83,21,"M-H algorithm example
Plot the ï¬rst 500 samples
First 500 MCMC samples from bivariate Normal distribution
Notice the long â€˜tailâ€™ on the sampled distribution from the burn-in period.",181,semantic
6584a330-7614-4850-b423-3343e8751136,Bayes MCMC Models.pdf,CSCI_83,22,"M-H algorithm example
Marginal distributions of the MCMC samples, less ï¬rst 500
First 500 MCMC samples from bivariate Normal distribution
These marginals are similar to the original distribution",194,semantic
d98d5854-fa52-4285-9015-c72050e553f8,Bayes MCMC Models.pdf,CSCI_83,23,"Convergence and sampling efï¬ciency of MCMC
How can we understand the onvergence properties of the M-H MCMC sampler
MCMC sampling generally convergences to the underlying distribution, but can be slow
In some pathological cases, convergence may not occur at all
The acceptance rate and rejection rate are key convergence statistics for the M-H algorithm
Low acceptnce or high rejection rate are signs of poor convergence
Too few rejections, indicate that the algorithm is not exploring the parameter space sufï¬ciently
Trade-off between these statistics is controlled by the dispersion of the sampling distribution
Unfortunately, there are few useful rules of thumb one can use
For our example these statistics are fairly good:
ğ´ğ‘ğ‘ğ‘’ğ‘ğ‘¡ğ‘ğ‘›ğ‘ğ‘’Â ğ‘Ÿğ‘ğ‘¡ğ‘’=0.81
ğ‘…ğ‘’ğ‘—ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘›Â ğ‘Ÿğ‘ğ‘¡ğ‘’=0.19",766,semantic
ed9b7d51-761c-4b4d-8da4-8beb89cf2670,Bayes MCMC Models.pdf,CSCI_83,24,"Evaluation of MCMC sampling
Trace plot of the samples displays the sample value of the parameter with sample number
Trace plots for two variables from the M-H algorithm
The traces show sampling around the highest density values of the parameters, indicating good convergence",274,semantic
45390f16-0d19-453d-a7f9-e3265e94557a,Bayes MCMC Models.pdf,CSCI_83,25,"Evaluation of MCMC sampling
An autocorrelation plot shows how a sample value is related to the previous samples
The autocorrelation of the Markov chain sampling
Autocorrelation of Markov chain for the two parameters
Notice that the autocorrelation dies off fairly quickly with lag
Low auto correlation indicates good sampling efï¬ciency",335,semantic
e179bc9b-4554-41c1-94d0-d039d89bb6a4,Bayes MCMC Models.pdf,CSCI_83,26,"Evaluation of MCMC sampling
We can relate sampling efï¬ciency to the autocorrelation of the samples
Intuitively, uncorrelated samples provide maximum information on the distribution sampled
With signiï¬cant autocorrelation, the new information gathered per-sample is reduced
Effective sample size or ESS.is the ratio between the number of samples adjusted for the autocorrelation and the hypothetical number ofuncorrelated samples, N
ESS close to N indicates low autocorrelation and high sample efï¬ciency
ğ¸ğ‘†ğ‘†= ğ‘1+2 ğ´ğ¶ğ¹(ğ‘˜)âˆ‘ğ‘˜",521,semantic
3a2f9d39-b6b6-4dc0-859a-42dfe3509e41,Bayes MCMC Models.pdf,CSCI_83,27,"Other MCMC Sampling Algorithms
A number of other powerful MCMC sampling algorithms have been developed
The M-H can suffer from slow convergence for several reasons
Generally has fairly high serial correlation and low ESS
Must â€˜tuneâ€™ the state selection probability distribution
As a result of these limitations, other MCMC sampling methods have been proposed in a quest to improve sample efï¬ciency including:
Gibbs sampling
No U turn sampling (NUTS)",449,semantic
d04601f2-ca50-4af2-a148-9de17d38e340,Bayes MCMC Models.pdf,CSCI_83,28,"Gibbs sampling
Gibbs sampler is an improved MCMC sampler which speeds convergence
Named for the 19th Century physicist Josiah Willard Gibbs; inspired by statistical mechanics
Gibbs sampler samples each dimension of the parameter space sequentially in a round-robin manner
M-H algorithm attempts jumps across all dimensions of the parameter space. Compared to the M-H, Gibbs sampling reduces serial correlation through round-robin sampling
Update along each dimension approximately orthogonal to the preceding sampled dimension
There are no tuning parameters since sampling is based on the marginals of the likelihood.",617,semantic
38b8d2ba-4b10-42c6-96e9-2e4c2cf39bb2,Bayes MCMC Models.pdf,CSCI_83,29,"Gibbs sampling
The basic Gibbs sampler algorithm has the following steps:
1. For an N dimensional parameter space, , ï¬nd a random starting point
2. In order, , assign the next dimension to sample, starting with dimension ; actual order not important
3.",252,semantic
94fb0959-f8c8-452b-96c2-9e07fc6eac3e,Bayes MCMC Models.pdf,CSCI_83,29,"Sample the marginal distribution of the parameter given the observations, , and other parameter values: 
4. Repeat steps 2 and 3 until convergence
{, ,â€¦, }ğœƒ1ğœƒ2 ğœƒğ‘
{1,2,3,â€¦,ğ‘} 1
ğ· ğ‘(|ğ·, , ,â€¦, )ğœƒ1 ğœƒ2ğœƒ3 ğœƒğ‘",202,semantic
a503a779-d084-4f46-8c3d-6169d7dcb132,Bayes MCMC Models.pdf,CSCI_83,30,"Hamiltonian MCMC
The Hamiltonian sampler was proposed in 1987 (Duane, et.al.) and uses a simple idea from classical mechanics
Attempt to improve convergence of Metropolis-Hastings algorithm
M-H algorithm is slow converging random walk
Leads to low ESS
Hamiltonian MCMC attempts to create a better directed sampling path
Uses an analogy with classical physics
Constrains the search to favor high-density parts of the posterior distribution
Imagine that the posterior density is like a hilly landscape
We want to sample around the high spots, the maximum density points
We call density potential
We roll an imaginary ball around the landscape to the highest potential energy points
Ball has position and velocity in the space
These high density points attract the ball - a â€˜ï¬eld of attractionâ€™",791,semantic
bcf2320c-e7ae-4925-81e4-b1b2a4ec0448,Bayes MCMC Models.pdf,CSCI_83,31,"Hamiltonian MCMC
The Hamiltonian sampler uses a simple idea from classical mechanics
Ball has potential energy and kinetic energy
Potential energy determined by probability density
Kinetic energy determined by rate of change of position, velocity
Hamiltonian arise from the principle of conservation of energy
Where:
 momentum vector
 position vector
 kinetic energy
 potential energy
The Hamiltonian of the system must remain constant over the sample space
ğ™·(ğ‘,ğ‘)=ğ™º(ğ‘,ğ‘)+ğš…(ğ‘)
ğ‘=
ğ‘=
ğ™º(ğ‘,ğ‘)=
ğš…(ğ‘)=",496,semantic
355b1f51-3b74-4083-99a4-7a5cb0659c5b,Bayes MCMC Models.pdf,CSCI_83,32,"Hamiltonian MCMC
The Hamiltonian sampler uses a simple idea from classical mechanics
We can relate position and momentum to the density we want to sample
The Boltzmann distribution of the Hamiltonian
Where:
 normalization or partition coefï¬cient
 temperature
ğ‘(ğ‘,ğ‘)=ğ‘’âˆ’ğ™·(ğ‘,ğ‘)ğ‘˜ğ‘‡
ğ‘˜=
ğ‘‡=",282,semantic
86e1d9ed-781f-4429-b66e-7e7e3c35b59d,Bayes MCMC Models.pdf,CSCI_83,33,"Hamiltonian MCMC
The Hamiltonian sampler uses a simple idea from classical mechanics
Hamiltonian arise from the principle of conservation of energy
Solve for velocity and momentum as system of coupled differential equations:
Notice that HMCMC only works for distributions with ï¬nite derivatives
Works for nearly all continuous distributions
Cannot be applied to discrete distributions
The above is rather intimidating, and we skip the details! ğ™·(ğ‘,ğ‘)=ğ™º(ğ‘,ğ‘)+ğš…(ğ‘)
ğ‘‘Â ğ‘ğ‘‘Â ğ‘¡ğ‘‘Â ğ‘ğ‘‘Â ğ‘¡
= = +âˆ‚ğ™·âˆ‚ğ‘ âˆ‚ğ™ºâˆ‚ğ‘ âˆ‚ğš…âˆ‚ğ‘
= = +âˆ‚ğ™·âˆ‚ğ‘ âˆ‚ğ™ºâˆ‚ğ‘ âˆ‚ğš…âˆ‚ğ‘",515,semantic
ef693bbf-febc-490a-9b4f-16456ee7271c,Bayes MCMC Models.pdf,CSCI_83,34,"Hamiltonian MCMC
Hamiltonian MCMC is an extension of the M-H algorithm with steps:
1. Sample 
2. Simulate  and  for a  time steps to time , using the coupled differential equations
3.",183,semantic
dc39d3a4-54f7-478a-b935-3c2f74731aca,Bayes MCMC Models.pdf,CSCI_83,34,"Get  as the new proposed state
4. Apply the M-H acceptance criteria to 
Must ï¬nd good value of  to tune the algorithm
 too small, search is only local
 too large, search makes large jumps
Hard to ï¬nd good hyperparameter value in practice
ğ‘âˆ¼ ğ™½(0,ğœ)
ğ‘ğ‘¡ ğ‘ğ‘¡ ğ¿ ğ‘‡
ğ‘ğ‘‡
ğ‘ğ‘‡
ğ¿
ğ¿
ğ¿",269,semantic
56fbc46f-23d7-4a7a-88ee-c16718101dee,Bayes MCMC Models.pdf,CSCI_83,35,"No U-Turn Sampler
NUTS represents the state of the art in MCMC samplers: Hoffman and Gelman 2014
PyMC3 package uses the NUTS MCMC algorithm. NUTS improves on HMC
Run time forward and backward when solving coupled differential equations
Find equilibrium at the no U-turn point
Equilibrium point determines  automatically
Eliminates tuning
Why even discuss other samplers when we have the NUTS
NUTS works well in many common continuous distribution cases, it is not guaranteed to converge
In some cases a Gibbs sampler works better
Use M-H for discrete distributioins
ğ¿",567,semantic
4b853435-2459-40e6-9d26-ee62c28b2c31,Bayes MCMC Models.pdf,CSCI_83,36,"Hamiltonian MCMC
Hamiltonian MCMC is a complex algorithm - some resources for additional details
A Conceptual Introduction to Hamiltonian Monte Carlo, Michael Betancourt, 2017
MCMC using Hamiltonian dynamics, Radford Niel, 2012
Hamiltonian Monte Carlo explained, Alex Rogozhnikov, 2016",285,semantic
ffd47a73-5229-420d-bf21-5cb2f431a05a,Bayes MCMC Models.pdf,CSCI_83,37,"Key Steps for MCMC Bayes Modeling
Focus of modern Bayes modeling is no evalaution
1. Deï¬ne the problem
2. Collect the dataset
3.",128,semantic
cf217a74-1044-4bda-9d48-4ba8fb629353,Bayes MCMC Models.pdf,CSCI_83,37,"Deï¬ne a model, including priors
4. MCMC sample the model
5. Verify MCMC sampling convergence and sufï¬ciency
6. Prior predictive checks - are the priors reasonable for the problem? 7. Posterior predictive checks - Do the posterior predictions agree with the observed response? Packages like ArviZ are dedicated to evaluation of Bayes MCMC models. Multiple examples diagnostics for the breakdown of MCMC sampling can be found in an online Appendix to a seminal paper Vehtari et.al., 2019",485,semantic
16f29771-de58-4542-88f1-d54e051041fb,Bayes MCMC Models.pdf,CSCI_83,38,"Constructing an MCMC Model with PyMC
Deï¬ne and construct a simple linear regression model
Simple dataset:
Dependent variable 
Independent variables , 
Data for regression example
ğ‘Œ
ğ‘¥1ğ‘¥2",185,semantic
2dbc9f39-22e7-4ace-8d57-49165d179fa2,Bayes MCMC Models.pdf,CSCI_83,39,"Constructing an MCMC Model with PyMC
Deï¬ne and construct a simple linear regression model
Model has 4 parameters
Deï¬ne prior distributions for these parameters:
 is the intercept term
 are the partial slopes
 is the standard deviation
, indicates a half Normal distribution
ğ›½0
ğ›½1
ğ›½2
ğœ
âˆ¼ ğ™½(0,2)âˆ¼ ğ™½(0,2)âˆ¼ ğ™½(0,2)âˆ¼ |ğ™½(0,1)
ğ›½0
,ğ›½1ğ›½2
ğœ
|ğ™½(.,.)",337,semantic
672d28ca-9ab4-40b1-9319-27a130cacfbb,Bayes MCMC Models.pdf,CSCI_83,40,"Constructing an MCMC Model with PyMC
Deï¬ne and construct a simple linear regression model
Model has 4 parameters, 
Likelihood model:
Start with independent variables, ,
Sample posterior of parameters
Posterior distribution (likelihood) of observations, :
Expected value of the response,  is computed deterministically:
[, , ,ğœ]ğ›½0ğ›½1ğ›½2
,ğ‘¥1ğ‘¥2
ğ‘Œ
ğ‘Œâˆ¼ ğ™½(ğœ‡,ğœ)
ğœ‡
ğœ‡= + âˆ— + âˆ— ğ›½0 ğ›½1 ğ‘¥1 ğ›½2 ğ‘¥2",379,semantic
b29a8faf-52d8-441f-ab85-23b3bb750671,Bayes MCMC Models.pdf,CSCI_83,41,"Constructing an MCMC Model with PyMC
Deï¬ne and construct a simple linear regression model
Use Python PyMC package to construct the model
with pymc3.Model() as regression_model:    # Priors for unknown model parameters
    betas = pymc3.Normal(""betas"", mu=0 , sigma=2 , shape=3 )    sigma = pymc3.HalfNormal(""sigma"", sigma=1 )
    # Deterministic expected value of outcome
    mu = betas[0 ] + betas[1 ] * x1 + betas[2 ] * x2
    # Likelihood (sampling distribution) of observations
    Y_obs = pymc3.Normal(""Y_obs"", mu=mu, sigma=sigma, observed=Y)",547,semantic
48a7903a-2c73-42b5-a600-233c0da91dc5,Bayes MCMC Models.pdf,CSCI_83,42,"Prior Predictive Checks
Prior predictive checks to verify prior distribution choice
Prior predictive checks verify that choices of prior distributions are at least reasonable
Compare the distribution of the response variable with the posterior distribution generated by priors. Is the posterior distribution in the absence of evidence (data)
Prior predictive response vs response variable distribution
Example looks promising
Distribution shape is similar
No noticeable anomalies
Higher dispersion of prior check distribution is desired to not restrict the solution",565,semantic
bbbddde2-5f5c-4c74-81d5-92aaca9e76bc,Bayes MCMC Models.pdf,CSCI_83,43,"Prior Predictive Checks
Prior predictive checks to verify prior distribution choice
Prior predictive checks verify that choices of prior distributions are at least reasonable
Verify that we are happy with the density of the  and  priors
Prior density of Betas and sigma
Shape of these distributions seems reasonable
Range of possible values will not restrict the solution
No anomalies in the shape of the distribution
ğ›½ ğœ",421,semantic
6f31d442-4a0f-4c92-9a9c-6f800c355650,Bayes MCMC Models.pdf,CSCI_83,44,"Sampling and Inference with Model
Evaluation the sampled posterior and perform inference
We sample the posterior using one or more Markov chains
Examine the traces (path of chains)
Traces and posterior density of MCMC traces
Traces and density of chains are similar
No divergence between traces
2 s have limited uncertainty
 has high uncertainty as expected
 has limited uncertainty
ğ›½
ğ›½2
ğœ",389,semantic
35896eb3-8a1a-441f-8556-64e0797ca70b,Bayes MCMC Models.pdf,CSCI_83,45,"Sampling and Inference with Model
Evaluation the sampled posterior and perform inference
There are a number of ways MCMC sampling can breakdown and not converge;
Example from Vehtari et.al., 2021, a seminal paper on evaluation of MCMC",234,semantic
dcb89e0b-eef0-4ca2-be4a-9ce23f609e4c,Bayes MCMC Models.pdf,CSCI_83,46,"Sampling and Inference with Model
We must verify that the samples are representative
Table of MCMC convergence statistics
A lot of information in this summary table
1. The mean MCMC error
Estimated error tells us how much error the MCMC sampling has introduced
2.",263,semantic
0706e4e9-6735-41c9-a323-4a6aa4cd0114,Bayes MCMC Models.pdf,CSCI_83,46,"The standard deviation (sd) of the mean error of the posterior distribution
3. Metrics of effective sample size (ESS)
ESS of bulk (middle) portion of posterior distributions
ESS for tails ESS of posterior distributions
4. The Gelman-Rudin statistic () measures the ratio of the variance shrinkage between chains to the variance shrinkage withinchains
Gelman-Rudin statistic should converge to 1.0
If all chains converge, the reduction in variance between chains and within the chains should is same
ğ‘…Ì‚",501,semantic
ed885822-af6e-4e21-a56c-beea3f6b09f2,Bayes MCMC Models.pdf,CSCI_83,47,"Sampling and Inference with Model
We must verify that the samples are representative
Plot the MCSE by quantile for each model parameter
MCSE by quantile for each model parameter
Ideally want MCSE to be small and uniform with quantile
Notice the MCSE is higher for outer quantiles
Not an unusual situation since less sampling in thin tails of distributions
Overall MCSE is low, even in tails",390,semantic
a2ebd9b1-be40-4bf6-ba27-81e14b6afdae,Bayes MCMC Models.pdf,CSCI_83,48,"Sampling and Inference with Model
We must verify that the samples are representative
Plot the ESS locally and by quantile for each model parameter
ESS locally and by quantile for each model parameter
Ideally want ESS to be large and uniform with quantile
Local ESS is fairly uniform with quantile
Notice the quantile ESS is lower for outer quantiles
Not an unusual situation since less sampling in thin tails of distributions
Overall ESS is reasonably high, even in tails",471,semantic
69dd44aa-7b1a-4341-b2b6-3ad0b47a44fc,Bayes MCMC Models.pdf,CSCI_83,49,"Sampling and Inference with Model
Evaluate the sampled posterior and perform inference
Examine HDIs of model parameters by trace with tree plot
HDIs of model parameters by trace
HDIs are similar for each parameter by trace
HDIs conï¬rm inferences on uncertainty of the model parameters",284,semantic
36e48d28-5e97-4352-bc39-b5cbe4267e2d,Bayes MCMC Models.pdf,CSCI_83,51,"Sampling and Inference with Model
Evaluate the sampled posterior and perform inference
How can we interpret the results? Posterior distribution of parameters with HDIs
Interpret s by examining HDIs
 is the mean response for the centered independent variable
 is partial slope with respect to 
 coefï¬cient large compared to  with high uncertainty
 has small value with limited uncertainty
ğ›½
ğ›½0
ğ›½1 ğ‘¥1
ğ›½2 ğ›½1
ğœ",406,semantic
52932fa4-7350-49f0-bbe5-67580dbd90d0,Bayes MCMC Models.pdf,CSCI_83,52,"Posterior Predictive Checks
We must check that predictions from the model are reasonable
Does the posterior distribution look like the distribution of the observed response? Can simply compare density plots
Apply a test statistic 
Bayesian p-value
Bayesian u-value
Others
ğ‘‡",273,semantic
eb6b63ae-0136-449e-95f4-6fc7a7d4c901,Bayes MCMC Models.pdf,CSCI_83,53,"Posterior Predictive Checks
We must check that predictions from the model are reasonable
Base statistic on posterior predictive distribution
Approximate PPD by resampling from the posterior distribution:
Where: realization drawn from the posterior distribution observation draw from posterior distribution with parameters  posterior distribution of model parameters, 
We can sample the posterior as many times as required to get better estimates of 
ğ‘( |ğ‘¦)= ğ‘( |ğœƒ)ğ‘(ğœƒ|ğ‘¦)ğ‘¦ğ‘Ÿğ‘’ğ‘ âˆ‘ğ‘–
ğ‘¦ğ‘Ÿğ‘’ğ‘
=ğ‘¦ğ‘Ÿğ‘’ğ‘
ğ‘¦=ğ‘( |ğœƒ)=ğ‘¦ğ‘Ÿğ‘’ğ‘ ğœƒğ‘(ğœƒ|ğ‘¦)= ğœƒ
ğ‘( |ğ‘¦)ğ‘¦ğ‘Ÿğ‘’ğ‘",524,semantic
f764eeaa-cc43-4553-811c-ed1de239b3cd,Bayes MCMC Models.pdf,CSCI_83,54,"Posterior Predictive Checks
We must check that predictions from the model are reasonable
Posterior predictive distribution is compared to the distribution of observations
Posterior predictive distribution vs. observed responses
Agreement is generally good between posterior predictive and actual observations
Average predictive is heavier in the tails",351,semantic
1b6d7fce-1f6f-44e5-a63d-a7deee61a103,Bayes MCMC Models.pdf,CSCI_83,55,"Posterior Predictive Checks
We must check that predictions from the model are reasonable
Bayesian p-value is a test statistic, , for the distribution differences between observed and predicted responses:
Intuitively, for representative posterior,  over many draws of 
 half the time
 half the time
Distribution should be symmetric
ğ‘‡
ğ‘=ğ‘( <ğ‘¦|ğ‘¦)ğ‘¦ğ‘Ÿğ‘’ğ‘
ğ‘=0.5 ğ‘¦ğ‘Ÿğ‘’ğ‘
<ğ‘¦|ğ‘¦ğ‘¦ğ‘Ÿğ‘’ğ‘
>ğ‘¦|ğ‘¦ğ‘¦ğ‘Ÿğ‘’ğ‘",376,semantic
c4637a09-0545-4b8e-9fd7-e05bf570eadd,Bayes MCMC Models.pdf,CSCI_83,56,"Posterior Predictive Checks
We must check that predictions from the model are reasonable
Posterior predictive Bayesian p-value
Distribution of Bayesian p-values
Plot is centered on 0.5
Distribution is symmetric",210,semantic
5892cf62-36d6-4bbf-9511-25193b59c06c,Bayes MCMC Models.pdf,CSCI_83,57,"Posterior Predictive Checks
We must check that predictions from the model are reasonable
Posterior predictive Bayesian u-value is also known as the marginal p-value:
U-value computed for speciï¬c (ordered) observation 
 should be centered on 1.0
 should be close to uniformly distributed
Distribution of Bayesian u-values
The values are close to 1.0 across all values of the observatons
=ğ‘( <|ğ‘¦)ğ‘ğ‘– ğ‘¦ğ‘Ÿğ‘’ğ‘ğ‘– ğ‘¦ğ‘–
ğ‘¦ğ‘–
ğ‘ğ‘–
ğ‘ğ‘–",414,semantic
9346f735-3ab0-4e54-9824-01dfb5dc7645,Bayes MCMC Models.pdf,CSCI_83,58,"British Coal Mine Disasters
Analysis of change point in British coal mine disasters
Toward the end of 19th Century was growing public awareness of the rising coal mine deaths
Mine disaster deï¬ned as incident leading to 6 or more deaths
As early as 1886, Royal Commission published recommendations for safety practices
Adoption of these practices remained voluntary
British Coal Mines Act of 1911 ï¬nally codiï¬ed mine safety procedures into law.",443,semantic
42dcc440-94ab-4dba-9619-2aa7bf8f02cc,Bayes MCMC Models.pdf,CSCI_83,59,"British Coal Mine Disasters
Analysis of change point in British coal mine disasters
Famous dataset from Kaggle, and other sources
Goal to create a model to quantify the differences in the rate of mine disasters before and after the introduction of safety practices
Example of a point process model
Point process model has an intensity or rate of occurrence of events, e.g. mine disasters
Number events over a time period for this type of point process is iid
The number of events per time period are a iid draw from some probability distribution
Because of the iid nature of the point process, no autoregressive (AR) effect",623,semantic
2fbbb38a-8161-4146-aa6f-fdd2855e6311,Bayes MCMC Models.pdf,CSCI_83,60,"British Coal Mine Disasters
Analysis of change point in British coal mine disasters
iid nature of the point process does not mean the distribution of the process cannot change in time
Change points or switch points in time are quite common in the real-world
Rates changes at change point
Rate constant betwen change point
Identifying these change points is often a challenging problem",384,semantic
0e51b9f4-7550-4ad9-b945-f033710545f3,Bayes MCMC Models.pdf,CSCI_83,61,"British Coal Mine Disasters
Analysis of change point in British coal mine disasters
Number of disasters per year 1851-1962
Two related questions we can ask about these data
At what point in time was there a signiï¬cant change in intensity, the switch point? Not obvious, changes in safety practice were adopted over about 25 years
How signiï¬cant was the reduction in the rate?",375,semantic
d9f94529-eeb4-4756-b031-032687016fec,Bayes MCMC Models.pdf,CSCI_83,62,"British Coal Mine Disasters
Analysis of change point in British coal mine disasters
Poisson likelihood model with switch switch point , rate before,  and after :
Prior distributions of parameters, uniform and exponential:
ğ‘  ğ‘’ ğ‘™
âˆ¼ ğ‘ƒğ‘œğ‘–ğ‘ (),Â  {ğ·ğ‘¡ ğ‘Ÿğ‘¡ ğ‘Ÿğ‘¡ ğ‘’,Â ğ‘¡<ğ‘ ğ‘™,Â ğ‘¡â‰¥ğ‘ 
ğ‘ ğ‘’ğ‘™
âˆ¼ ğ‘¢ğ‘›ğ‘–ğ‘“( , )ğ‘¡ğ‘šğ‘–ğ‘›ğ‘¡ğ‘šğ‘ğ‘¥
âˆ¼ ğ‘’ğ‘¥ğ‘(2)âˆ¼ ğ‘’ğ‘¥ğ‘(2)",302,semantic
502c8c7b-251c-4d1a-b1bd-4dc335a4ab36,Bayes MCMC Models.pdf,CSCI_83,63,"British Coal Mine Disasters
Analysis of change point in British coal mine disasters
Deï¬ne model using PyMC
with pymc3.Model() as disaster_model:    # Uniform prior on the switch point  
    switchpoint = pymc3.DiscreteUniform(""switchpoint"", lower=disaster_data.Year.min(), upper=disaster_data.Year.max())    # More informative prior based on domain knowledge    #    switchpoint = pymc3.Triangular(""switchpoint"", lower=disaster_data.Year.min(), upper=disaster_data.Year.max(), c=1900)
    switchpoint = pymc3.DiscreteUniform(""switchpoint"", lower=disaster_data.Year.min(), upper=disaster_data.Year.max())
    # Priors for pre- and post-switch rates 
    early_rate = pymc3.Exponential(""early_rate"", 2 . 0 )    late_rate = pymc3.Exponential(""late_rate"", 2 . 0 )
    # Poisson rate switch for years before and after current
    rate = pymc3.math.switch(switchpoint >= disaster_data.Year, early_rate, late_rate)
    disasters = pymc3.Poisson(""disasters"", rate, observed=disaster_data.Count)",986,semantic
1d5de063-24da-4f91-bf26-6171f26bad87,Bayes MCMC Models.pdf,CSCI_83,64,"British Coal Mine Disasters
Analysis of change point in British coal mine disasters
Distributions and traces from sampling
HDI of parameters by trace
Good agreement between traces",179,semantic
792f72f6-fa59-4a21-8609-afe23cadb4d8,Bayes MCMC Models.pdf,CSCI_83,65,"British Coal Mine Disasters
Analysis of change point in British coal mine disasters
Examine the posterior HDI of the parameters for each trace",142,semantic
ac06969f-c2fd-467e-876c-a0a39d6e0e84,Bayes MCMC Models.pdf,CSCI_83,66,"HDI of parameters by trace
The HDIs are consistent across traces",64,semantic
64f8ba80-1573-4672-bfe4-15e10c13dffd,Bayes MCMC Models.pdf,CSCI_83,67,"See notebook for MCMC, Prior and Posterior checks",49,semantic
75a31ffa-3632-48be-ab04-45c19768e508,Bayes MCMC Models.pdf,CSCI_83,68,"British Coal Mine Disasters
Analysis of change point in British coal mine disasters
Examine the posterior HDI of the parameters
Posterior distribution of parameters with HDI
HDI of switch point is in reasonable range
Signiï¬cant difference in HDI of rates",254,semantic
16df5d65-aa6a-4e02-8f25-d09178531d3b,Bayes MCMC Models.pdf,CSCI_83,69,"British Coal Mine Disasters
Analysis of change point in British coal mine disasters
Posterior Predictive checks
Posterior predictive checks",139,semantic
11fd2382-f2b9-4e5e-ab90-327741f3e240,Bayes MCMC Models.pdf,CSCI_83,70,"Summary
For complex Bayesian models we need a computationally efï¬cient aproximation
Grid sampling is inefï¬cient
MCMC sampling is based on Markov chains
Markov process is memoryless
Sampling converges to probability distribution
Several MCMC sampling methods have been developed
Metropolis-Hastings (M-H) algorithm uses random sampling with acceptance probability
Gibbs sampling round robins on the dimensions of the parameter space
NUTS
NUTS is the state of the art MCMC algorithm
Uses a ï¬eld of attraction to the highest density parts of the parameter space
No hyperparameters to tune",585,semantic
a95ff823-a744-415e-ba39-63e2d750ce71,Bayes MCMC Models.pdf,CSCI_83,71,"Summary
Performance metrics of MCMC sampling
Sample efï¬ciency
Serial correlation reduces sample efï¬ciency
ESS
Convergence
Multiple chains should converge to the similar distributions
Want between chain and within chain variance to be the same",242,semantic
5ef3055a-76d4-48ff-bff2-18019bbe62b7,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,0,"Introduction to Inference and Conï¬dence Intervals
Steve Elston
10/06/2022",73,semantic
fcd18f02-853c-4ed8-83e7-882bfcf9f6e0,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,1,"Review
Likelihood is a measure of how well a parametric model ï¬ts a data sample
In most practical cases, we work with the log likelibood
The maximum likelihood for the model parameters is achieved when two conditions are met:
Gradient of the log-likelihood is known as the score function
 (ğ—|Â ğœƒ)= ğ‘“(|ğœƒ)âˆğ‘–=1
ğ‘›
ğ‘¥ğ‘–
ğ‘™(ğ—|Â ğœƒ)=ğ‘™ğ‘œğ‘”( (ğ—|Â ğœƒ))= ğ‘™ğ‘œğ‘”(ğ‘“(Â |Â ))âˆ‘ğ‘—
ğ‘¥ğ‘— ğœƒâƒ—Â 
=0âˆ‚Â ğ‘™(ğ—|Â ğœƒ)âˆ‚ğœƒ
<0Â ğ‘™(ğ—|Â ğœƒ)âˆ‚2
âˆ‚ğœƒ2",385,semantic
c63f1107-2610-4953-a520-ca85b12383ed,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,2,"Review
Matrix is the observed information matrix of the model, . The more negative the values of the second partial derivatives, the greater the curvature of the log-likehihood
Fisher information or expected information is the expectation over the second derivative of the observed information
Fisher information relates to the score function as its variance
For large sample, , take the expectation over :
The maximum likelihood estimate of model parameters, , is Normally distributed
The larger the Fisher information, the lower the variance of the parameter estimate
Greater curvature of the log likelihood function gives more certain the parameter estimates
The variance of the parameter estimate is inversely proportional to the number of samples, 
 ()ğœƒâƒ—Â 
 ()=âˆ’ğ„{ ()}=âˆ’ğ„{ }ğœƒâƒ—Â  ğœƒâƒ—Â  Â ğ‘™(ğ—|ğœƒ)âˆ‚2
âˆ‚ğœƒ2
Â Â  (0,1/ )âˆ‚Â ğ‘™(ğœƒ)âˆ‚ğœƒ âˆ¼ Ë™  ğœƒ
ğ‘›â†’âˆ ğ—
 (ğœƒ, )ğœƒÌ‚Â âˆ¼ Ë™ 1ğ‘› (ğœƒ)
ğœƒÌ‚Â 
ğ‘›",857,semantic
571a0288-a268-4b7e-8ccc-9d8c4759769f,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,3,"Review
The gradient descent method ï¬nds the maximum of the log-likelihood function by following the gradient â€˜uphillâ€™
Given a current parameter estimate vector at step n , , the improved parameter estimate vector, , is found:
The hyperparameter  is the learning rate or step size
Stochastic optimization uses a Bernoulli random sample of the data to estimate the expected update of the model weights
Where,  is the expected value of the gradient given the Bernoulli sample of the data, . Empirically, SGD has good convergence properties
ğœƒÌ‚Â ğ‘› ğœƒÌ‚Â ğ‘›+1
= +ğ›¾Â  Â ğ‘™()ğœƒÌ‚Â ğ‘›+1 ğœƒÌ‚Â ğ‘› âˆ‡ ğœƒ ğœƒÌ‚Â 
ğ›¾
= +ğ›¾Â  [ Â ğ‘™()]ğœƒÌ‚Â ğ‘›+1 ğœƒÌ‚Â ğ‘› ğ¸ğ‘‘ğ‘ğ‘¡ğ‘ğ‘Ì‚Â  âˆ‡ ğœƒ ğœƒÌ‚Â 
[]ğ¸ğ‘‘ğ‘ğ‘¡ğ‘ğ‘Ì‚Â  ğ‘‘ğ‘ğ‘¡ğ‘ğ‘Ì‚",640,semantic
623f0236-a269-4131-b02a-540419bec4c4,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,4,"Review
The maximum likelihood estimator has a number of important limitations, including
Incorrect model and complex distributions
Parameter near limits
High dimensional problems
Correlated features",198,semantic
1ebf5bd6-a335-4686-a1d1-636ae57034f2,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,5,"Introduction to Statistical Inference
All statistical inference has uncertainty
Characterization of uncertainty is a goal of statistical inference
Any model using real-world data has inherent uncertainty
Statistical inference seeks to avoid being fooled by randomness; A catchy title of a book
A very few examples of false inference from randomness
Hypothesis Fooled by Randomness
How certain are you that eating fast food improves your health? Some of my friends are doing great on this diet
How much conï¬dence should we have that a marketing campaign increased sales?Sales are up in the last month since the campaign started
How effective is a certain stock trading strategy for actually improving returns? Traders using this strategy have made money recently
How good are the model parameter estimates?",805,semantic
c6b88b6b-9e48-4b32-ba22-03f71063a4c8,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,5,The model has made accurate predictions so far,46,semantic
98b56c63-3031-425a-8a3c-a5a3af984836,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,6,"Applications of Statistical Inference
Confusingly, the term statistical inference is applied in many contexts
Some applications of statistical inference include:
Inference on differences in distributions: Are samples drawn from the same distribution or not? - A null hypothesis is the hypothesis that thedistributions are the same
Inference for model parameters and model selection: Statistical models, including machine learning models, have unknown parameters for whichthe values must be estimated- Compute uncertainty in model parameter estimates- Compare model performance
Inference for prediction: In recent decades the distinction between prediction and classical inference has blurred to the point of being irrelevant- A common machine learning example, classiï¬cation, uses decision rules determine the most likely class for each case of input values- inference also used to determine the conï¬dence in predictions",920,semantic
08221610-4afd-4cd1-a352-8ce9cd20027f,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,7,"Types of Hypothesis Tests
Different tests for one sample, two samples or more
Parametric test uses assumptions about the distribution of the data
Non-parametric tests make no distribution assumptions
Select test for discrete or continuous, or numeric or categorical, ordinal, interval, or ratio variables
Tests can compare effect size (averages), variance, goodness of ï¬t, distribution assumptions, or Independence",414,semantic
dc4d736b-ba3f-45b9-9d7f-c4f799eaf033,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,8,"Inference for Hypotheses
Hypothesis testing seeks to answer the question, are the differences in a statistic statistically signiï¬cant?",134,semantic
3d33485e-add9-4703-8d0d-34ea98b0c081,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,8,"Statistical signiï¬cance must not be confused importance to the problem being addressed? In 1922, Ronald A. Fisher warned:
â€œI believe that no one who is familiar, either with mathematical advances in other ï¬elds, or with the range of special biological conditions to be considered, would everconceive that everything could be summed up in a single mathematical formula, however complex.â€",386,semantic
f19f26a6-5def-4ae0-bf60-b94b91ac35f1,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,9,"Inference for Hypotheses
Many statisticians would say that a statistically signiï¬cant result indicates a relationship worthy of further investigation
Examples of further investigation include:
Gather more data, either observational or experimental
Find other variables which might illuminate the relationship under investigation
Consider the theoretical basis of the relationship. E.G., can known science add understanding?",423,semantic
4e472565-6ae8-48b0-815e-59359c5dc083,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,10,"Conï¬dence Intervals; the Key to Inference
In frequentist statistics uncertainty of an inference is expressed in terms of a conï¬dence interval
A conï¬dence interval is deï¬ned as the expected range of a statistical point estimate
A point estimate is the best estimate of a statistic
For example, the maximum likelihood estimate of a model parameter given the data
Two types of of conï¬dence intervals:
Two-sided conï¬dence intervals: express conï¬dence that a value is within some range around the point estimate
One-sided conï¬dence intervals: express conï¬dence that the point estimate is greater or less than some range of values",624,semantic
43222f1e-516d-44a3-8eb5-ffdbd779334b,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,11,"Conï¬dence Intervals; the Key to Inference
Can understand two sided conï¬dence interval by looking at the  and  quantiles of a distribution
Conï¬dence interval corresponds to the span of the distribution between quantiles
Express a two-sided conï¬dence interval for a random variable, , in terms of the probability as:
Example: For Normal distribution the right and left tail probabilities are :
ğ›¼/2 1âˆ’ğ›¼/2
ğ±
1âˆ’ğ›¼=ğ‘ƒ(ğ¿ğ‘œğ‘¤ğ‘’ğ‘ŸÂ ğ¶ğ¼â‰¤ğ±â‰¤ğ‘ˆğ‘ğ‘ğ‘’ğ‘ŸÂ ğ¶ğ¼)
ğ›¼/2
1âˆ’ğ›¼/2=ğ‘ƒ(âˆ’âˆâ‰¤ğ‘¥â‰¤ğ¿ğ‘œğ‘¤ğ‘’ğ‘ŸÂ ğ¶ğ¼),ğ‘ğ‘›ğ‘‘,1âˆ’ğ›¼/2=ğ‘ƒ(ğ‘¢ğ‘ğ‘ğ‘’ğ‘ŸÂ ğ¶ğ¼â‰¥ğ‘¥â‰¥âˆ)",483,semantic
64b6f027-d613-4a35-9d78-fc94465c3c46,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,12,"Conï¬dence Intervals; the Key to Inference
Can understand one sided conï¬dence interval by looking at either  or  quantiles of a distribution
Characterize uncertainty of maximum or minimum value of a random variable
For lower one-sided CI:
For upper one-sided CI:
Example: one-sided CIs of Normal distribution, starting with the lower CI:
Or for the upper conï¬dence interval,
Or,
ğ›¼ 1âˆ’ğ›¼
ğ›¼=ğ‘ƒ(ğ‘¥â‰¤ğ¿ğ‘œğ‘¤ğ‘’ğ‘ŸÂ ğ¶ğ¼)
1âˆ’ğ›¼=ğ‘ƒ(ğ‘¥â‰¤ğ‘ˆğ‘ğ‘ğ‘’ğ‘ŸÂ ğ¶ğ¼)
ğ›¼=ğ‘ƒ(âˆ’âˆâ‰¤ğ‘¥â‰¤ğ‘™ğ‘œğ‘¤ğ‘’ğ‘ŸÂ ğ¶ğ¼)
1âˆ’ğ›¼=ğ‘ƒ(âˆ’âˆâ‰¤ğ‘¥â‰¤ğ‘¢ğ‘ğ‘ğ‘’ğ‘ŸÂ ğ¶ğ¼)
ğ›¼=ğ‘ƒ(ğ‘¢ğ‘ğ‘ğ‘’ğ‘ŸÂ ğ¶ğ¼â‰¥ğ‘¥â‰¥âˆ)",475,semantic
3cdd8ae6-fd49-4890-b9d8-796a13427f51,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,13,"Example; conï¬dence intervals of the Normal distribution
Illustrate the concept of conï¬dence intervals with an example
The cumulative distribution function (CDF) of a standard Normal distribution, 
Double ended arrows with annotation are plotted to illustrate the two-sided 95% conï¬dence interval on the CDF
Horizontal double arrow shows the range of the conï¬dence interval
Vertical double arrow shows the part of the distribution within the conï¬dence intervals
## Confidence Interval, lower: -1.96, upper:  1.96
ğ‘(0,1)",518,semantic
f06d4909-e4bb-45f9-90d0-586cd444bbee,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,14,"Example, Inference for the mean
Where,-  is the standard Normal distribution evaluated at conï¬dence level, -  is the maximum likelihood estiamte of the mean- Standard error is given by -  is the standard deviation estimate-  is the number of samples
ğ¶ğ¼(ğ‘šğ‘’ğ‘ğ‘›)=ğ¶ğ¼()=ğ‘€ğ¿ğ¸(ğœƒ|ğ—)Â Â±Â ğ—Â¯ ğ‘ ğ‘›âˆšğ‘ğ›¼
ğ‘ğ›¼ ğ›¼ğ‘€ğ¿ğ¸(ğœƒ|ğ—) ğ‘ /ğ‘›âˆšğ‘ ğ‘›",303,semantic
1203de73-4163-41c5-874f-caa9bd7957fd,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,15,"Interpretation of Conï¬dence Intervals
How can we interpret the conï¬dence interval? Conï¬dence intervals are with respect to the the sampling distribution of a statistic 
CIs are a measure of variation from sampling alone with probability  - the basis of hypothesis testing! With probability  the sample statistic values computed from resamples of the population, are within the CI
Conï¬dence intervals do not indicate the probability  is in a range
Sampling distribution of unknown population parameter
ğ‘ () Ì‚Â 
ğ›¼
ğ›¼  Ì‚Â ğ‘–
ğ‘ () Ì‚",522,semantic
9c972e92-286d-4827-91ba-416e7482558f,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,16,"Conï¬dence and Hypothesis Testing
Purpose of a statistical test is determining if the value of a chosen test statistic exceeds a cutoff value
Select the cutoff value based on the conï¬dence we wish to have in the test result
Example, by specifying a cutoff of 0.05 we are saying that the probability that the test statistic exceeding the cutoff from random variation alone is 0.05
Once the cutoff value has been set and the test statistic computed, interpret the results:
If the test statistic does not exceed the cutoff value, fail to reject the null hypothesis
If the test statistic exceeds the cutoff reject the null hypothesis",628,semantic
7539aef7-1a06-42df-b2b4-d3a25bf84e55,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,17,"Cutoffs for Hypothesis Tests
Examples of one-sided and two-sided cut-offs
Cutoff is probability, , that variation this great or greater arises from random sampling alone
In other words, cutoff is the conï¬dence we have in rejecting a null hypothesis. A p-value is the probability of the statistic this extreme or more extreme arising from random sampling (random variation)alone
## (0.0, 1.0, 0.0, 1.0)
ğ›¼",403,semantic
d6df208c-020c-4bf0-8ce2-fcafcbc8df88,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,18,"Hypothesis testing steps
The steps required to perform a formal two-sample hypothesis test
State population assumptions of null hypothesis: 
State alternative hypothesis: 
Typically stated in terms of a treatment group vs. control group
Treatment is the factor that differentiates the population
Decide on a signiï¬cance level (probability cutoff or Type I error threshold): e.g. 0.1, 0.05, and 0.01
Data is collected for the different treatments
Treatment used for comparison is the control
Compute the test statistic and evaluate based on the cutoff value
ğ»0
ğ»ğ‘",562,semantic
a4747308-da9e-46ae-94ba-8282b2e73a3b,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,19,"Hypothesis testing steps
Test statistic used a a decision rule
Only two possible outcomes
Reject the null-hypothesis: This is not the same as accepting the alternative hypothesis
Fail to reject the null hypothesis: This is not the same as accepting the null hypothesis
Failing to rejecting the null hypothesis can occur for several reasons
The alternative hypothesis was false to begin with
There is not enough evidence for the effect size
Roughly speaking, the effect size is the difference of the test statistic in populations under the different treatments",559,semantic
a7bd01eb-aaad-4f91-a6bd-ca1b23f8d660,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,20,"the p-value
P-value is the probability, under , that we get a statistic as extreme or more extreme than the one we got
Extreme depends on whether the test is one-tailed or two-tailed
Derive the p-value by computing the statistic(s) and evaluate the quantile of the null distribution, 
Low p-value means our evidence against  is too strong to ignore
How strong it needs to be is controlled by our choice of 
Smaller  means we need stronger evidence to reject ; a more conservative test
Together the p-value and the signiï¬cance threshold  determine whether we reject or fail to reject 
The decision rule of the hypothesis test
ğ»0
ğ»0
ğ»0
ğ›¼
ğ›¼ ğ»0
ğ›¼ ğ»0",645,semantic
60bbec66-a59b-46a5-8dea-5b099f0ded52,Introduction to Inference and Confidence Intervals.pdf,CSCI_83,21,"Summary
Statistical inference seeks to characterize the uncertainty in estimates
Statistics are estimates of population parameters
Inferences using statistics must consider the uncertainty in the estimates
Conï¬dence intervals quantify uncertainty in statistical estimates
Two-sided conï¬dence intervals: express conï¬dence that a value is within some range around the point estimate
One-sided conï¬dence intervals: express conï¬dence that the point estimate is greater or less than some range of values
Hypothesis tests based on the conï¬dence we have that variation in statistic arrises from sampling variation alone",612,semantic
da459734-2bb1-4838-a642-97a1d134850f,Review of Common Probability Distributions.pdf,CSCI_83,0,"Review of Common Probability Distributions
Steve Elston
09/15/2022",66,semantic
dad62bd4-bfc9-4cdb-b66e-11a0f33b2075,Review of Common Probability Distributions.pdf,CSCI_83,1,"Importance of Probability Theory
Probability theory is the basis of statistics, machine learning, and much AI
An understanding of probability theory is an important foundation to understand these methods
In this lesson we will review some basic concepts
Properties of probability distributions
Some commonly used probability distributions - focus on difï¬cult to understand properties
Many texts provide comprehensive introductions to probability theory",452,semantic
57651744-2ee1-44d2-b0e8-d7e296a591c6,Review of Common Probability Distributions.pdf,CSCI_83,2,"Probability Theory Has a Long History
First probability textbook by Jacob Bernoulli, published posthumously in 1713
First probability textbook Credit, Wikipedia commons
Probability theory is still an active area of research",223,semantic
6ee71cd2-100a-4a17-8068-9830b9311042,Review of Common Probability Distributions.pdf,CSCI_83,3,"Probability Distributions
Probability distributions are models for uncertainty of random variables
A random variable is any mapping, , from from some outcome of a random event, , to a real number, :
Example: The mapping can be a count
Example: A function which transforms  to a real number, 
This concept appears abstract at ï¬rst glance, but is fundamental to the theory of probability
We will see many examples in this course
ğ‘‹ ğœ” â„
ğ‘‹(ğœ”)â†’â„
ğœ” â„",443,semantic
aa0bff53-edee-4b6f-a4fe-5262d98c7d3d,Review of Common Probability Distributions.pdf,CSCI_83,4,"Two Types of Probability Distributions
Discrete
Model countable events
Examples; people making a purchase, number of patients with disease,â€¦. Characterized by a probability mass function (PMF)
Continuous
Examples; temperature, velocity, price,â€¦â€¦. Characterized by a probability density function (PDF)",300,semantic
859afe88-0ede-4166-8ee6-5c8aad2d2005,Review of Common Probability Distributions.pdf,CSCI_83,5,"Axioms of Probability
For discrete distributions, we can speak of a set of events within the sample space of all possible events
1. Probability for any set of events, A, is greater than 0 and less than or equal to 1
2. The sum of the probability mass functions over the sample space must add to 1
3.",299,semantic
3aca3ee5-e018-454b-9e8d-cc2d2e45fcb3,Review of Common Probability Distributions.pdf,CSCI_83,5,"If sets of events A and B are mutually exclusive, then the probability of either A and B is the probability of A plus the probability of B
0â‰¤ğ‘ƒ(ğ´)â‰¤1
ğ‘ƒ(ğ‘†)= ğ‘ƒ()=1âˆ‘âˆˆ ğ´ğ‘ğ‘–
ğ‘ğ‘–
ğ‘ƒ(ğ´Â âˆª ğµ)=ğ‘ƒ(ğ´)+ğ‘ƒ(ğµ)ğ‘–ğ‘“Â ğ´âŠ¥ ğµ",194,semantic
dc6580d5-c765-4234-94c5-00d5c17fc65b,Review of Common Probability Distributions.pdf,CSCI_83,6,"Axioms of Probability
From these three axioms we can draw some useful conclusions
Events which cannot occur have probability 0
Events that must occur have probability 1
Events must have a probability mass function between 0 and 1",229,semantic
c8835b2a-b6d1-4687-8826-90c64795b5b4,Review of Common Probability Distributions.pdf,CSCI_83,7,"What do you expect: discrete distributions
What value we should expect to ï¬nd when we sample a random variable? This is the expected value or simply the expectation
For  samples, , of a random variable probability mass function  the expected value is:
How can we interpret expectation? Expectation is a probability weighted sum of the sample of the random variable 
By the second axiom of probability the weights must sum to 1.0
ğ‘› ğ—= , ,â€¦,ğ‘¥1ğ‘¥2 ğ‘¥ğ‘› ğ‘()ğ‘¥ğ‘–
E[ğ—]= Â ğ‘()âˆ‘ğ‘–=1
ğ‘›
ğ‘¥ğ‘– ğ‘¥ğ‘–
ğ—",477,semantic
ba2355af-c5e6-4007-9c50-5dcc378ab798,Review of Common Probability Distributions.pdf,CSCI_83,8,"Properties of Expectation
Useful properties of expectation
1. The relationship is linear in probability
2. The expectation of the sum of two random variables,  and , is the sum of the expectations:
3.",200,semantic
3aaffbfc-7559-4493-b73a-b777c7426c4d,Review of Common Probability Distributions.pdf,CSCI_83,8,"The expectation of an afï¬ne transformation of a random variable, , is an afï¬ne transformation of the expectation:
ğ‘‹ ğ‘Œ
E[ğ—,ğ˜]=E[ğ—]+E[ğ˜]
ğ‘‹
E[ğšÂ ğ—+ğ›]=ğ‘Â E[ğ—]+ğ‘",154,semantic
b55c6150-31c2-4def-b122-7258793e3758,Review of Common Probability Distributions.pdf,CSCI_83,9,"Axioms of probability for continuous distributions
Axioms of probability for continuous probability density function, 
1. On the interval, , , must be bounded by 0 and 1:
Note: if  the integral is 0
2. The area under the entire PDF over the limits must be equal to 1:
Note: many distributions lower =  or  and upper = 
3.",321,semantic
4eb24dba-3af7-4e7c-8ef6-ebbc422f6cf1,Review of Common Probability Distributions.pdf,CSCI_83,9,"If events A and B are mutually exclusive:
ğ‘“(ğ‘¥)
{, }ğ‘¥1ğ‘¥2 ğ‘ƒ(ğ‘¥)
0â‰¤ ğ‘“(ğ‘¥)ğ‘‘ğ‘¥Â â‰¤1âˆ«
ğ‘¥2
ğ‘¥1
=ğ‘¥1 ğ‘¥2
ğ‘“(ğ‘¥)ğ‘‘ğ‘¥=1âˆ«
ğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ
ğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ
0 âˆ’âˆ âˆ
ğ‘ƒ(ğ´Â âˆª ğµ)=ğ‘ƒ(ğ´)+ğ‘ƒ(ğµ)Â ğ‘–ğ‘“Â ğ´âŠ¥ ğµ",143,semantic
3f19db65-c6c5-4e2c-b562-48ae254085da,Review of Common Probability Distributions.pdf,CSCI_83,10,"What do you expect: continuous distributions
Expected value with PDF , over the interval, :
Values  are weighted by the PDF
By the second axiom of probability presented above, PDF must equal 1.0 integrated over the entire range of 
Transformation of expectation is same as for discrete random variables
ğ‘“(ğ‘¥) {ğ‘,ğ‘}
E[ğ—]= ğ‘¥Â ğ‘“(ğ‘¥)Â ğ‘‘ğ‘¥âˆ«
ğ‘
ğ‘
ğ‘¥
ğ‘¥",338,semantic
1cd80833-0e81-4041-9be8-dd6ca5ac9aaa,Review of Common Probability Distributions.pdf,CSCI_83,11,"Bernoulli and Binomial Distributions
Bernoulli distributions model the results of a single trial or single realization with a binary outcome
For outcomes , or , with probability  of success:
Model the number of successful outcome in  trials with the Binomial distribution
Binomial distribution is product of multiple Bernoulli trials:
Product of Bernoulli trials is normalized by the Binomial coefï¬cient
{0,1} {ğ‘“ğ‘ğ‘–ğ‘™ğ‘¢ğ‘Ÿğ‘’,ğ‘ ğ‘¢ğ‘ğ‘ğ‘’ğ‘ ğ‘ } ğ‘
ğ‘ƒ(ğ‘¥Â |Â ğ‘)
ğ‘œğ‘Ÿğ‘ƒ(ğ‘¥Â |Â ğ‘)
={ ğ‘Â ğ‘–ğ‘“Â ğ‘¥=1(1âˆ’ğ‘)Â ğ‘–ğ‘“Â ğ‘¥=0
= (1âˆ’ğ‘ Â ğ‘¥âˆˆ 0,1ğ‘ğ‘¥ )(1âˆ’ğ‘¥)
ğ‘
ğ‘ƒ(ğ‘˜Â |Â ğ‘,ğ‘)=()(1âˆ’ğ‘ğ‘ğ‘˜ ğ‘ğ‘˜ )(ğ‘âˆ’ğ‘˜)",529,semantic
d81a2113-458f-4393-a3a8-41b9c3c897ab,Review of Common Probability Distributions.pdf,CSCI_83,12,"Distributions for Multiple Outomes; the Categorical and MultinomialDistribution
Many real-world cases have many possible outcomes
In these cases need a probability distribution for multiple outcomes
Categorical distribution models multiple outcomes
Categorical Distribution is the multiple-outcome extension of the Bernoulli distribution, and is sometimes call the Multinoulli distribution.",390,semantic
4fd290f5-110e-464f-b329-d57539b718e2,Review of Common Probability Distributions.pdf,CSCI_83,13,"The Categorical distribution
Sample space of  possible outcomes, . For each trial, there can only be one outcome
For outcome  we can encode the results as:
Only  value  has a value of ; one hot encoding
For a single trial the probabilities of the  possible outcomes are expressed:
ğ‘˜  =(, ,â€¦, )ğ‘’1ğ‘’2 ğ‘’ğ‘˜
ğ‘–
=(0,0,â€¦,1,â€¦,0)ğğ¢
1 ğ‘’ğ‘– 1
ğ‘˜
Î 
ğ‘¤ğ‘–ğ‘¡â„ Â âˆ‘ğ‘–
ğœ‹ğ‘–
=( , ,â€¦, )ğœ‹1ğœ‹2 ğœ‹ğ‘˜
=1",363,semantic
ed557567-b483-4441-9caa-3b8aa54128c9,Review of Common Probability Distributions.pdf,CSCI_83,14,"The Categorical distribution
And consequently, we can write the simple probability mass function as:
For a series  of trials we can estimate each of the probabilities of the possible outcomes, :
Where  is the count of outcome . ğ‘“(|Î )=ğ‘¥ğ‘– ğœ‹ğ‘–
ğ‘˜ ( , ,â€¦, )ğœ‹1ğœ‹2 ğœ‹ğ‘˜
=ğœ‹ğ‘– #Â ğ‘’ğ‘–
ğ‘˜
#Â ğ‘’ğ‘– ğ‘’ğ‘–",277,semantic
144d990f-988c-4a33-bb57-75996580304d,Review of Common Probability Distributions.pdf,CSCI_83,15,"The Categorical distribution
For the case of  you can visualize the possible outcomes of a single Categorical trial
Each discrete outcome must fall at one of the corners of a simplex
The probabilities of of each outcome is . Simplex for 
ğ‘˜=3
( , , )ğœ‹1ğœ‹2ğœ‹3
ğ‘€ğ‘¢ğ‘™ğ‘¡3",261,semantic
5002ecd7-696f-47c7-9dbb-ca0b412438a1,Review of Common Probability Distributions.pdf,CSCI_83,16,"Normal distribution
The Normal distribution or Gaussian distribution is one of the most widely used probability distributions
The distribution of mean estimates of observations of a random variable drawn from any distribution converge to a Normal distribution by thecentral limit theorem (CLT)
Many physical processes produce Normal measurement values
Normal distribution has tractable mathematical properties",409,semantic
e84fe523-7d02-4457-b23e-3e995680440a,Review of Common Probability Distributions.pdf,CSCI_83,17,"Normal distribution
For a univariate Normal distribution we can write the density function as:
The parameters can be interpreted as:
ğ‘ƒ(ğ‘¥)= exp12ğœ‹ğœ2âˆš âˆ’(ğ‘¥âˆ’ğœ‡)2
2ğœ2
ğœ‡ğœğœ2
=ğ‘™ğ‘œğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›Â ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿ=ğ‘šğ‘’ğ‘ğ‘›=ğ‘ ğ‘ğ‘ğ‘™ğ‘’=ğ‘ ğ‘¡ğ‘ğ‘›ğ‘‘ğ‘ğ‘Ÿğ‘‘Â ğ‘‘ğ‘’ğ‘£ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›=ğ‘‰ğ‘ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘ğ‘’",224,semantic
0996e41d-08d9-4b01-b0a1-9abf0563dcca,Review of Common Probability Distributions.pdf,CSCI_83,18,"Multivariate Normal
Many practical applications have an -dimensional parameter vector in , requiring multivariate distributions
Multivariate Normal distribution, parameterized by:
n-dimensional vector of locations, 
The vector(multi) valued version of univariate location
 x  dimensional covariance matrix, 
The multi-dimensional version of univariate variance 
 is the determinant of the covariance matrix. Along the diagonal the values are the  variances of each dimension, 
Off-diagonal terms describe the dependency between the  dimensions of the distribution. ğ‘› â„ğ‘›
ğœ‡âƒ—Â 
ğ‘› ğ‘› ğšº
ğœ2
ğ‘“()= ğ‘’ğ‘¥ğ‘( (âˆ’ ğšº(âˆ’))ğ±âƒ—Â  1(2ğœ‹|ğšº|)ğ‘›âˆš
12ğ±âƒ—Â ğœ‡âƒ—Â )ğ‘‡ ğ±âƒ—Â ğœ‡âƒ—Â 
|ğšº|
ğ‘› ğœğ‘–,ğ‘–
ğ‘›",646,semantic
124a6b7f-0a1c-4763-9e37-f83bd922f031,Review of Common Probability Distributions.pdf,CSCI_83,19,"Multivariate Normal
We can write the covariance matrix:
For a Normally distributed n-dimensional multivariate random variable,  computed from the sample, :
Where  is the inner product operator and  is the mean of . ğšº=
â¡
â£
â¢â¢â¢â¢
ğœ1,1
ğœ2,1
â‹® ğœğ‘›,1
ğœ1,2
ğœ2,2
â‹® ğœğ‘›,2
â€¦â€¦
â‹® â€¦
ğœ1,ğ‘›
ğœ2,ğ‘›
â‹® ğœğ‘›,ğ‘›
â¤
â¦
â¥â¥â¥â¥
ğœğ‘–,ğ‘— ğ—
ğœğ‘–,ğ‘—=E[( âˆ’E[])â‹… ( âˆ’E[])]ğ‘¥âƒ—Â ğ‘– ğ‘¥âƒ—Â ğ‘– ğ‘¥âƒ—Â ğ‘— ğ‘¥âƒ—Â ğ‘—
=E[( âˆ’)â‹… ( âˆ’)]ğ‘¥âƒ—Â ğ‘– ğ‘¥Â¯ğ‘– ğ‘¥âƒ—Â ğ‘— ğ‘¥Â¯ğ‘—
= ( âˆ’)â‹… ( âˆ’)1ğ‘˜ğ‘¥âƒ—Â ğ‘– ğ‘¥Â¯ğ‘– ğ‘¥âƒ—Â ğ‘— ğ‘¥Â¯ğ‘—
â‹… ğ‘¥ğ‘–Â¯ ğ‘¥ğ‘–â†’",418,semantic
5d526a62-c6e2-4f7f-96b7-91258fea1afb,Review of Common Probability Distributions.pdf,CSCI_83,20,"Multivariate Normal
2-dimensional Normal with  and 
ğœ‡=[0,0] ğœ=[ ]1.00.0 0.01.0",78,semantic
06fbc494-5805-46cb-8a7e-b47b25a2742c,Review of Common Probability Distributions.pdf,CSCI_83,21,"Multivariate Normal
2-dimensional Normal with  and 
ğœ‡=[0,0] ğœ=[ ]1.00.0 0.00.5",78,semantic
019dfa73-c7e8-44db-9004-4fcbe333bb9e,Review of Common Probability Distributions.pdf,CSCI_83,22,"Multivariate Normal
2-dimensional Normal with  and 
ğœ‡=[0,0] ğœ=[ ]1.00.5 0.51.0",78,semantic
5143fab3-44dd-47f8-b0d0-97d26b63797a,Review of Common Probability Distributions.pdf,CSCI_83,23,"Multivariate Normal
2-dimensional Normal with  and 
ğœ‡=[0,0] ğœ=[ ]1.0âˆ’0.5 âˆ’0.51.0",80,semantic
8f32b2fe-c9ee-4233-998e-e871002f7a9c,Review of Common Probability Distributions.pdf,CSCI_83,24,"Multivariate Normal
2-dimensional Normal with  and 
ğœ‡=[0,0] ğœ=[ ]1.00.9 0.91.0",78,semantic
770df623-ca6c-4447-920c-bb07024f55c0,Review of Common Probability Distributions.pdf,CSCI_83,25,"Log-Normal distribution
Log Normal distribution is deï¬ned for continuous random variables in the range - Examples price, weight, length, and volume
The Log-Normal distribution is based on a log-transformation of the random variable:
0<ğ‘¥â‰¤âˆ
ğ‘ƒ(ğ‘¥)= exp1ğ‘¥ 1ğœ2ğœ‹âˆš âˆ’(ğ‘™ğ‘œğ‘”(ğ‘¥)âˆ’ğœ‡)2
2ğœ2",273,semantic
2912b34e-481b-4718-849a-20ede33897f0,Review of Common Probability Distributions.pdf,CSCI_83,26,"Student t-distribution
Student t-distribution, or simply the t-distribution, is of importance in statistics since it is the distribution of the difference of means of two Normallydistributed random variables
t-distribution has one parameter, the degrees of freedom, denoted as 
The PDF of the t-distribution is a rather complex looking result:
ğœˆ
ğ‘ƒ(ğ‘¥Â |Â ğœˆ)= (1+Î“( )ğœˆ+12Î“()ğœˆğœ‹âˆš ğœˆ2
ğ‘¥2
ğœˆ)
âˆ’ğœˆ+12
ğ‘¤â„ ğ‘’ğ‘Ÿğ‘’Î“(ğ‘¥)=ğºğ‘ğ‘šğ‘šğ‘Â ğ‘“ğ‘¢ğ‘›ğ‘ğ‘¡ğ‘–ğ‘œğ‘›",414,semantic
588d1427-d29f-4f67-9f73-442590c1eda8,Review of Common Probability Distributions.pdf,CSCI_83,27,"The Gamma and  distributions
Gamma family of distributions includes several members which are important in statistics
Gamma distributions are a two-parameter exponential family
PDF is deï¬ned in the range 
Gamma family are used in problems, ranging from measurements of physical systems to hypothesis testing
ğœ’2
0â‰¤ğ‘¥â‰¤âˆ",316,semantic
7ab94947-05ea-48b2-b905-43f0d80ce366,Review of Common Probability Distributions.pdf,CSCI_83,28,"The Gamma and  distributions
Gamma family can be parameterized in several ways; we will use:- A shape parameter, , the degrees of freedom- A scale parameter, 
Alternatively, use an inverse scale parameter, . ğœ’2
ğœˆğœ
ğºğ‘ğ‘š(ğœˆ,ğœ)= Â ğ‘¥ğœˆâˆ’1ğ‘’âˆ’ğ‘¥/ğœ
Â Î“(ğœˆ)ğœğœˆ
ğ‘¤â„ ğ‘’ğ‘Ÿğ‘’ğ‘¥â‰¥0,Â ğœˆ>0,Â ğœ>0ğ‘ğ‘›ğ‘‘Î“(ğœˆ)=ğºğ‘ğ‘šğ‘šğ‘Â ğ‘“ğ‘¢ğ‘›ğ‘ğ‘¡ğ‘–ğ‘œğ‘›
ğ›½=1/ğœ",290,semantic
1dfcd8e2-fa3a-42c6-987c-97d6b585497b,Review of Common Probability Distributions.pdf,CSCI_83,29,"The Gamma and  distributions
Two useful special cases of the Gamma distribution are:
 is the exponential distribution with decay constant , and PDF:
 is the Chi-squared distribution with  degrees of freedom The  distribution has many uses in statistics. Used for estimates of the variance of the Normal distribution
PDF of the  distribution:
ğœ’2
ğºğ‘ğ‘š(1,1/ğœ†) ğœ†
ğ‘’ğ‘¥ğ‘(ğœ†)=ğœ†ğ‘’âˆ’ğœ†ğ‘¥
ğºğ‘ğ‘š(ğœˆ/2,2)=ğœ’2ğœˆ ğœˆ ğœ’2ğœˆ
ğœ’2ğœˆ
=ğœ’2ğœˆ Â ğ‘¥ğœˆ/2âˆ’1ğ‘’âˆ’ğ‘¥
Â Î“(ğœˆ/2)ğœğœˆ/2
ğ‘“ğ‘œğ‘ŸÂ ğœˆÂ ğ‘‘ğ‘’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘ Â ğ‘œğ‘“Â ğ‘“ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘œğ‘š",448,semantic
c2d6df00-3839-4cda-ac94-c7506d16130c,Review of Common Probability Distributions.pdf,CSCI_83,30,"Odds
Odds are the ratio of the number of ways an event occurs to the number of ways it does not occur
Can say that odds are the count of events in favor of an event vs. the count against the event
Example: Flip a fair coin, odds of getting heads are  (1 in 1)
Example: Roll a single fair die your odds of rolling a 6 are  (1 in 5), or 0.2
1:1
1:5",346,semantic
3f419a73-3838-4ee4-9572-0e360e5a7b45,Review of Common Probability Distributions.pdf,CSCI_83,31,"Odds
What is the relationship between odds and probability of an event? For some event with count  in a set of all outcomes with count , and count of negative outcomes :
Example: For the fair coin ï¬‚ip, the odds are . So we can compute the probability of heads as:
Example In statistics the odds ratio, , used to predict the response variable in logistic regression
ğ´ ğ‘† ğµ=ğ‘†âˆ’ğ´
ğ‘ƒ(ğ´)= = = = Â ğ´ğ‘† ğ´ğ´+(ğ‘†âˆ’ğ´) ğ´ğ´+ğµ ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡Â ğ‘–ğ‘›Â ğ‘“ğ‘ğ‘£ğ‘œğ‘Ÿğ‘ğ‘œğ‘¢ğ‘›ğ‘¡Â ğ‘–ğ‘›Â ğ‘“ğ‘ğ‘£ğ‘œğ‘ŸÂ +ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡Â ğ‘›ğ‘œğ‘¡Â ğ‘–ğ‘›Â ğ‘“ğ‘ğ‘£ğ‘œğ‘Ÿğ‘¤â„ ğ‘–ğ‘â„ Â ğ‘–ğ‘šğ‘ğ‘™ğ‘–ğ‘’ğ‘ ğ‘œğ‘‘ğ‘‘ğ‘ =ğ´:(ğ‘†âˆ’ğ´)
1:1
ğ‘ƒ(ğ»)= =11+1 12
ğ‘1âˆ’ğ‘",504,semantic
829b91ea-fb8e-4ccb-a18b-3a591a36bad8,Review of Common Probability Distributions.pdf,CSCI_83,32,"Summary
Axioms of probability; for discrete distribution
Expectation
0<ğ‘ƒ(ğ´)â‰¤1
ğ‘ƒ(ğ‘†)= ğ‘ƒ()=1âˆ‘âˆˆ ğ´ğ‘ğ‘–
ğ‘ğ‘–
ğ‘ƒ(ğ´Â âˆª ğµ)=ğ‘ƒ(ğ´)+ğ‘ƒ(ğµ)ğ‘–ğ‘“Â ğ´âŠ¥ ğµ
E[ğ—]= Â ğ‘()âˆ‘ğ‘–=1
ğ‘›
ğ‘¥ğ‘– ğ‘¥ğ‘–",147,semantic
5b009eee-693f-4695-be05-e4366def3a1d,Review of Common Probability Distributions.pdf,CSCI_83,33,"Summary
The Categorical distribution
For outcome  we one hot encode the results as:
For a single trial the probabilities of the  possible outcomes are expressed:
probability mass function as:
Multivariate Normal distribution, parameterized by n-dimensional vector of locations,  and  x  dimensional covariance matrix
ğ‘–
=(0,0,â€¦,1,â€¦,0)ğğ¢
ğ‘˜
Î =( , ,â€¦, )ğœ‹1ğœ‹2 ğœ‹ğ‘˜
ğ‘“(|Î )=ğ‘¥ğ‘– ğœ‹ğ‘–
ğœ‡âƒ—Â  ğ‘› ğ‘›
ğ‘“()= ğ‘’ğ‘¥ğ‘( (âˆ’ ğšº(âˆ’))ğ±âƒ—Â  1(2ğœ‹|ğšº|)ğ‘˜âˆš 12ğ±âƒ—Â ğœ‡âƒ—Â )ğ‘‡ ğ±âƒ—Â ğœ‡âƒ—",426,semantic
669b0b7c-8f9f-4f26-b2d4-7b1786a153b9,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,0,"Models Categorical Variables and Nonlinear Response
Steve Elston
10/27/2022",75,semantic
6ab7ae20-e2c2-446f-ba92-c77d6205a645,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,1,"Review
Linear models are a ï¬‚exible and widely used class of models
Fit model coefï¬cients by least squares estimation
Can use many types of predictor variables
We prefer the simplest model that does a reasonable job
The principle of Occamâ€™s razor
Must consider the bias-variance trade-off",287,semantic
04a55c56-3038-462d-a351-0f9265f8d6d8,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,2,"Review
When evaluating any machine learning model consider all evaluation methods available
No one method best all of the time
Homoskedastic Normally distributed residuals
Reasonable values , RMSE, etc
Are the model coefï¬cients all signiï¬cant? Different methods highlight different problems with your model
Donâ€™t forget to check that the model must make sense for your application! ğ‘…2",384,semantic
4f200aa1-56e2-4c07-ba82-25816cd867d6,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,3,"Review
Representation of machine learning models
The key representation is the model matrix
Column of 1s for intercept
Columns of feature or predictor values
There are two standards for signatures of ML functions
A model matrix  (exogenous-features) and label array  (dependent-endogenous) - Scikit-learn and base Statsmodels
A data frame with all features (predictors) and label (dependent) columns plus a model formula - Statsmodels formula and R
ğ´=
â¡
â£
â¢â¢â¢â¢â¢â¢
1, , ,â€¦,ğ‘¥1,1ğ‘¥1,2 ğ‘¥1,ğ‘
1, , ,â€¦,ğ‘¥2,1ğ‘¥2,2 ğ‘¥2,ğ‘
1, , ,â€¦,ğ‘¥3,1ğ‘¥3,2 ğ‘¥3,ğ‘
â‹® ,Â â‹® ,Â â‹® ,Â â‹® ,Â â‹® 1, , ,â€¦,ğ‘¥ğ‘›,1ğ‘¥ğ‘›,2 ğ‘¥ğ‘›,ğ‘
â¤
â¦
â¥â¥â¥â¥â¥â¥
ğ‘‹ ğ‘Œ",583,semantic
f30229f3-563e-4a0a-a209-c1bcf8cd6c6c,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,4,"Review
There are a number of assumptions in linear models that you overlook at your peril! The feature or predictor variables should be independent of one another
This is rarely true in practice
Multi-collinearity between features makes the model under-determined
We assume that numeric features or predictors have zero mean and about the same scale
We do not want to bias the estimation of regression coefï¬cients with predictors that do not have a 0 mean
We do not want to have predictors with a large numeric range dominate training
Example: income is in the range of 10s or 100s of thousands and age is in the range of 10s, but apriori income is no more important than age as a predictor
Values of each predictor or feature should be iid
If variance changes with sample, the optimal value of the coefï¬cient could not be constant
If there serial correlation in the predictor values, the iid assumption is violated - but can account for this such as in time series models",972,semantic
015df549-b977-4efe-930d-bbafb1d04592,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,5,"Review
Scaling of features is required for many machine learning models
Several commonly used approaches
Z-score scaling results in features with zero mean and unit variance
Use Z-score scaling for features approximately normally distributed
Min-max scaling transforms feature values to range 0-1
Use min-max scaling for features with truncated range of values
Effect on model coefï¬cients
Scaling changes model coefï¬cients by the scale factor applied
Can re-scale (unscale) model coefï¬cients before processing unknown cases
Or use same scaling for unknown feature values
When coding categorical variables as binary dummy variables no need to scale - already in range [0-1]",672,semantic
5e649e3c-ec5b-476c-8e36-359dfd45a1a0,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,6,"Introduction
Working with categorical variables
One-hot encoding
Working with contrasts
Effects and adjustments
Building models with nonlinear or non-Normal response
Use generalized linear model for nonlinear response
Link function transforms to nonlinear model to linear model
Evaluating Binomial response models
Compare model performance with deviance
Poisson regression",372,semantic
63d142da-5a33-430f-bd89-c399a47f5045,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,7,"Working with Categorical Variables
Linear models, like nearly all machine learning models, use numeric features
How can categorical variables be used in linear models? Need to transform categories to numeric variables with one hot encoding
Each category becomes a binary dummy variable, encoded [0,1]
Only one dummy variable has nonzero value - encodes the category
n categories represented by n-1 dummy variables; all 0s encodes one level
Binary variables are an exception
Represent with a single binary variable. [0,1] values",527,semantic
94531dbd-5cd5-4853-9e1a-a16aa2f6fc45,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,8,"Working with Categorical Variables
Example: Consider a data set with categorical variables
##     id  female  race  ses  schtyp  prog  read  write  math  science  socst## 0   70       0     4    1       1     1    57     52    41       47     57## 1  121       1     4    2       1     3    68     59    53       63     61## 2   86       0     4    3       1     1    44     33    54       58     31## 3  141       0     4    3       1     3    63     44    47       53     56## 4  172       0     4    2       1     2    47     52    57       53     61## 5  113       0     4    2       1     2    44     52    51       63     61## 6   50       0     3    2       1     1    50     59    42       53     61## 7   11       0     1    2       1     2    34     46    45       39     36## 8   84       0     4    2       1     1    63     57    54       58     51## 9   48       0     3    2       1     2    57     55    52       50     51",938,semantic
9dd4b3d9-2e89-476f-91be-8c8356dbf491,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,9,"Working with Categorical Variables
Example: split the data into train and test subsets
## (120, 11)
## (80, 11)
nr.seed(2 3 4 5 )
msk = nr.choice(test_scores.index, size = 1 2 0 , replace=False)test_scores_train = test_scores.iloc[msk,:]print(test_scores_train.shape)
test_scores_test = test_scores.drop(msk, axis=0 ) 
print(test_scores_test.shape)",348,semantic
de826094-ee33-4353-9c00-cdc89311923c,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,10,"Working with Categorical Variables
Example: We can encode a categorical variable with the Python patsy package to get the X (model) and Y(label) arrays:
## [[1. 0. 0.]##  [1.",174,semantic
73e74f69-69be-4d3a-af24-eef17d97b95b,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,10,"1. 0.]##  [1. 0. 1.]##  [1. 0. 1.]##  [1. 1. 0.]]
## [[57.]##  [61.]##  [31.]##  [56.]##  [61.]]
from patsy import dmatrices
Y, X = dmatrices(""socst ~ C(ses, levels=[1,2,3])"", data=test_scores)print(X[:5 ])
print(Y[:5 ])",220,semantic
1f3961ad-8ce0-4b31-9cb8-9c09bd20faed,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,11,"Working with Categorical Variables
Example: A simple linear model with one categorical variable
## <class 'statsmodels.iolib.summary.Summary'>## """"""##                             OLS Regression Results                            ## ==============================================================================## Dep. Variable:                  socst   R-squared:                       0.097## Model:                            OLS   Adj. R-squared:                  0.082## Method:                 Least Squares   F-statistic:                     6.285## Date:                Mon, 31 Oct 2022   Prob (F-statistic):            0.00255## Time:                        10:34:48   Log-Likelihood:                -446.60## No.",721,semantic
f6b81059-c062-4515-aa39-073b22b62757,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,11,"Observations:                 120   AIC:                             899.2## Df Residuals:                     117   BIC:                             907.6## Df Model:                           2                                         ## Covariance Type:            nonrobust                                         ## ===============================================================================##                   coef    std err          t      P>|t|      [0.025      0.975]## -------------------------------------------------------------------------------## Intercept      47.5926      1.949     24.415      0.000      43.732      51.453## C(ses)[T.2]     4.8635      2.366      2.055      0.042       0.177       9.550## C(ses)[T.3]     9.1296      2.579      3.540      0.001       4.023      14.237## ==============================================================================## Omnibus:                        3.392   Durbin-Watson:                   2.023## Prob(Omnibus):                  0.183   Jarque-Bera (JB):                3.413## Skew:                          -0.397   Prob(JB):                        0.182## Kurtosis:                       2.772   Cond. No. 4.53## ==============================================================================## ## Notes:## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.## """"""
import statsmodels.formula.api as smf 
linear_model = smf.ols(""socst ~ C(ses)"", data=test_scores_train).fit()linear_model.summary()",1515,semantic
16047182-0fa3-4063-8a8f-7dd9784e6ff8,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,12,"Working with Categorical Variables
Wait!",40,semantic
1352e7cb-6d33-41b0-ba6b-17151b5e1a71,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,12,"What happened to the coefï¬cient for the ï¬rst level of ses? The intercept is the mean response of the ï¬rst level
The other coefï¬cients are contrasted with respect to the mean of the ï¬rst level. Consider the following possible ways we can encode responses to a categorical variable - often called a treatment
For  treatments, there are mean responses 
The alternative encoding is a treatment with intercept, , at  contrasts, 
The means and contrasts are related:
ğ‘› [ , ,â€¦, ]ğœ‡1ğœ‡2 ğœ‡ğ‘›
ğ¼ ğ‘›âˆ’1 [ğ¼, ,â€¦, ]ğ‘1 ğ‘ğ‘›âˆ’1
=
â¡
â£
â¢â¢â¢â¢
ğ¼ğ‘2
â‹® ğ‘ğ‘›
â¤
â¦
â¥â¥â¥â¥
â¡
â£
â¢â¢â¢â¢
ğœ‡1
âˆ’ğœ‡2 ğœ‡1
â‹® âˆ’ğœ‡ğ‘› ğœ‡1
â¤
â¦
â¥â¥â¥â¥",568,semantic
55714835-b47d-4e34-bc7e-8a0ea655d41c,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,13,"Working with Categorical Variables
In a linear model we can sometimes relate the coefï¬cient values to an effect size
Start with  treatments  with effect sizes, 
With no intercept term the means represent the effect sizes:
With intercept term compute effect sizes using contrasts:
ğ‘› [, ,â€¦, ]ğ‘¡1ğ‘¡2 ğ‘¡ğ‘› [, ,â€¦, ]ğ‘’1ğ‘’2 ğ‘’ğ‘›
=
â¡
â£
â¢â¢â¢â¢
ğ‘’1
ğ‘’2
â‹® ğ‘’ğ‘›
â¤
â¦
â¥â¥â¥â¥
â¡
â£
â¢â¢â¢â¢
ğœ‡1
ğœ‡2
â‹® ğœ‡ğ‘›
â¤
â¦
â¥â¥â¥â¥
=
â¡
â£
â¢â¢â¢â¢
ğ‘’1
ğ‘’2
â‹® ğ‘’ğ‘›
â¤
â¦
â¥â¥â¥â¥
â¡
â£
â¢â¢â¢â¢
ğ¼ğ¼+ğ‘1
â‹® ğ¼+ğ‘ğ‘›
â¤
â¦
â¥â¥â¥â¥",435,semantic
cc233800-7eae-45ac-a6aa-8d821a873ec1,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,14,"Working with Categorical Variables
In a linear model we can sometimes relate the coefï¬cient values to an effect size
Assumes the treatments are orthogonal
In other words, applied on at a time
e.g. a case can only be in one category
Assumes that the model coefï¬cients are statistically independent
Coefï¬cients are dependent in overï¬t model
Often need to adjust for other effects
Other treatments
Levels of other categorical variables
Use partial slope of continuous variables
In other words apply with care! Donâ€™t over-interpret your model
Conditions in real world hard to verify, particularly for observational data",615,semantic
ca68a1a6-137c-4c1a-a1cf-fb868ad4b2f0,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,15,"Models with Nonlinear Response
How do we deal with models that do not have nonlinear response variables? Example: binary response variable, ,  distributed
Probability parameter 
A binary classiï¬er
Example: Intensity of an arrival process,  response
 is the average rate or intensity of a point process
Estimate the parameter 
Example: Categorical response variable for  categories, :
 category classiï¬er
Response is probability probability for each category, 
[0,1]ğµğ‘–ğ‘›(ğœƒ)
ğœƒ
ğ‘ğ‘œğ‘–ğ‘ ğ‘ ğ‘œğ‘›(ğœ†)
ğœ†
ğœ†
ğ‘› ğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–( , ,â€¦, )ğœ‹1ğœ‹2 ğœ‹ğ‘›
ğ‘›
Î =[ , ,â€¦, ]ğœ‹1ğœ‹2 ğœ‹ğ‘›",533,semantic
d4d33d7a-3b7c-49a3-959a-21ea71afd27c,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,16,"Models with Nonlinear Response
The generalized linear model (GLM) is a framework for nonlinear response models
Nonlinear response is non-Normally distributed
For each distribution use a link function to transform to a linear model
Linear model has Normally distributed response
Link function transform nonlinear response to Normal distribution
To compute the nonlinear response
Start with a linear model, OLS
Transform response with inverse link function
Works for all exponential family response distributions",510,semantic
1b3e2342-31f2-46cf-9043-7e6a19f60467,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,17,"The Generalized Linear Model
Link functions are available for many distributions
Supported in statsmodels
Supported in Scikit-Learn
Examples:
Gaussian, identity function
Inverse Gaussian
Binomial, logit function
Multinomial
Poisson
Negative Binomial
Gamma
Tweedie",263,semantic
7eeac90b-2c41-48a3-bd96-181c90f2ef4e,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,18,"The Generalized Linear Model
General form for link function, , mapping response variable, , observation vector, , to linear model
Given linear model :
To ï¬nd the value of the response variable we apply the inverse link function:
ğ‘”() ğ‘¦ ğ‘¥
= +Â ğ‘¥ğœ†Ì‚Â  ğ›½0 ğ›½1
ğ‘”(ğ™´[| ])= = +Â ğ‘¥ğ‘¦ğ‘– ğ‘¥ğ‘– ğœ†Ì‚Â  ğ›½0 ğ›½1
ğ™´[| ]= ()= ( +Â ğ‘¥)ğ‘¦ğ‘– ğ‘¥ğ‘– ğ‘”âˆ’1 ğœ†Ì‚Â  ğ‘”âˆ’1 ğ›½0 ğ›½1",323,semantic
98d22c6b-33b8-4a2b-8bb3-028705989680,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,19,"The Generalized Linear Model
OLS has Normal response
What is the link function
Link function for OLS is just unity, or 
Output of linear model directly maps to Normally distributed response
1
+Â ğ‘¥âˆ¼ ğ‘( +Â ğ‘¥,ğœ)ğ›½0 ğ›½1 ğ›½0 ğ›½1",217,semantic
c8c88c11-ea94-4754-993f-2c48f44510a3,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,20,"The Logistic Regression Model
Construct a generalized linear model using a Binomial distribution
Commonly known as logistic regression
Logistic regression widely used as a classiï¬cation model
Logistic regression is linear model, with a binary response or label values, {False, True} or {0, 1}
Response computed as a log likelihood, leading to a Binomial distributed response",374,semantic
67b8dfd8-589f-4f98-b8e3-0cf220886be1,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,21,"The Logistic Regression Model
Construct logistic regression as a GLM
Start with a model for the log-odds of response  vs. Probability of success, or , 
Independent variable vector 
Model parameter vector, 
Binary response, 
Deï¬ne the link function, know as the or logit function:
1 0
=1ğ‘¦ğ‘– ğ‘ğ‘–
ğ‘¥ğ‘–
ğ›½
=[0,1]âˆ¼ ğµğ‘–ğ‘›()ğ‘¦ğ‘– ğ‘ğ‘–
ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡(ğ™´[| ])=ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡(())=ğ‘™ğ‘›( )= +ğ›½ğ‘¦ğ‘– ğ‘¥ğ‘– ğ‘ğ‘– ğ‘¥ğ‘– ()ğ‘ğ‘– ğ‘¥ğ‘–
1âˆ’()ğ‘ğ‘– ğ‘¥ğ‘– ğ›½0 ğ‘¥ğ‘–",383,semantic
6d0bfcd3-fc8d-4597-a89a-9e38f26293f0,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,22,"The Logistic Regression Model
Response of linear model is transformed to the binomially distributed random variable through the inverse link function
Known as the inverse logit function, or logistic function, 
After some algebra we can arrive at:
ğ‘“()ğ‘¥ğ‘–
ğœ†ğ‘–
()=ğ™´[| ]ğ‘ğ‘– ğ‘¥ğ‘– ğ‘¦ğ‘– ğ‘¥ğ‘–
()=ğ‘“()ğ‘ğ‘– ğ‘¥ğ‘– ğ‘¥ğ‘–
= +ğ›½ğ›½0 ğ‘¥ğ‘–
=ğ‘“()=ğ‘™ğ‘œğ‘”ğ‘– ()ğœ†ğ‘– ğ‘¡âˆ’1ğœ†ğ‘–
=ğ‘™ğ‘œğ‘”ğ‘– ()= =ğ‘¡âˆ’1ğœ†ğ‘– 11+ğ‘’âˆ’ğœ†ğ‘–
11+ğ‘’âˆ’( +ğ›½)ğ›½0 ğ‘¥ğ‘–",362,semantic
07495235-7a95-4fc8-97df-beb5206ed859,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,23,"Logistic Regression Model
What does the transformation function look like? Consider a simple 1-dimensional case
The response is bound in the range 
We say the logistic transformation squashes the linear response  to binary, 
Can set a decision threshold for binary response
Default 
=ğ‘¦ğ‘– 11+ğ‘’âˆ’ğ‘¥ğ‘–
[0,1]
ğœ†=ğ´ğ‘âƒ—Â  [0,1]
=0.5",318,semantic
3e54a401-f83c-483f-9edc-8e030eee1f8b,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,24,"Evaluation of Classiï¬ers
How can we evaluate a classiï¬erâ€™s accuracy? Determine proportions of test cases which are classiï¬ed as:
True Positives (TP): Are positive and should be positive
True Negatives (TN): Are negative and should be negative
False Positives (FP): Classiï¬ed as positive but are actually negative; Type I errors
False Negatives (FN): Classiï¬ed as negative but are actually positive; Type II errors
Organize these metrics into a confusion matrix
Classiï¬ed NegativeClassiï¬ed Positive
Negative TN FP
Positive FN TP",527,semantic
31cd27dd-a808-4aa8-a842-f3cd09e4e737,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,25,"Evaluation of Classiï¬ers
The other metrics are deï¬ned as follows:
Accuracy = (TP + TN) / (TP + FP + TN + FN)
Selectivity or Precision = TP / (TP + FP)
Precision is the fraction of the relevant class predictions are actually correct
Sensitivity or Recall = TP / (TP + FN)
Recall is the fraction of the relevant class were we able to predict
Is a trade-off between precision and recall
Consider changing the decision threshold
High threshold  higher recall, fewer false negative
Low threshold  higher precision, fewer false positives
â†’
â†’",535,semantic
7aef0fab-1e1f-44dc-a3ea-b3b918c95c86,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,26,"Example of Logistic Regression
How well can we predict the type of school given the test scores? ##                  Generalized Linear Model Regression Results                  ## ==============================================================================## Dep. Variable:                 schtyp   No.",305,semantic
56d8c396-eab1-4804-bce9-194591542032,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,26,"Observations:                  200## Model:                            GLM   Df Residuals:                      198## Model Family:                Binomial   Df Model:                            1## Link Function:                  Logit   Scale:                          1.0000## Method:                          IRLS   Log-Likelihood:                -86.978## Date:                Mon, 31 Oct 2022   Deviance:                       173.96## Time:                        10:34:48   Pearson chi2:                     199.## No. Iterations:                     4   Pseudo R-squ. (CS):           0.009511## Covariance Type:            nonrobust                                         ## ==============================================================================##                  coef    std err          z      P>|z|      [0.025      0.975]## ------------------------------------------------------------------------------## Intercept     -3.1718      1.129     -2.809      0.005      -5.385      -0.959## math           0.0283      0.020      1.382      0.167      -0.012       0.068## ==============================================================================
## Prep the data
test_scores['schtyp'] = np.subtract(test_scores['schtyp'], 1 )
## Fit the model
formula = 'schtyp ~ math'logistic_reg_model = smf.glm(formula=formula, data=test_scores, family=sm.families.Binomial()).fit()
print(logistic_reg_model.summary())",1427,semantic
3daa310f-1ade-42ad-a257-9be7026657ab,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,27,"Example of Logistic Regression
The data frame now looks like this with the predicted probability and the binary scores:
##      female  race  ses  schtyp  prog  ... math  science  socst  predicted  score## id                                    ... ## 70        0     4    1       0     1  ...",292,semantic
a213296e-b207-4879-869f-aedca8eb5381,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,27,41       47     57   0.117996      0## 121       1     4    2       0     3  ... 53       63     61   0.158163      0## 86        0     4    3       0     1  ... 54       58     31   0.161968      0## 141       0     4    3       0     3  ... 47       53     56   0.136844      0## 172       0     4    2       0     2  ... 57       53     61   0.173824      0## 113       0     4    2       0     2  ... 51       63     61   0.150772      0## 50        0     3    2       0     1  ... 42       53     61   0.120973      0## 11        0     1    2       0     2  ... 45       39     36   0.130295      0## 84        0     4    2       0     1  ... 54       58     51   0.161968      0## 48        0     3    2       0     2  ... 52       50     51   0.154432      0## 75        0     4    2       0     3  ... 51       53     61   0.150772      0## 60        0     4    2       0     2  ... 51       63     61   0.150772      0## 95        0     4    3       0     2  ... 71       61     71   0.238200      1## 104       0     4    3       0     2  ... 57       55     46   0.173824      0## 38        0     3    1       0     2  ... 50       31     56   0.147184      0## 115       0     4    1       0     1  ... 43       50     56   0.124015      0## 76        0     4    3       0     2  ... 51       50     56   0.150772      0## 195       0     4    2       1     1  ...,1376,semantic
82c51dc4-15fb-4ba1-871c-83b3f10aa479,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,27,60       58     56   0.186356      1## 114       0     4    3       0     2  ...,80,semantic
8e344610-8943-45b2-b504-33dcbf78ad95,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,27,"62       55     61   0.195090      1## 85        0     4    2       0     1  ... 57       53     46   0.173824      0## ## [20 rows x 12 columns]
## score the results 
threshold=0 . 1 8 test_scores['predicted'] = logistic_reg_model.predict()test_scores['score'] = [1  if x>threshold else 0  for x in test_scores['predicted']]
test_scores.head(2 0 )",348,semantic
82ef1238-42b7-4615-adf3-ef2b71debcf5,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,28,"Example of Logistic Regression
Now, evaluate the model - the classiï¬er is almost useless - no Kagle awards!:
##                  Confusion matrix##                  Score negative    Score positive## Actual negative       127                41## Actual postitive        22                10## ## Accuracy  0.69##  ##            Negative      Positive## Num case      168            32## Precision    0.85          0.20## Recall       0.76          0.31## F1           0.80          0.24
import sklearn.metrics as sklm  
def print_metrics(labels, scores):    metrics = sklm.precision_recall_fscore_support(labels, scores)    conf = sklm.confusion_matrix(labels, scores)
    print('                 Confusion matrix')    print('                 Score negative    Score positive')    print('Actual negative    %6d' % conf[0 ,0 ] + '             %5d' % conf[0 ,1 ])    print('Actual postitive    %6d' % conf[1 ,0 ] + '             %5d' % conf[1 ,1 ])
    print('')    print('Accuracy  %0.2f' % sklm.accuracy_score(labels, scores))    print(' ')
    print('           Negative      Positive')    print('Num case   %6d' % metrics[3 ][0 ] + '        %6d' % metrics[3 ][1 ])    print('Precision  %6.2f' % metrics[0 ][0 ] + '        %6.2f' % metrics[0 ][1 ])    print('Recall     %6.2f' % metrics[1 ][0 ] + '        %6.2f' % metrics[1 ][1 ])
    print('F1         %6.2f' % metrics[2 ][0 ] + '        %6.2f' % metrics[2 ][1 ])    print_metrics(test_scores['schtyp'], test_scores['score'])",1478,semantic
09fd7438-605a-4dc0-bbf9-b445d402f333,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,29,"Example of Logistic Regression
How can we understand the cut-off value in terms of the CDF of the positive and negative cases? Both CDFs are at 1.0 by about probability = 0.25 - this is a very skewed situation! CDF curves nearly the same = poor model
Positive cases to the left of cut-off are Type II errors
Negative cases to the right of cut-off are Type I errors",364,semantic
8d8cffee-3c4b-44f5-9486-1109a4db20ee,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,30,"What is Deviance? The signiï¬cance of the GLM is expressed in terms of a statistic called deviance
It can be challenging to understand what deviance really means
To further complicate the problem there are several commonly used forms of deviance
OLS regression models are often evaluated based on variance ratios, such as the  metric, or error metrics like RMSE
Given a nonlinear mapping between the linear model and the response, these methods are not suitable
ğ‘…2",463,semantic
5f6eb21b-b8f3-406c-929e-9cb82a14d419,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,31,"What is Deviance? Focus on one the simplest and easiest to understand forms, known as null deviance
Null deviance is 2 times the square of the log odds ratio between a model and a null model
Intuitively, the null model is informed guessing
Deviance is a measure of how much the model improves accuracy beyond guessing",317,semantic
8a16202c-3003-4ec0-83ac-d00898469070,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,32,"What is Deviance? The deviance statistic is  distributed
Can apply a signiï¬cance test on a model
Model with small deviance is little better that informed guessing
Small 
Not signiï¬cant improvement
Model with large deviance has a large 
Signiï¬cant improvement in accuracy
ğœ’2
ğœ’2
ğœ’2",279,semantic
cd758b44-812a-4e4c-bdc2-eef92f671c35,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,33,"What is Deviance? To understand binomial deviance, start with the expected value of the binomial log-likelihood:
Calculate the binomial probability:
For the null model we set , so the logistic function for a null model is:
And with expected log-likelihood:
(ğ‘˜,ğ‘›|ğ‘)=ğ‘™ğ‘œğ‘”()+ğ‘˜Â ğ‘™ğ‘œğ‘”(ğ‘)+(ğ‘›âˆ’ğ‘˜)Â ğ‘™ğ‘œğ‘”(1âˆ’ğ‘)ğ‘™Ì‚Â  ğ‘›ğ‘˜
==ğ‘ğœ™ ğ‘¦Â¯ ğ‘˜ğ‘›
=ğ›½0 ğ‘ğœ™
()= =ğ‘“ğœ™ğ‘¦Ì‚Â  11+ğ‘’âˆ’ğ‘ğœ™
11+ğ‘’âˆ’ğ›½0
(ğ‘˜,ğ‘›| )=ğ‘™ğ‘œğ‘”()+ğ‘˜Â ğ‘™ğ‘œğ‘”( )+(ğ‘›âˆ’ğ‘˜)Â ğ‘™ğ‘œğ‘”(1âˆ’ )ğ‘™Ì‚Â ğ‘â„ ğ‘– ğ‘ğœ™ ğ‘›ğ‘˜ ğ‘ğœ™ ğ‘ğœ™",402,semantic
6ce8b4ef-ada5-4e6d-8182-960663d2a0a8,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,34,"What is Deviance? Gain intuitive understand of the behavior of the null model by example
Consider the case where half the values of the response, , are 1s
, so 
For each value of , model randomly selects a 1 or a 0 response with probability of 0.5
This model is random guessing with accuracy of 0.5
In other words, the null model is no better in terms of predictive power than just saying that all values of  are either 0 or all values are 1. ğ‘¦
ğ‘˜=ğ‘›/2 =ğ‘›/2=0.5ğ‘ğœ™
ğ‘¥ğ‘–
ğ‘¦",466,semantic
ddd29bfe-f8e2-41ae-a5a0-6af7406c8f04,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,35,"What is Deviance? Form of null deviance of a linear model
Deviance is log of the square expected likelihood ratio:
Expected null log-likelihood, , is ï¬xed by the observed response values
-Therefore, the better the model, and higher the likelihood, - Higher deviance and therefore the value of - Model with large  has greater signiï¬cance and accuracy of predictions
ğ‘™ğ‘œğ‘”ğ‘– (ğ‘())=ğ‘¡ğœ™ ğ‘¥ğ‘– ğ›½0
ğ·( +ğ›½ )ğ›½0 ğ‘¥ğ‘– =ğ‘™ğ‘›( )ğ¿Ì‚Â 2
ğ¿Ì‚Â 2ğœ™
=2(âˆ’)lÌ‚Â lÌ‚Â ğœ™
lÌ‚Â ğœ™
lÌ‚Â ğœ’2
ğœ’2",441,semantic
ff819e9f-1a9b-4dc0-8c0d-938b0f0351c1,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,36,"What is Deviance? What are some key properties of null deviance? Recall that log-likelihood is a negative number
Higher log-likelihood has smaller negative magnitude
Log-likelihood of null model has large negative magnitude
Deviance  always
If model is no better than null model, 
For model with greater predictive power, 
â‰¥0
l=,Â ğ·=0lğœ™
l>,Â ğ·>0lğœ™",345,semantic
0ccf3d2c-cbcc-482c-bc67-f483d7862348,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,37,"What is Deviance? Deviance concepts discussed here can be applied to any GLM
In some cases, deviance can be used directly
In other cases, log-likelihood ratio is used directly for model evaluation and signiï¬cance",212,semantic
2a4d23c4-47c8-40f7-99c3-d516a0b36b9a,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,38,"Solving Maximum Likelihood Problem
We have investigated methods for ï¬nding maximum likelihood solutions at large scale
ML algorithms are employed routinely to logistic regression problems on a massive scale
Variations of the stochastic gradient descent (SGD) algorithms
Quasi-Newtonâ€™s methods like the limited memory Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno (l-BFGS) algorithm",368,semantic
badf654d-9841-4681-a337-971fcf0bdb0f,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,39,"Poisson Regression as GLM
Poisson regression is example of a GLM
Poisson regression is example of nonlinear response model
Recall, the Poisson distribution has an exponential form with a single parameter, 
Parameter, , is the expected arrival rate of the process
Predictions, , given the observations,  and the model parameter, :
Above is the Poisson link function and inverse link function
ğœ†
ğœ†
ğ‘¦ğ‘– ğ‘¥ğ‘– ğœ†ğ‘–
ğ‘™ğ‘œğ‘”[ğ™´(| )]=ğ‘¥ âŸº ğ™´(| )=ğ‘¦ğ‘– ğ‘¥ğ‘– ğœ†ğ‘– ğ‘¦ğ‘– ğ‘¥ğ‘– ğ‘’ğ‘¥ğœ†ğ‘–",444,semantic
acf066c8-335f-4af0-a4e2-80dae637feb0,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,40,"Poisson Regression as GLM
Extend relationship using a linear model for 
Expected arrival rate changes with the independent variable
Example, linear model with intercept
, and  dimensional slope parameter vector, 
Estimate of , for a  dimensional observation vector 
ğœ†
ğ›½0 ğ‘ ğ›½âƒ—Â 
ğœ†ğ‘– ğ‘ ğ±ğ‘–
ğ‘™ğ‘œğ‘”()= + âŸº =ğœ†ğ‘– ğ›½0 ğ±ğ‘–ğ›½âƒ—Â  ğœ†ğ‘– ğ‘’( +$)ğ›½0 ğ±ğ‘– ğ›½âƒ—",326,semantic
30e54a62-21c9-4c98-8e68-7946c89ca153,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,41,"Poisson Regression as GLM
Use formulation of the Poisson distribution
Link function
Link function  Inverse link function:
ğ‘™ğ‘œğ‘”[ğ™´(| )]=ğ‘¦ğ‘– ğ‘¥ğ‘– ğ‘¥ğ‘–ğœ†ğ‘–
âŸº 
ğ‘™ğ‘œğ‘”[ğ™´(| )]=ğ‘¦ğ‘– ğ‘¥ğ‘– ğ‘¥ğ‘–ğœ†ğ‘–
ğ‘™ğ‘œğ‘”[ğ™´(| )]= +ğ‘¦ğ‘– ğ‘¥ğ‘– ğ›½0 ğ±ğ‘–ğ›½âƒ—Â 
âŸº ğ™´(| )=ğ‘¦ğ‘– ğ‘¥ğ‘– ğ‘’ğ‘¥ğ‘–ğœ†ğ‘–
âŸº ğ™´(| )=ğ‘¦ğ‘– ğ‘¥ğ‘– ğ‘’+ğ›½0 ğ±ğ‘–ğ›½âƒ—",240,semantic
217b89e7-2337-4832-8e80-2837584ec677,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,42,"Poisson Regression Example
Data for number of awards for students by program and math score
Count data - suitable for Poisson model
Note exponential decrease in counts with math score and program",195,semantic
86988414-4f47-4799-87b2-0665b2792317,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,43,"Poisson Regression Example
Fit a model and examine result
## Optimization terminated successfully.##          Current function value: 0.913761##          Iterations 6
## <class 'statsmodels.iolib.summary.Summary'>## """"""##                           Poisson Regression Results                          ## ==============================================================================## Dep. Variable:             num_awards   No. Observations:                  200## Model:                        Poisson   Df Residuals:                      196## Method:                           MLE   Df Model:                            3## Date:                Mon, 31 Oct 2022   Pseudo R-squ.:                  0.2118## Time:                        10:34:55   Log-Likelihood:                -182.75## converged:                       True   LL-Null:                       -231.86## Covariance Type:            nonrobust   LLR p-value:                 3.747e-21## =========================================================================================##                             coef    std err          z      P>|z|      [0.025      0.975]## -----------------------------------------------------------------------------------------## Intercept                -4.1633      0.663     -6.281      0.000      -5.462      -2.864## C(prog)[T.General]       -1.0839      0.358     -3.025      0.002      -1.786      -0.382## C(prog)[T.Vocational]    -0.7140      0.320     -2.231      0.026      -1.341      -0.087## math                      0.0702      0.011      6.619      0.000       0.049       0.091## =========================================================================================## """"""",1690,semantic
d5288d69-1005-4b7d-adb8-f9fd0d875c0a,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,44,"Poisson Regression Example
Maximum likelihood regression lines for counts with Poisson model",92,semantic
ce85e315-dde3-46c3-a497-d3303c17a92b,Models Categorical Variables and Nonlinear Response.pdf,CSCI_83,45,"Summary
Models with nonlinear response have non-Normal distributions
The generalized linear model accommodates nonlinear response distributions
Link function transforms to linear model
Inverse link function transforms from Normal distribution to response distribution
Evaluating Binomial response models
Confusion matrix organizes
Compute metrics from elements of confusion matrix
Use multiple evaluation criteria
Compare model performance with deviance",453,semantic
433aee37-248e-482f-ad49-d4479e5b60ad,Introduction to Linear Models.pdf,CSCI_83,0,"Introduction to Linear Models
Steve Elston
10/20/2022",53,semantic
69e13554-3560-4cd0-af63-d79c41483fd0,Introduction to Linear Models.pdf,CSCI_83,1,"Welcome to the Second Half of CSCI E-83! Plan going forward:
Week 8: Introduction to Linear Models
Week 9: Linear Models Part 2 - Categorical data and nonlinear response models
Week 10: Linear Models Part 3 - Regularization and sparse models
Week 11: Time Series Models
Week 12: Bayes MCMC methods
Week 13: Hierarchical models
Week 14: TBD - More on time series? Dec 17: Submit Graduate Independent Projects
Let me know if you have suggestions to change this schedule",467,semantic
0058eca9-1d4f-4315-b86c-7427887b2277,Introduction to Linear Models.pdf,CSCI_83,2,"Review
Bayesian analysis is a contrast to frequentist methods
The objective of Bayesian analysis is to compute a posterior distribution
Contrast with frequentist statistics; computing a point estimate and conï¬dence interval from a sample
Bayesian models allows expressing prior information in the form of a prior distribution
Selection of prior distributions can be performed in a number of ways
The posterior distribution is said to quantify our current belief
We update beliefs based on additional data or evidence
A critical difference with frequentist models which must be computed from a complete sample
Inference can be performed on the posterior distribution by ï¬nding the maximum a postiori (MAP) value and a credible interval
Predictions are made by simulating from the posterior distribution a",803,semantic
105e4253-d65d-4bfe-99d4-22cb6e6306d3,Introduction to Linear Models.pdf,CSCI_83,3,"Introduction - Why linear models? Linear models are widely used in statistics and machine learning
Understandable and interpretable
Generalize well, if properly ï¬t
Highly scalable â€“ computationally efï¬cient
Can approximate fairly complex functions
A basis of understanding complex models
Many non-linear models are at locally linear at convergence
We can learn a lot about the convergence of DL and RL models from linear approximations
In this lesson we take a frequentist view of the linear model
Bayesian view is also widely used",531,semantic
45227cfa-2af9-4cb1-bfaa-297a6ee1fd22,Introduction to Linear Models.pdf,CSCI_83,4,Introduction - Why linear models?,33,semantic
6b61ecad-ab9c-4dd3-b334-7523870db51a,Introduction to Linear Models.pdf,CSCI_83,4,"Linear models are readily interpretable! Human interpretability is of critical importance for models used for critical decisions
Health care
Safe operation of autonomous systems
Social justice for applications with human impact
etc. Model coefï¬cients provide information on response sensitivities to changes in variables
Low chance of unexpected output
Complex and nonlinear model result in poor human intuition about expected response
Complex and nonlinear models vulnerable to unexpected output",496,semantic
473c87aa-f1f6-4b66-b8f0-e8efe3c2b4e3,Introduction to Linear Models.pdf,CSCI_83,5,"What is regression? In statistics, regression refers to a family of model that attempt to predict the value of numeric random variable
Regression is a common form of a linear model
Linear regression is a building block of many statistical and ML methods:
multivariate regression and principal component
Analysis of variance (ANOVA)
Polynomial regression
Logistic regression for binary classiï¬cation
Poisson regression
Many time series models
Neural networks (and deep learning)",477,semantic
e9d8361c-d48a-4468-b8e4-17f905449c22,Introduction to Linear Models.pdf,CSCI_83,6,"Terminology
The confusing division in terminology arises from different communities within statistics and machine learning
Machine Learning TerminologyStatistical Terminology
Regression vs classiï¬cation Continuous numeric vs categorical response
Learning algorithm or model Model
Training Fitting
Trained model Fitted model
Supervised learning Predictive modeling",363,semantic
7b52b1fb-d230-40ec-aea7-67e7cd1b5440,Introduction to Linear Models.pdf,CSCI_83,7,"Terminology
Different communities have created different terminology at different times for the variables in a machine learning models
Predicted VariableVariables Used to Predict
y x
Dependent Independent
Endogenous Exogenous
Response Predictor
Response Explanatory
Label Feature
Regressand Regressors
Outcome Design
Left Hand Side Right Hand Side",347,semantic
5e623e5c-2390-4895-a175-e695979f3f4f,Introduction to Linear Models.pdf,CSCI_83,8,"Formulating the Linear Model
The general formulation of a linear model can be written:
 is the vector of  dependent variables or labels we seek to predict
 is the  x  model matrix or design matrix
Deï¬nes the structure of the model
 columns are values of the predictor variables or features
 rows of training cases
 is the vector of  model coefï¬cients
One coefï¬cient for each predictor or feature
Model is ï¬t by ï¬nding an optimal value for each coefï¬cient
 is the vector representing prediction error
The  residuals
Is iid Normally distributed; 
=ğ´+ğ‘¦âƒ—Â  ğ‘âƒ—Â ğœ–âƒ—Â 
ğ‘¦âƒ—Â  ğ‘›
ğ´ ğ‘› ğ‘
ğ‘
ğ‘›
ğ‘âƒ—Â  ğ‘
ğœ–
ğ‘›
ğœ–âˆ¼  (0, )ğœ2",596,semantic
a441f411-4859-4147-948f-a3b5f79bde58,Introduction to Linear Models.pdf,CSCI_83,9,"Single Predictor Regression
Consider a simple case of regression with a single predictor
Only two coefï¬cients ( ) deï¬ning a straight line
 the intercept term
Intercept is value of  at 
 model coefï¬cient for the predictor variable
The slope coefï¬cient
Given a variable predictor value , the prediction, , is:
 is the 0 mean Normally distributed residual; 
ğ‘=2
=[]ğ‘âƒ—Â  ğ›½0
ğ›½1
=ğ›½0
ğ‘¦ ğ‘¥=0
=ğ›½1
ğ‘¥ğ‘– ğ‘¦Ì‚Â ğ‘–
ğ‘¦Ì‚Â ğ‘–
ğ‘¦Ì‚Â ğ‘–
ğœ–ğ‘–
= +ğ›½0 ğ›½1ğ‘¥ğ‘–
=ğ‘¦âˆ’ğœ–ğ‘–
âˆ¼  (0, )ğœ2
ğœ–ğ‘– ğ™´()=0ğœ–ğ‘–",445,semantic
b11ce611-247c-433b-940b-684ea6d1264c,Introduction to Linear Models.pdf,CSCI_83,10,"Example
Letâ€™s start with a simulated data set with one predictor and one response variable 0
The response variable,  is linear in  with additive random noise 
Intercept  and slope 
The ï¬rst 10 rows:
##           x         y## 0  0.000000  1.951736## 1  0.204082  0.627047## 2  0.408163  3.025441## 3  0.612245  1.112869## 4  0.816327  5.225976## 5  1.020408 -0.382646## 6  1.224490  3.969339## 7  1.428571  2.834755## 8  1.632653  1.516411## 9  1.836735  2.958745
ğ‘¦ ğ‘¥ âˆ¼  (0,2)
=0 =1.0",484,semantic
2163b8fe-dfea-411d-82b0-86d9a5c490e3,Introduction to Linear Models.pdf,CSCI_83,11,"Example
Notice the linear trend for these data
How do we compute a best ï¬t model for this relationship",102,semantic
909fd854-6e17-41c1-a148-9f4653582c00,Introduction to Linear Models.pdf,CSCI_83,12,"The Model Matrix
How do we create the model matrix
Start with a data table of  samples with  columns
First column is predictor variable
Second column is the response variable
ğ‘› ğ‘=2
â¡
â£
â¢â¢â¢â¢â¢â¢
,ğ‘¥1ğ‘¦1
,ğ‘¥2ğ‘¦2
,ğ‘¥3ğ‘¦3
â‹® ,â‹® ,ğ‘¥ğ‘›ğ‘¦ğ‘›
â¤
â¦
â¥â¥â¥â¥â¥â¥",231,semantic
a20ab610-b90f-4384-be0f-756adca6d203,Introduction to Linear Models.pdf,CSCI_83,13,"The Model Matrix
The model matrix for this case, including the intercept term is:
The column of 1â€™s deï¬ne the intercept term
â¡
â£
â¢â¢â¢â¢â¢â¢
1,ğ‘¥1
1,ğ‘¥2
1,ğ‘¥3
â‹® ,â‹® 1,ğ‘¥ğ‘›
â¤
â¦
â¥â¥â¥â¥â¥â¥",171,semantic
a1f405ee-3e31-40e1-a029-0d03eeeb7096,Introduction to Linear Models.pdf,CSCI_83,14,"Constructing the Model
For the  data samples and the parameter vector  we can construct the entire model:
For a single prediction:
Or, in matrix notation:
We are assuming that the error, , is all attributable to the dependent variable, 
ğ‘› ğ‘âƒ—Â 
= []+
â¡
â£
â¢â¢â¢â¢â¢â¢
ğ‘¦1
ğ‘¦2
ğ‘¦3
â‹® ğ‘¦ğ‘›
â¤
â¦
â¥â¥â¥â¥â¥â¥
â¡
â£
â¢â¢â¢â¢â¢â¢
1,ğ‘¥1
1,ğ‘¥2
1,ğ‘¥3
â‹® ,â‹® 1,ğ‘¥ğ‘›
â¤
â¦
â¥â¥â¥â¥â¥â¥
ğ›½0
ğ›½1
â¡
â£
â¢â¢â¢â¢â¢â¢
ğœ–1
ğœ–2
ğœ–3
â‹® ğœ–ğ‘›
â¤
â¦
â¥â¥â¥â¥â¥â¥
= + +ğ‘¦ğ‘– ğ›½0 ğ›½1ğ‘¥ğ‘– ğœ–ğ‘–
=ğ´+ğ‘¦Ì‚Â  ğ‘âƒ—Â ğœ–âƒ—Â 
ğœ– ğ‘¦",410,semantic
8dde7dc8-4386-4d44-b2b2-4ab7ce07f8f9,Introduction to Linear Models.pdf,CSCI_83,15,"Estimating the Model Parameters
How do we ï¬nd the best value for the coefï¬cients
Need to minimize an error metric
Find  by minimizing the sum of squared errors is known as the least squares method
Given training data, minimize the squared error between the prediction, , and the observed response variable or label, :
 is the ith row of 
ğ‘âƒ—Â 
ğ‘¦Ì‚Â ğ‘– ğ‘¦ğ‘–
( âˆ’ = ( âˆ’ =minğ‘âƒ—Â  âˆ‘ğ‘–
ğ‘¦ğ‘– ğ´ğ‘–,.ğ‘Ì‚Â )2 minğ‘âƒ—Â  âˆ‘ğ‘–
ğ‘¦ğ‘– ğ‘¦ğ‘–^)2 minğ‘âƒ—Â  âˆ‘ğ‘–
ğœ–2ğ‘–
ğ´ğ‘–,. ğ´",423,semantic
c29a4f92-53dd-41e3-a07e-d889eb988e50,Introduction to Linear Models.pdf,CSCI_83,16,"Estimating the Model Parameters
We could try a naive solution:
Where  is the matrix inverse of 
This might work, BUT
Direct matrix inverse algorithm has complexity , so inefï¬cient
There is no guarantee the inverse exists
The  features can be colinear
= ğ‘¦ğ‘âƒ—Â  ğ´âˆ’1
ğ´âˆ’1 ğ´
ğ‘‚()ğ‘›3
ğ‘",275,semantic
9341d8f4-7c14-41eb-beb9-97f0393ca69c,Introduction to Linear Models.pdf,CSCI_83,17,"Estimating the Model Parameters
We can use the Normal equations
Start with the problem:
Multiply by  and set 
Taking the inverse of  we arrive at the normal equations
 is the covariance matrix for the data set
Is dimension only  x 
For single predictor model this is just dimension 2x2
Much easier to take inverse
But poor scaling for large-scale problems, 
ğ‘¦âˆ’ğ´=â†’0ğ‘âƒ—Â  ğœ–âƒ—Â 
ğ´ğ‘‡ =0ğœ–âƒ—Â 
ğ´=ğ´ğ‘‡ ğ‘âƒ—Â  ğ´ğ‘‡ğ‘¦Ì‚Â 
ğ´ğ´ğ‘‡
=( ğ´ğ‘âƒ—Â  ğ´ğ‘‡ )âˆ’1ğ´ğ‘‡ğ‘¦Ì‚Â 
ğ´ğ´ğ‘‡
ğ‘ğ‘
ğ‘>1,000",434,semantic
3d6ce7a7-b3ac-4fce-9eed-a01ed74be9dc,Introduction to Linear Models.pdf,CSCI_83,18,"Estimating the Model Parameters
What is the relationship between the normal equations, least squares and maximum likelihood? First consider how the Normal likelihood can be written in terms of  with  feature vectors of observations  and  labels :
The log-likelihood is then:
For a ï¬xed  one can see that the log-likelihood is maximized by minimizing the squared error:
ğ‘âƒ—Â  ğ‘› ğ‘¥âƒ—Â ğ‘– ğ‘› ğ‘¦ğ‘–
 (, )= ğ‘’ğ‘¥ğ‘{âˆ’ ( âˆ’ }ğ‘âƒ—Â ğœ2 âˆğ‘–=1
ğ‘› 1(2ğœ‹ğœ2)1/2
12ğœ2ğ‘¦ğ‘– ğ‘¥ğ‘‡ğ‘–ğ‘âƒ—Â )2
l(, )=âˆ’{ğ‘›Â ğ‘™ğ‘œğ‘”( )+ ( âˆ’ }ğ‘âƒ—Â ğœ2 12 ğœ2 1ğœ2âˆ‘ğ‘–=1
ğ‘›
ğ‘¦ğ‘– ğ‘¥ğ‘‡ğ‘–ğ‘âƒ—Â )2
ğœ2
ğ‘†ğ‘†()= ( âˆ’ğ‘âƒ—Â  âˆ‘ğ‘–=1
ğ‘›
ğ‘¦ğ‘– ğ‘¥ğ‘‡ğ‘–ğ‘âƒ—Â )2",534,semantic
4daf8959-0ba5-4fd6-818e-7e57a14e5816,Introduction to Linear Models.pdf,CSCI_83,19,"Estimating the Model Parameters
Minimize sum of square errors to maximize log-likelihood
Set the ï¬rst derivative of sum of square errors to zero
Or, in matrix form:
Solving the above leads to the normal equations:
But still need to compute inverse of covariance matrix, 
=2 ( âˆ’ )=0âˆ‚Â ğ‘†ğ‘†()ğ‘âƒ—Â 
âˆ‚Â ğ‘âƒ—Â  âˆ‘ğ‘–=1
ğ‘›
ğ‘¥ğ‘‡ğ‘– ğ‘¦ğ‘– ğ‘¥ğ‘‡ğ‘–ğ‘âƒ—Â 
(âˆ’ğ´)=0ğ´ğ‘‡ğ‘¦âƒ—Â  ğ‘âƒ—Â 
=( ğ´ğ‘âƒ—Â  ğ´ğ‘‡ )âˆ’1ğ´ğ‘‡ğ‘¦Ì‚Â 
ğ´ğ´ğ‘‡",357,semantic
72f2d6a6-0859-4e98-b176-3201c1e0338a,Introduction to Linear Models.pdf,CSCI_83,20,"Estimating the Model Parameters
Are there more scalable ways to solve the least squares problems? Inverting the covariance matrix is a big improvement over a naive approach, it still requires taking a large matrix inverse at scale. Can we solve the least squares problem in a more computationally efï¬cient way?",310,semantic
5b1835bd-6e33-45bc-a75d-66bf9b020b47,Introduction to Linear Models.pdf,CSCI_83,20,"Start with the linear equations for maximum likelihood
Eliminate  from both sides
We can ï¬nd the minimum of this linear system with an efï¬cient solver
Stochastic Gradient Descent (SGD) and its relatives
Quasi-Newton methods like L-BFGS
ğ‘>1,000
(âˆ’ğ´)=0ğ´ğ‘‡ğ‘¦âƒ—Â  ğ‘âƒ—Â 
ğ´ğ‘‡
âˆ’ğ´=0ğ‘¦âƒ—Â  ğ‘âƒ—",273,semantic
b5100c4e-8876-4733-b009-241dcf13b865,Introduction to Linear Models.pdf,CSCI_83,21,"Estimating the Model Parameters
What is the relationship between the normal equations, least squares and maximum likelihood? Solving the least problem is equivalent to solving the maximum likelihood estimation problem
The normal equations are a maximum likelihood estimator
Solving the system of linear equations, , results in a least squares maximum likelihood solution
The above applies when residuals are Normally distributed
âˆ’ğ´=0ğ‘¦âƒ—Â  ğ‘âƒ—",439,semantic
1105780b-8d3b-4bae-a75b-4337dca1c535,Introduction to Linear Models.pdf,CSCI_83,22,"Example - Specifying the Model
How do we specify the model formula with statsmodels? Use the S/R style model formula developed by Chambers and Hastie; Statistical Models in S (1992). Uses the  operator to mean *modeled by**
Example; dependent variable (dv) modeled by two independent variables (var1 and var2):
Example; dependent variable (dv) modeled by independent variables (var1) and its square, uses the  operator to wrap a function:
Example; dependent variable (dv) is modeled by two independent variables (var1 and var2) and the interaction term with no intercept term:
Example; dependent variable (dv) modeled by independent numeric variable (var1) and a categorical variable (var2):
âˆ¼ 
ğ‘‘ğ‘’ğ‘ğ‘’ğ‘›ğ‘‘ğ‘’ğ‘›ğ‘¡Â ğ‘£ğ‘ğ‘Ÿğ‘–ğ‘ğ‘ğ‘™ğ‘’âˆ¼ ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘ğ‘’ğ‘›ğ‘’ğ‘›ğ‘¡Â ğ‘£ğ‘ğ‘Ÿğ‘–ğ‘ğ‘ğ‘™ğ‘’ğ‘ 
ğ‘‘ğ‘£âˆ¼ ğ‘£ğ‘ğ‘Ÿ1+ğ‘£ğ‘ğ‘Ÿ2
ğ¼()
ğ‘‘ğ‘£âˆ¼ ğ‘£ğ‘ğ‘Ÿ1+ğ¼(ğ‘£ğ‘ğ‘Ÿ1âˆ— âˆ— 2)
ğ‘‘ğ‘£âˆ¼ âˆ’1+ğ‘£ğ‘ğ‘Ÿ1âˆ— ğ‘£ğ‘ğ‘Ÿ2âŸ· ğ‘‘ğ‘£âˆ¼ âˆ’1+ğ‘£ğ‘ğ‘Ÿ1+ğ‘£ğ‘ğ‘Ÿ2+ğ‘£ğ‘ğ‘Ÿ1:ğ‘£ğ‘ğ‘Ÿ2
ğ‘‘ğ‘£âˆ¼ ğ‘£ğ‘ğ‘Ÿ1+ğ¶(ğ‘£ğ‘ğ‘Ÿ2)",838,semantic
63946922-9a42-4d45-9f0c-3324a535f3c0,Introduction to Linear Models.pdf,CSCI_83,23,"Example - Fitting the Model
Fit the model using statsmodels.formula.api.ols to create a linear model object
## Intercept = 1.611  Slope = 0.882
Find the predicted values for each value of :
##           x         y  predicted## 0  0.000000  1.951736   1.611189## 1  0.204082  0.627047   1.791233## 2  0.408163  3.025441   1.971277## 3  0.612245  1.112869   2.151321## 4  0.816327  5.225976   2.331365
## Define the regression model and fit it to the data
ols_model = smf.ols(formula = 'y ~ x', data=sim_data).fit()
## Print the model coefficient
print('Intercept = %4.3f  Slope = %4.3f' % (ols_model._results.params[0 ], ols_model._results.params[1 ]))
ğ‘¥
# Add predicted to pandas dataframe
sim_data['predicted'] = ols_model.predict(sim_data.x)# View head of data framesim_data.head(5 )",786,semantic
1ecfef3f-32a0-40f5-8349-fd9e7ae5dbff,Introduction to Linear Models.pdf,CSCI_83,24,"Example - Model Checking
Plot the regression line against the original data
This looks like a good ï¬t, but how good is it really?",129,semantic
4b9df581-2bff-4375-bd13-cd459e366c6d,Introduction to Linear Models.pdf,CSCI_83,25,"Evaluating Regression Models
Evaluation of regression models focuses on the residuals or errors
Residuals should be Normally distributed with  mean and constant variance
The residuals must be homoskedastic with respect to the ï¬tted values
Homoskedastic residuals have constant variance with predicted values
Any trend or structure in the residuals indicates a poor model ï¬t
In these cases variance is not constant and we say these are heteroskedastic residuals
Heteroskedastic residuals indicate that model has not incorporated all available information
=âˆ’ğ´Â =âˆ’ğœ–âƒ—Â  ğ‘¦âƒ—Â  ğ‘âƒ—Â  ğ‘¦âƒ—Â ğ‘¦Ì‚Â 
0
âˆ¼  (0, )ğœ–âƒ—Â  ğœ2",595,semantic
167b58a6-6e3c-43c9-b5c3-2a0d9e892898,Introduction to Linear Models.pdf,CSCI_83,26,"Evaluating Regression Models
Residual plots is a key diagnostic for any regression model
Plot residual against the predicted values
These residuals look homoskedastic - we are happy!",182,semantic
c071d288-9ac4-45a8-8920-7eee201d17e3,Introduction to Linear Models.pdf,CSCI_83,27,"Evaluating Regression Models
Graphically test that the residuals are iid Normal
These plots look promising",106,semantic
778a4a72-0bde-4fdb-aad1-2cdfa6ba5e39,Introduction to Linear Models.pdf,CSCI_83,28,"Evaluating Regression Models
We can quantitatively understand model performance by deï¬ning these relationships
The relationship between these metrics:
Or, we can say that the sum of squares explained by the model is:
ğ‘†ğ‘†ğ‘‡ğ‘†ğ‘†ğ¸ğ‘†ğ‘†ğ‘…
=ğ‘ ğ‘¢ğ‘šÂ ğ‘ ğ‘ğ‘¢ğ‘ğ‘Ÿğ‘’Â ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™Â = ( âˆ’Î£ğ‘–ğ‘¦ğ‘– ğ‘ŒÂ¯)2
=ğ‘ ğ‘¢ğ‘šÂ ğ‘ ğ‘ğ‘¢ğ‘ğ‘Ÿğ‘’Â ğ‘’ğ‘¥ğ‘ğ‘™ğ‘ğ‘–ğ‘›ğ‘’ğ‘‘Â = ( âˆ’Î£ğ‘–ğ‘¦ğ‘–^ ğ‘ŒÂ¯)2
=ğ‘ ğ‘¢ğ‘šÂ ğ‘ ğ‘ğ‘¢ğ‘ğ‘Ÿğ‘’Â ğ‘Ÿğ‘’ğ‘ ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™Â = ( âˆ’Î£ğ‘–ğ‘¦ğ‘– ğ‘¦ğ‘–^)2
ğ‘†ğ‘†ğ‘‡=ğ‘†ğ‘†ğ‘…+ğ‘†ğ‘†ğ¸
ğ‘†ğ‘†ğ¸=ğ‘†ğ‘†ğ‘‡âˆ’ğ‘†ğ‘†ğ‘…",358,semantic
2470021b-c1c2-4d03-8abd-fee0d9004882,Introduction to Linear Models.pdf,CSCI_83,29,"Evaluating Regression Models
Compute the sums of squares for the running example
## SST = 494.61
## SSE = 337.53
## SSR = 157.08
## SSE + SSR = 494.61
The model has explained most of the TSS
y_bar = np.mean(sim_data.y)
SST = np.sum(np.square(np.subtract(sim_data.y, y_bar)))SSR = np.sum(np.square(sim_data.resids))SSE = np.sum(np.square(np.subtract(sim_data.predicted, y_bar)))
print('SST = {0:6.2f}'.format(SST))
print('SSE = {0:6.2f}'.format(SSE))
print('SSR = {0:6.2f}'.format(SSR))
print('SSE + SSR = {0:6.2f}'.format(SSE + SSR))",533,semantic
eb2296b4-f081-4d4d-b696-266c423f89fe,Introduction to Linear Models.pdf,CSCI_83,30,"Evaluating Regression Models
We can compare the sum of square residual to the sum of square total to evaluate how well our model explains the data
We call the ratio   or the coefï¬cient of determination
The  for a perfect model would behave as follows:
A model which does not explain the data at all has:
ğ‘†ğ‘†ğ‘‡âˆ’ğ‘†ğ‘†ğ‘…=ğ‘†ğ‘†ğ¸
ğ‘†ğ‘†ğ¸ğ‘†ğ‘†ğ‘‡ğ‘…2
=1âˆ’ğ‘…2 ğ‘†ğ‘†ğ‘…ğ‘†ğ‘†ğ‘‡
ğ‘…2
ğ‘†ğ‘†ğ‘…ğ‘…2â†’0â†’1
ğ‘†ğ‘†ğ‘…ğ‘…2â†’ğ‘†ğ‘†ğ‘‡â†’0",362,semantic
ff964408-4777-492a-bffc-42f09f3c2303,Introduction to Linear Models.pdf,CSCI_83,31,"Evaluating Regression Models
As the number of model parameters increases the model will ï¬t the data better
But, the model will become over-ï¬t as the number of parameters increases
Must adjust model performance for degrees of freedom - adjusted 
This gives  as:
Or, we can rewrite  as:
ğ‘…2
ğ‘…2ğ‘ğ‘‘ğ‘—
ğ‘¤â„ ğ‘’ğ‘Ÿğ‘’ğ‘‘ğ‘“ğ‘†ğ‘†ğ‘…
ğ‘‘ğ‘“ğ‘†ğ‘†ğ‘‡
=1âˆ’ =1âˆ’
ğ‘†ğ‘†ğ‘…ğ‘‘ğ‘“ğ‘†ğ‘†ğ‘…
ğ‘†ğ‘†ğ‘‡ğ‘‘ğ‘“ğ‘†ğ‘†ğ‘‡
ğ‘£ğ‘ğ‘Ÿğ‘Ÿğ‘’ğ‘ ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™
ğ‘£ğ‘ğ‘Ÿğ‘¡ğ‘œğ‘¡ğ‘ğ‘™
=ğ‘†ğ‘†ğ‘…Â ğ‘‘ğ‘’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘ Â ğ‘œğ‘“Â ğ‘“ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘œğ‘š=ğ‘†ğ‘†ğ‘‡Â ğ‘‘ğ‘’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘ Â ğ‘œğ‘“Â ğ‘“ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘œğ‘š
ğ‘…2ğ‘ğ‘‘ğ‘—
ğ‘…2ğ‘ğ‘‘ğ‘—
ğ‘¤â„ ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘˜
=1âˆ’(1âˆ’ )ğ‘…2ğ‘›âˆ’1ğ‘›âˆ’ğ‘˜
=ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘ŸÂ ğ‘œğ‘“Â ğ‘‘ğ‘ğ‘¡ğ‘Â ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ =ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘ŸÂ ğ‘œğ‘“Â ğ‘šğ‘œğ‘‘ğ‘’ğ‘™Â ğ‘ğ‘œğ‘’ğ‘“ğ‘“ğ‘–ğ‘ğ‘–ğ‘’ğ‘›ğ‘¡ğ‘ 
ğ‘…2ğ‘ğ‘‘ğ‘—
=1.0âˆ’ğ‘…2ğ‘ğ‘‘ğ‘— ğ‘†ğ‘†ğ‘…ğ‘†ğ‘†ğ‘‡ ğ‘›âˆ’1ğ‘›âˆ’1âˆ’ğ‘˜",529,semantic
82417869-4d96-4687-88db-0f1cddcbeb0f,Introduction to Linear Models.pdf,CSCI_83,32,"Evaluating Regression Models
The summary table for the OLS model provides a number of summary statistics
## <class 'statsmodels.iolib.summary.Summary'>## """"""##                             OLS Regression Results                            ## ==============================================================================## Dep. Variable:                      y   R-squared:                       0.682## Model:                            OLS   Adj. R-squared:                  0.676## Method:                 Least Squares   F-statistic:                     103.1## Date:                Thu, 27 Oct 2022   Prob (F-statistic):           1.52e-13## Time:                        09:27:07   Log-Likelihood:                -99.565## No.",730,semantic
653dc1c2-e242-4fc0-aa8e-d89ecba3a230,Introduction to Linear Models.pdf,CSCI_83,32,"Observations:                  50   AIC:                             203.1## Df Residuals:                      48   BIC:                             207.0## Df Model:                           1                                         ## Covariance Type:            nonrobust                                         ## ==============================================================================##                  coef    std err          t      P>|t|      [0.025      0.975]## ------------------------------------------------------------------------------## Intercept      1.6112      0.504      3.196      0.002       0.598       2.625## x              0.8822      0.087     10.156      0.000       0.708       1.057## ==============================================================================## Omnibus:                        0.850   Durbin-Watson:                   2.312## Prob(Omnibus):                  0.654   Jarque-Bera (JB):                0.450## Skew:                          -0.228   Prob(JB):                        0.799## Kurtosis:                       3.091   Cond. No. 11.7## ==============================================================================## ## Notes:## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.## """"""
ols_model.summary()",1316,semantic
4055c3ae-04c7-4944-a494-aeee81748e4c,Introduction to Linear Models.pdf,CSCI_83,33,"Evaluating Regression Models
We also can evaluate models by error metrics
The root mean square error (RMSE) is a measure of the mean of the squared residuals
## RMSE =   0.25
The median absolute error (MAE) is a robust measure of mean residuals
## MAE=    1.3
And many more possibilitiesâ€¦
ğ‘…ğ‘€ğ‘†ğ¸= =( âˆ’Î£ğ‘›ğ‘–âˆ’1ğ‘¦ğ‘– ğ‘¦ğ‘–^)2
ğ‘›âˆš ğ‘†ğ‘†ğ‘…ğ‘›âˆš
print('RMSE = {0:6.2}'.format(sqrt(np.sum(np.square(sim_data.resids)))/ float(sim_data.shape[0 ])))
ğ‘€ğ´ğ¸=ğ‘šğ‘’ğ‘‘[| âˆ’|]ğ‘¦ğ‘– ğ‘¦ğ‘–^
print('MAE= {0:6.2}'.format(np.median(np.absolute(sim_data.resids))))",511,semantic
40379007-ed85-4927-a3a7-b0e2969dedc8,Introduction to Linear Models.pdf,CSCI_83,34,"Evaluating Regression Models
When evaluating any machine learning model consider all evaluation methods available
No one method is most important all of the time
Different methods highlight different problems with your model
Donâ€™t forget to check that the model must make sense for your application!",299,semantic
3adb5822-c119-4b3e-bc90-3a7dd68e1b88,Introduction to Linear Models.pdf,CSCI_83,35,"Example - Fitting center Model
Fit the model using statsmodels.formula.api.ols with centered independent variable to create a linear model object
## Intercept = 6.022  Slope = 0.882
We can now interpret this model
Intercept is the mean of the dependent variable
Slope is the rate of change of the dependent variable for unit change in independent variable
Interceopt is value of independent variable where independent varaibles all 
May not even be in deï¬ned range of independent varaible
e.g. How can we interpret a negaive life expectancy
## Center the independent variable   
sim_data.loc[:,'x_centered'] = np.subtract(sim_data.x, np.mean(sim_data.x))## Define the regression model and fit it to the dataols_model_centered = smf.ols(formula = 'y ~ x_centered', data=sim_data).fit()
## Print the model coefficientprint('Intercept = %4.3f  Slope = %4.3f' % (ols_model_centered._results.params[0 ], ols_model_centered._results.params[1 ]))
=0",942,semantic
095ef3c9-4e68-434d-93a6-d7201c175504,Introduction to Linear Models.pdf,CSCI_83,36,"Example - Fitting center Model
Plot the regression line against the centered independent variable",97,semantic
6fec5f6e-8927-4149-9d12-a6c3b6868b2a,Introduction to Linear Models.pdf,CSCI_83,37,"Extending the Linear Model
We extend the linear model by adding new features or predictor variables
Higher order terms - e.g. polynomial regression
Other exogenous variables
We prefer the simplest model that does a reasonable job
The principle of Occamâ€™s razor
Must consider the bias-variance trade-off
High complexity model ï¬ts the training data well
Low bias
But might not generalize well to new cases - high variance
Lower complexity model can generalize to new cases
low variance
But does not ï¬t training data as well - high bias",533,semantic
ddce1012-0ecc-4dcb-a564-4cc4b1bec2d6,Introduction to Linear Models.pdf,CSCI_83,38,"Extending the Linear Model
Building a model matrix for a more complex linear model is easy
We now have  model coefï¬cients, including intercept term
With  features the model matrix is:
We still seek the least squares solution
The covariance matrix is now  x , including intercept term
ğ‘+1
=[, , ,â€¦, ]ğ‘âƒ—Â  ğ›½0ğ›½1ğ›½2 ğ›½ğ‘
ğ‘
ğ´=
â¡
â£
â¢â¢â¢â¢â¢â¢
1, , ,â€¦,ğ‘¥1,1ğ‘¥1,2 ğ‘¥1,ğ‘
1, , ,â€¦,ğ‘¥2,1ğ‘¥2,2 ğ‘¥2,ğ‘
1, , ,â€¦,ğ‘¥3,1ğ‘¥3,2 ğ‘¥3,ğ‘
â‹® ,Â â‹® ,Â â‹® ,Â â‹® ,Â â‹® 1, , ,â€¦,ğ‘¥ğ‘›,1ğ‘¥ğ‘›,2 ğ‘¥ğ‘›,ğ‘
â¤
â¦
â¥â¥â¥â¥â¥â¥
ğ‘+1ğ‘+1",452,semantic
527f11bb-faaf-41a8-b264-ea66b2cfba2f,Introduction to Linear Models.pdf,CSCI_83,39,"Example of Multi-Feature Linear Model
A new simulated data set",62,semantic
a06654f4-3822-4467-9a2f-6ba4ae5a6da9,Introduction to Linear Models.pdf,CSCI_83,40,"Example of Multi-Feature Linear Model
First, try a simple straight line model with intercept and slope terms
## Intercept = 10.256  Slope = 1.296",145,semantic
c478b42e-9827-4b87-b5c2-f3181375f632,Introduction to Linear Models.pdf,CSCI_83,41,"Example of Multi-Feature Linear Model
What do the residuals look like? These residuals look heteroskedastic!",108,semantic
6441f8ef-6446-4991-a50e-8a9d6fcdd8ac,Introduction to Linear Models.pdf,CSCI_83,42,"Example of Multi-Feature Linear Model
Test that the residuals are iid Normal
Do these residuals have Normal distribution?",121,semantic
0482736e-261b-4e7e-887b-e9ac922d11d1,Introduction to Linear Models.pdf,CSCI_83,43,"Example of Multi-Feature Linear Model
The model summary is:
## <class 'statsmodels.iolib.summary.Summary'>## """"""##                             OLS Regression Results                            ## ==============================================================================## Dep. Variable:                      y   R-squared:                       0.106## Model:                            OLS   Adj. R-squared:                  0.087## Method:                 Least Squares   F-statistic:                     5.684## Date:                Thu, 27 Oct 2022   Prob (F-statistic):             0.0211## Time:                        09:27:09   Log-Likelihood:                -191.24## No.",685,semantic
6ca62c54-957b-494b-a627-68a2a101b090,Introduction to Linear Models.pdf,CSCI_83,43,"Observations:                  50   AIC:                             386.5## Df Residuals:                      48   BIC:                             390.3## Df Model:                           1                                         ## Covariance Type:            nonrobust                                         ## ==============================================================================##                  coef    std err          t      P>|t|      [0.025      0.975]## ------------------------------------------------------------------------------## Intercept     10.2555      1.600      6.408      0.000       7.038      13.473## x              1.2955      0.543      2.384      0.021       0.203       2.388## ==============================================================================## Omnibus:                        0.893   Durbin-Watson:                   1.502## Prob(Omnibus):                  0.640   Jarque-Bera (JB):                0.269## Skew:                          -0.037   Prob(JB):                        0.874## Kurtosis:                       3.352   Cond. No. 2.95## ==============================================================================## ## Notes:## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.## """"""",1296,semantic
a2899926-0a1d-48c8-9678-2693594e6720,Introduction to Linear Models.pdf,CSCI_83,44,"Example of Multi-Feature Linear Model
Letâ€™s add a second order polynomial term, so the model is now:
## Intercept = 4.051  Partial Slope = 1.296  Second Order Partial slope = 0.715
ğ‘¦= +ğ‘¥+ğ›½0 ğ›½1 ğ›½2ğ‘¥2",197,semantic
b665f273-ce1e-428f-bd78-278fa4d149cc,Introduction to Linear Models.pdf,CSCI_83,45,"Example of Multi-Feature Linear Model
What do the residuals look like with the second order term? These residuals are close to homoskedastic!",141,semantic
3e02c281-a345-4732-93ca-1acd2c0653fd,Introduction to Linear Models.pdf,CSCI_83,46,"Example of Multi-Feature Linear Model
Test that the residuals are iid Normal for the polynomial model
Do these residuals have close to a Normal distribution?",157,semantic
d6095d8b-6694-4012-9dbd-5d6527241683,Introduction to Linear Models.pdf,CSCI_83,47,"Example of Multi-Feature Linear Model
The second order model summary is:
## <class 'statsmodels.iolib.summary.Summary'>## """"""##                             OLS Regression Results                            ## ==============================================================================## Dep. Variable:                      y   R-squared:                       0.330## Model:                            OLS   Adj. R-squared:                  0.301## Method:                 Least Squares   F-statistic:                     11.55## Date:                Thu, 27 Oct 2022   Prob (F-statistic):           8.29e-05## Time:                        09:27:10   Log-Likelihood:                -184.04## No.",698,semantic
714c91fe-6753-4f6b-ba6d-c58934185683,Introduction to Linear Models.pdf,CSCI_83,47,"Observations:                  50   AIC:                             374.1## Df Residuals:                      47   BIC:                             379.8## Df Model:                           2                                         ## Covariance Type:            nonrobust                                         ## ==============================================================================##                  coef    std err          t      P>|t|      [0.025      0.975]## ------------------------------------------------------------------------------## Intercept      4.0507      2.101      1.928      0.060      -0.177       8.278## x              1.2955      0.476      2.725      0.009       0.339       2.252## I(x ** 2)      0.7154      0.181      3.960      0.000       0.352       1.079## ==============================================================================## Omnibus:                        0.305   Durbin-Watson:                   1.995## Prob(Omnibus):                  0.859   Jarque-Bera (JB):                0.014## Skew:                           0.009   Prob(JB):                        0.993## Kurtosis:                       3.081   Cond. No. 17.5## ==============================================================================## ## Notes:## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.## """"""",1377,semantic
6db68e36-47c6-407c-ba73-4a54672faa09,Introduction to Linear Models.pdf,CSCI_83,48,"Dealing With Outliers
Outliers are a persistent problem with statistical and machine learning models
What are outliers? Errors or noisy measurements
Result of improper stratiï¬cation
But, may be of interest
Depending on the application can be the most interesting values!! May need to explicitly model
Example: Fraud detection
Example: Scientiï¬c discovery
Outliers can be hard to detect
Difï¬cult in high-dimensions
Often ï¬nd by inï¬‚uence on model",444,semantic
0e6e6384-603c-4a75-8e92-db8a8bb03177,Introduction to Linear Models.pdf,CSCI_83,49,"Dealing With Outliers
Example: Add a single outlier regression data set
## Intercept = 5.098  Partial Slope = 0.365
This outlier has high leverage and changes the slope signiï¬cantly",181,semantic
5daa9356-3df9-4c33-8c5f-510ddf915cd9,Introduction to Linear Models.pdf,CSCI_83,50,"Dealing With Outliers
Cookâ€™s distance, , measures the inï¬‚uence of an outlier on a model
Cookâ€™s distance for the ith data point is the degree of freedom adjusted average squared error against a model without this value
where, the jth prediction computed with all observations the jth prediction computed without the ith observation number of parameters number of data points
Cookâ€™s distance is computed using a leave-one-out resampling algorithm! ğ·ğ‘–
=ğ·ğ‘– ( âˆ’Î£ğ‘›ğ‘—=1ğ‘ŒÌ‚Â ğ‘— ğ‘ŒÌ‚Â ğ‘—(ğ‘–))2
ğ‘›(ğ‘+1)ğœ2^
=ğ‘ŒÌ‚Â ğ‘— =ğ‘ŒÌ‚Â ğ‘—(ğ‘–)ğ‘=ğ‘›=",504,semantic
7a5df6dd-2341-4128-8db3-751bc9f0d38a,Introduction to Linear Models.pdf,CSCI_83,51,"Dealing With Outliers
Plot Cookâ€™s distance as leverage vs. the residual size
Outlying points shown at edges of plot
Not all outliers have high leverage
Can detect outliers in moderate dimensions
_=influence_plot(ols_model_ol)",225,semantic
f616e5be-b464-4a50-94c7-4f70fb8b6d2d,Introduction to Linear Models.pdf,CSCI_83,52,"Dealing With Outliers
Why are linear models sensitive to outliers
Ordinary linear regression use squared error loss function
Optimal if the errors are iid Normal
Is an unbiased estimator
In 1-dimension median is robust to outliers
But far from an optimal estimator
High bias
Hard to implement beyond 1-dimension
We can study the response of estimators to outliers using an inï¬‚uence function",390,semantic
a876119f-85f0-418e-8c97-39edb9befa4d,Introduction to Linear Models.pdf,CSCI_83,53,"Dealing With Outliers
Compare the inï¬‚uence functions of the mean and median estimators
Mean estimator has linear inï¬‚uence function
Inï¬‚uence of outliers is unbounded
Derivative of the inï¬‚uence function is constant
Inï¬‚uence function of median estimator is discontinuous
Inï¬‚uence of any observation is constant
Derivative of inï¬‚uence function is not deï¬ned
Inï¬‚uence functions for mean and median
Figure from Hampel, el.al., Robust Statistics, 1986",444,semantic
b928f11b-0cef-46e7-8e74-5cc520a08c5a,Introduction to Linear Models.pdf,CSCI_83,54,"Dealing With Outliers
Could we simply edit out the outliers? But what fraction of the data are outliers? Know as the alpha trimmed mean algorithm
Order the values and remove  highest and lowest
But, alpha trimming is a bit arbitrary
Is a biased estimator, with bias increasing with 
 is the median
Alpha trimming hard to implement in higher dimensions
Inï¬‚uence functions for alpha trimmed mean
Figure from Hampel, el.al., Robust Statistics, 1986
ğ›¼/2
ğ›¼
ğ›¼=0.5",457,semantic
77d5596f-1c98-4c2e-be11-842b3767064e,Introduction to Linear Models.pdf,CSCI_83,55,"Dealing With Outliers
Are there better estimators when outliers are present
Yes, but must accept some bias
Idea; estimator can be unbiased near the expected value, but limit inï¬‚uence of outliers
Trade-off between high robustness and low bias
Many ideas have been tried
A major research focus in the 1970s and 1980s
Huber estimator
Family of M-estimators",353,semantic
198ac0c4-389f-41aa-995c-c079fca7474e,Introduction to Linear Models.pdf,CSCI_83,56,"Dealing With Outliers
What are the properties of the Huber estimator? Inï¬‚uence function is linear near the mean but constant away from the mean
hinge point is at , where  median absolute deviation
Robustness and bias increases as  decreases
Huber estimator is low bias
Unbiased for samples near the point estimate
Constant inï¬‚uence away from the point estimate
Inï¬‚uence function of the Huber estimator
Figure from Hampel, el.al., Robust Statistics, 1986
Â±ğ‘¡âˆ— ğ‘€ğ´ğ· ğ‘€ğ´ğ·=
ğ‘¡",468,semantic
7b070987-5388-43f6-87b4-9c9b1619213f,Introduction to Linear Models.pdf,CSCI_83,57,"Dealing With Outliers
M-estimators tapper inï¬‚uence to zero
Approximately linear inï¬‚uence near point estimate
So nearly unbiased near the point estimate
Then inï¬‚uence tappers to 0 for extreme outliers
An example is Tukeyâ€™s biweight
Only a single parameter for biweight function, 
Robustness and bias increase with decreasing 
Inï¬‚uence function of the Tukeyâ€™s Biweight M-estimator
Figure from Hampel, el.al., Robust Statistics, 1986
ğ‘Ÿ
ğ‘Ÿ",434,semantic
416c9c1b-0db8-46eb-aeef-2e7a4f737763,Introduction to Linear Models.pdf,CSCI_83,58,"Dealing With Outliers
Example: regression with Huber loss function
## <class 'statsmodels.iolib.summary.Summary'>## """"""##                     Robust linear Model Regression Results                    ## ==============================================================================## Dep. Variable:                      y   No. Observations:                   51## Model:                            RLM   Df Residuals:                       49## Method:                          IRLS   Df Model:                            1## Norm:                          HuberT                                         ## Scale Est.:                       mad                                         ## Cov Type:                          H1                                         ## Date:                Thu, 27 Oct 2022                                         ## Time:                        09:27:11                                         ## No.",935,semantic
50d7c8d5-59ac-40f3-8c41-51d474a5fb7c,Introduction to Linear Models.pdf,CSCI_83,58,"Iterations:                    15                                         ## ==============================================================================##                  coef    std err          z      P>|z|      [0.025      0.975]## ------------------------------------------------------------------------------## Intercept      1.8991      0.503      3.779      0.000       0.914       2.884## x              0.8448      0.087      9.659      0.000       0.673       1.016## ==============================================================================## ## If the model instance has been used for another fit with different fit parameters, then the fit options might not be the correct ones anymore .## """"""
## Define the robust regression model and fit it to the data
ols_model_huber = smf.rlm(formula ='y~x', data = sim_data_ol).fit()# Add predicted to pandas data framesim_data_ol['predicted_huber'] = ols_model_huber.predict(sim_data_ol.x)
## Display sumamryols_model_huber.summary()",995,semantic
c36eead2-9529-4e92-a784-5bbe5e2f4331,Introduction to Linear Models.pdf,CSCI_83,59,"Dealing With Outliers
Example: regression with Huber loss function
Notice the different slope for the regression with Huber loss",128,semantic
39aff864-de7d-475f-bdd1-388ef3c4ac33,Introduction to Linear Models.pdf,CSCI_83,60,"Linear Model Assumptions
There are a number of assumptions in linear models that you overlook at your peril! The feature or predictor variables should be independent of one another
This is rarely true in practice
Multi-colinearity between features makes the model under-determined
We assume that numeric features or predictors have zero mean and about the same scale
We do not want to bias the estimation of regression coefï¬cients with predictors that do not have a 0 mean
We do not want to have predictors with a large numeric range dominate training
Example: income is in the range of 10s or 100s of thousands and age is in the range of 10s, but apriori income is no more important than age as a predictor
Values of each predictor or feature should be iid
If variance changes with sample, the optimal value of the coefï¬cient could not be constant
If there serial correlation in the predictor values, the iid assumption is violated - but can account for this such as in time series models",989,semantic
d52fec62-d3d5-474c-9055-48178e6e2c44,Introduction to Linear Models.pdf,CSCI_83,61,"Summary
Linear models are a ï¬‚exible and widely used class of models
Fit model coefï¬cients by least squares estimation
Can use many types of predictor variables
SGD and L-FBGS algorithms allow massive scaling of linear models
We prefer the simplest model that does a reasonable job
The principle of Occamâ€™s razor
Must consider the bias-variance trade-off",353,semantic
5f9b16ab-fb5d-4f43-aa34-4e39ffd8d01a,Introduction to Linear Models.pdf,CSCI_83,62,"Summary
When evaluating any machine learning model consider all evaluation methods available
No one method best all of the time
Homoskedastic Normally distributed residuals
Reasonable values , RMSE, etc
Are the model coefï¬cients all signiï¬cant? Different methods highlight different problems with your model
Donâ€™t forget to check that the model must make sense for your application! ğ‘…2",385,semantic
fd99c8aa-f855-4263-abae-bf686bf41634,IntroductionToClustering.pdf,CSCI_83,0,"CSCI E-83Introduction to Clustering ModelsSteve Elston
Copyright 2020,2021, Stephen F Elston. All rights reserved.",114,semantic
e778d7b5-ae83-4f19-bec8-873d7cd16281,IntroductionToClustering.pdf,CSCI_83,1,"Introduction to Unsupervised Learningâ€¢Until now we have been working with supervised machine learningâ€¢Training and evaluation cases are labeledâ€¢Data contain features, X, and labels, yâ€¢The model learns a function approximation using the labeled cases:ğ‘“ğ‘‹=ğ‘¦â€¢We say the model is trained by a supervisorâ€¢Can we always expect to have labeled cases? â€¢No!â€¢Most data is not labeledâ€¢What can be learned from unlabeled data?",413,semantic
37dd0f45-e352-48e6-9bd1-d3d4198655c9,IntroductionToClustering.pdf,CSCI_83,2,"Introduction to Unsupervised Learningâ€¢Without labeled data, what can a model learn?â€¢Can learn structure of dataâ€¢Structure is learned by determining association between casesâ€¢Association based on measures of proximity, distance or dissimilarityâ€¢Clustering algorithms are data mining methodsâ€¢Data mining seeks to find interesting relationships in dataâ€¢We have already encountered feature importance as a data mining method",420,semantic
6dbe6522-dbf9-4462-953b-37849cdfa1e3,IntroductionToClustering.pdf,CSCI_83,3,"Introduction to Unsupervised Learningâ€¢Algorithms can use different distance or dissimilarity metricsâ€¢Structure based on distance metricsâ€¢Different algorithms use different metricsâ€¢What are the ideal properties of clusters? â€¢Good clusters have two propertiesâ€¢Compactness: We what the clusters to be small with members close to each otherâ€¢Separation: We want the clusters are well separated, a closeness property",410,semantic
fa5fe34f-bd11-40d0-9aed-1ac1c3806ce6,IntroductionToClustering.pdf,CSCI_83,4,Introduction to Unsupervised Learningâ€¢Why should we care about unsupervised learning?â€¢Most data is not labeledâ€¢Learning data structure is useful in many applicationsâ€¢Find groups of similar purchasesâ€¢Discover unusual or outlier eventsâ€¢Similar genes in microsassay dataâ€¢Find groups of people similar behaviors â€“ e.g. votingâ€¢Find patients with related symptomsâ€¢Data compression algorithmsâ€¢And many more...,402,semantic
dbd14445-c65f-407c-92d9-50c93563f1da,IntroductionToClustering.pdf,CSCI_83,5,"Introduction to Unsupervised Learningâ€¢Different dissimilarity metrics will give different resultsâ€¢Dissimilarity metrics usually matter more than model choiceâ€¢But which one should we use? â€¢Evaluation is a significant problem with for unsupervised learningâ€¢There are no labels for objective evaluationâ€¢Evaluation is often subjectiveâ€¢But, do we have to pick one best model?â€¢Noâ€¢Different models can show different useful relationships",430,semantic
2f7d24d8-4aa8-47ba-ab68-a07decfa9c55,IntroductionToClustering.pdf,CSCI_83,6,"Measuring distance or dissimilarityâ€¢Structure in data is based on dissimilarity measured by some distance metricâ€¢The variables or cases generally have multiple observations or measurementsâ€¢Distance measures are therefore multivariate    â€¢Distance metrics can be computed for three types of variablesâ€¢Numeric distance for numeric variablesâ€¢Rank difference for ordinal variablesâ€¢Coded difference, usually binary, for unordered categorical variables",446,semantic
6201ea6f-8948-4c19-85ec-9d7621c425c2,IntroductionToClustering.pdf,CSCI_83,7,"Measuring distance or dissimilarityAxioms of distance metrics are required properties of any measure between two points in a space ğ‘‘(ğ‘¥,ğ‘¦)â€¢Distances are nonnegative ğ‘‘ğ‘¥,ğ‘¦â‰¥0â€¢Distances are positive except for the distance between a point and itself ğ‘‘ğ‘¥,ğ‘¦=0	ğ‘–ğ‘“ğ‘“	ğ‘¥=ğ‘¦â€¢Distances are symmetric ğ‘‘ğ‘¥,ğ‘¦=ğ‘‘(ğ‘¦,ğ‘¥)â€¢Distances must follow the triangle inequality (no shortcuts!)  ğ‘‘ğ‘¥,ğ‘¦â‰¤ğ‘‘ğ‘¥,ğ‘§+ğ‘‘ğ‘§,ğ‘¦	ğ‘“ğ‘œğ‘Ÿ	âˆ€	ğ‘ğ‘œğ‘–ğ‘›ğ‘¡ğ‘ 	ğ‘¥,ğ‘¦,ğ‘§",392,semantic
28dc5648-2c76-4ae6-ae4b-84224da8a659,IntroductionToClustering.pdf,CSCI_83,8,"Measuring distance or dissimilarityâ€¢Structure in data is based on dissimilarity measured by some distance metricâ€¢How do we organize the dissimilarity metrics? â€¢Create a dissimilarity matrix of the differences between each case xi and every other case xâ€™i, d(xi, xâ€™i)
DX=ğ‘‘ğ‘¥!,ğ‘¥!,ğ‘‘(ğ‘¥!,ğ‘¥"")â‹¯ğ‘‘(ğ‘¥!,ğ‘¥#)â‹® â‹± â‹®ğ‘‘ğ‘›,ğ‘¥!,ğ‘‘(ğ‘¥#,ğ‘¥"")â‹¯ğ‘‘(ğ‘¥#,ğ‘¥#)",322,semantic
25650d89-71bb-4bb9-b689-156fffb88272,IntroductionToClustering.pdf,CSCI_83,9,"Measuring distance or dissimilarityâ€¢A dissimilarity matrix contains the differences between each case xi and every other case xâ€™i, d(xi, xâ€™i)
DX=ğ‘‘ğ‘¥!,ğ‘¥!,ğ‘‘(ğ‘¥!,ğ‘¥"")â‹¯ğ‘‘(ğ‘¥!,ğ‘¥#)â‹® â‹± â‹®ğ‘‘ğ‘›,ğ‘¥!,ğ‘‘(ğ‘¥#,ğ‘¥"")â‹¯ğ‘‘(ğ‘¥#,ğ‘¥#)â€¢In general, the dissimilarity matrix is symmetricâ€¢The diagonal elements of the dissimilarity matrix are all 0, there is no dissimilarity",334,semantic
a7476816-8fbe-458c-9b97-2a589f5e1519,IntroductionToClustering.pdf,CSCI_83,10,"Measuring distance or dissimilarityâ€¢Different metrics for numeric dissimilarity can be computed for numeric variables in p dimensionsâ€¢Weighted sum of squared distance (square of Euclidian distance, L2)ğ‘‘$""ğ‘¥,ğ‘¥â€²=	>%&! 'ğ‘¤%(ğ‘¥%	âˆ’ğ‘¥â€²%)""
â€¢Weighted absolute distance or Manhattan distance (L1)ğ‘‘$!ğ‘¥,ğ‘¥â€²=>%&! 'ğ‘¤%ğ‘¥%âˆ’ğ‘¥â€²%â€¢Weighted Max; or ğ¿) normğ‘‘$)ğ‘¥,ğ‘¥â€²=max%	ğ‘¤%ğ‘¥%âˆ’ğ‘¥â€²%",351,semantic
b1f2eaf4-e39f-41ff-a06f-d2d7219a0cd9,IntroductionToClustering.pdf,CSCI_83,11,"Measuring distance or dissimilarityâ€¢Different metrics for numeric dissimilarity can be computed for numeric variables in p dimensionsâ€¢Mahalanobis Distance (standardized Euclidean distance) from a mean Âµ    	ğ‘‘(ğ‘¥,ğœ‡)=ğ‘¥âˆ’ğœ‡*ğ‘†+!ğ‘¥âˆ’ğœ‡Where, ğ‘†+! is the empirical estimate of covariance â€¢Mahalanobis Distance between two points     	ğ‘‘(ğ‘¥,ğ‘¥â€²)=ğ‘¥âˆ’ğ‘¥â€²*ğ‘†+!ğ‘¥âˆ’ğ‘¥â€²Where, ğ‘†+!=ğ‘¥Gğ‘¥â€²* is the empirical estimate of covariance between the two vectors â€¢Mahalanobis distance is scale invariant and unitless",475,semantic
846c6c52-fb17-4e03-b562-c2e08e0fbffc,IntroductionToClustering.pdf,CSCI_83,12,"Measuring distance or dissimilarityUnweighted sum of squares and Euclidian distanceğ‘‘ğ‘¥,ğ‘¥â€²=	>%&! '	(ğ‘¥%	âˆ’ğ‘¥â€²%)""
X1
X2
(2,1)
(4,3)â€¢Points at (2,1) and (4,3)â€¢What is the distance between them? â€¢Sum of squaresğ‘‘!!ğ‘¥,ğ‘¥â€²=2âˆ’4""+1âˆ’3""=8â€¢Euclidian Norm  ğ‘‘!!ğ‘¥,ğ‘¥â€²=ğ‘‘#(ğ‘¥,ğ‘¦)$/""=8=2.83â€¢Euclidian Norm is â€˜crow-fliesâ€™ distance",303,semantic
93cc6587-e551-4ce3-b99b-a5f72363995c,IntroductionToClustering.pdf,CSCI_83,13,"Measuring distance or dissimilarityManhattan Normğ‘‘ğ‘¥,ğ‘¥â€²=>%&! 'ğ‘¥%âˆ’ğ‘¥â€²%
X1
X2
(2,1)
(4,3)â€¢Points at (2,1) and (4,3)â€¢What is the distance between them? â€¢Manhattan Normğ‘‘&ğ‘¥,ğ‘¥â€²=2âˆ’4+1âˆ’3=4â€¢Manhattan distance is distance traveled on a grid",228,semantic
37d6b73a-a207-47ea-bc85-1429b6eeed0e,IntroductionToClustering.pdf,CSCI_83,14,"Measuring distance or dissimilarityâ€¢Different metrics for numeric distance can be computed for numeric variables in p dimensionsâ€¢Do all p dimensions matter equally in determining dissimilarity? â€¢No!â€¢The weights, wi, can be set for the dimensions of the attributesâ€¢Weights must add to 1.0:  âˆ‘'($)ğ‘¤'=1.0 â€¢Why not use equal weights?â€¢Attributes may not have equal importanceâ€¢Example; are the distance I have to walk and the calories of my take-out order of the same importance in selecting where I get my food? â€¢Perhaps, the calories matter more to me in determining dissimilarity between restaurants?",597,semantic
1d2b8c9d-6396-4f09-bd8c-9dfa9a2e612c,IntroductionToClustering.pdf,CSCI_83,15,"Measuring distance or dissimilarityâ€¢Different metrics for numeric distance can be computed for numeric variables in p dimensionsâ€¢Numeric variable must be standardized before performing unsupervised learningâ€¢Otherwise, like many ML methods, the importance of a variable or attribute must not be determined by the numeric rangeâ€¢A useful default is to set the weights, wi, to adjust for the variance and normalization, r, for p variables ğ‘¤%= 1ğ‘Ÿâˆ—ğ‘£ğ‘ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘ğ‘’ğ‘–, ğ‘Ÿ=>%&! 'ğ‘£ğ‘ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘ğ‘’%â€¢Or better, set weights to have meaning from the problemâ€¢Typically requires domain knowledge",562,semantic
4127900e-e55c-4591-8955-73319ee90be5,IntroductionToClustering.pdf,CSCI_83,16,"Measuring distance or dissimilarityâ€¢How can we express dissimilarity metrics for ordinal variables? â€¢Star ratings, e.g. 1 to 5 starsâ€¢Dollar range, $, $$, $$$, $$$$â€¢Based on rank differencesâ€¢Let 1 star = 1, 2 star = 2, 3 star = 3, 4 star = 4, and 5 star = 5â€¢The distance between a 2 star and 4 star restaurant is 2â€¢Or in normalized form with M possible levels, and ranks ğ‘¥%,ğ‘¥â€²% ğ‘‘ğ‘¥%,ğ‘¥â€²%=ğ‘¥%âˆ’ğ‘¥â€²%ğ‘€    On a 5-star scale, the dissimilarity between a 3 star and 5 star restaurant:3âˆ’55=0.4",480,semantic
d31741bf-b72c-4602-b24f-1e3792c62d91,IntroductionToClustering.pdf,CSCI_83,17,"Measuring distance or dissimilarityâ€¢How can we express dissimilarity metrics for unordered categorical variables? â€¢Use a coding schemeâ€¢e.g. a binary scheme, 1 if different categories, 0 if the sameâ€¢Then normalizeâ€¢e.g., normalize by dividing by number of categoriesâ€¢Example; restaurant type from 4 choices:â€¢Vegetarian, Thai, Mexican, Pizza, Burgers,â€¦.â€¢Distance Vegetarian to Burgers = Â¼ = 0.25â€¢Distance Burgers to Burgers = 0â€¢Can have more complex scheme if required",465,semantic
b2aad031-f98a-48ea-8cdb-62b67d50d225,IntroductionToClustering.pdf,CSCI_83,18,"Measuring distance or dissimilarityâ€¢Can we combine dissimilarities of different types of variablesâ€¢Yes, but with can!â€¢Scaling is important so some variable types do not dominate as a result of codingâ€¢The numeric range of values must be similarâ€¢e.g. We do not want ordinal variables to dominate numeric and categoricalâ€¢But, weighting is problem dependent!â€¢Unfortunately, no simple procedure for correct weightingâ€¢Do I care more about the star rating or type of food at a restaurant?",481,semantic
5df08d5f-5086-4bc5-89b6-b0f3101c7e81,IntroductionToClustering.pdf,CSCI_83,19,"Measuring distance or dissimilarityâ€¢An example of computing dissimilarity between restaurants
â€¢Absolute dissimilarity between R1 and R4, a Mexican and vegetarian restaurant[Â¼ + |0.8-0.3| + |0.7-0.1| + |3-1|/4 + |5-4|/5]/6 = [0.25 + 0.5 + 0.6 + 0.5 + 0.2]/6 = 0.342â€¢Absolute dissimilarity between R2 and R3, two pizza places[0 + |0.9-0.8| + |0.6-0.4| + |3-2|/4 + |4-3|/5]/6 = [0 + 0.1 + 0.2 + 0.25 + 0.2]/6 = 0.125
RestaurantTypeCaloriesDistancePrice RangeStarsR1 Mexican0.8 0.1 $ 4R2 Pizza 0.9 0.4 $$ 3R3 Pizza 0.8 0.6 $$$ 4R4 Vegetarian0.3 0.7 $$$ 5R5 Thai 0.6 0.3 $$ 4",570,semantic
3935694b-8e0f-4110-a2b7-c0937ef8fc5f,IntroductionToClustering.pdf,CSCI_83,20,"Measuring distance or dissimilarityâ€¢Another example of computing dissimilarity between restaurants
â€¢Squared dissimilarity between R1 and R4, a Mexican and vegetarian restaurant[0.252 + 0.52 + 0.62 + 0.52 + 0.22]/6 = [0.125 + 0.25 + 0.36 + 0.25 + 0.04]/6 = 0.171â€¢Squared dissimilarity between R2 and R3, two pizza places[0 + 0.12 + 0.22 + 0.252 + 0.22]/6 = [0 + 0.01 + 0.04 + 0.125 + 0.04]/6 = 0.029
RestaurantTypeCaloriesDistancePrice RangeStarsR1 Mexican0.8 0.1 $ 4R2 Pizza 0.9 0.4 $$ 3R3 Pizza 0.8 0.6 $$$ 4R4 Vegetarian0.3 0.7 $$$ 5R5 Thai 0.6 0.3 $$ 4",555,semantic
b6c6428f-730b-4199-9a1b-bf7fda58d1c0,IntroductionToClustering.pdf,CSCI_83,21,"Measuring distance or dissimilarityâ€¢How much do different metrics matter?â€¢A lot!â€¢Compare dissimilarity metrics between the restaurants
â€¢The squared distance emphasizes larger differences
Mexican â€“ Vegetarian2 Pizza PlacesAbsolute distance0.342 0.125Squared distance0.171 0.029",276,semantic
8face2a1-f140-4582-b7f3-a4fbfef4cd77,IntroductionToClustering.pdf,CSCI_83,22,"Measuring distance and similarityâ€¢Do we always work with dissimilarity?â€¢No! â€¢Some methods use measures of similarityâ€¢The closer points in a space are (smaller distance) the more similar they areâ€¢In many cases, similarity measures can be transform to dissimilarity â€¢For data with positive and negative coding similarity must be in the range [-1,1]â€¢Similarity = 1, maximum similarity, points are at the same location in the spaceâ€¢Similarity = 0, points are orthogonal in the space, no similarityâ€¢Similarity = -1, minimum similarity, points have completely opposite coding â€¢For non-negative data similarity in range [0,1] â€¢Example, binary data",640,semantic
e908251a-a52d-4d89-8699-7c5259fbacb2,IntroductionToClustering.pdf,CSCI_83,23,"Measuring Similarityâ€¢Different metrics for similarity can be computed for numeric variables in p dimensionsâ€¢Pearson distance correlation, for two vectors of values (ğ‘¥,ğ‘¥,)ğ‘ ğ‘¥,ğ‘¥â€²ğ¶ğ‘œğ‘£(ğ‘¥,ğ‘¥â€²)ğœ""(ğ‘¥)ğœ""(ğ‘¥â€²)=(ğ‘¥âˆ’Ì…ğ‘¥)(ğ‘¥â€²âˆ’Ì…ğ‘¥â€²)*ğœ""(ğ‘¥)ğœ""(ğ‘¥â€²)	â€¢Notice the different formulation as a similarity metricâ€¢Other correlation to measure similarity, e.g. Kendal, Spearman, more robust than Pearson",368,semantic
c42a957a-1f9f-4448-b517-bdd31e54d745,IntroductionToClustering.pdf,CSCI_83,24,"Measuring Similarityâ€¢Different metrics for similarity can be computed for discretely coded or numeric variables in p dimensionsâ€¢Jaccard Similarity: For discretely coded datağ‘ ğ‘¥,ğ‘¥â€²=ğ‘†ğ‘–ğ‘§ğ‘’	ğ‘œğ‘“	ğ¼ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘¥,ğ‘¥,)ğ‘†ğ‘–ğ‘§ğ‘’	ğ‘œğ‘“	ğ‘ˆğ‘›ğ‘–ğ‘œğ‘›(ğ‘¥,ğ‘¥,)=ğ‘¥âˆ©ğ‘¥â€²ğ‘¥âˆªğ‘¥â€²â€¢Jaccard similarity often used for data with many categories like natural languagesâ€¢Jaccard similarity is non-Euclidean!â€¢Cosine similarity: for numeric or discretely coded datasx,x,=cosğœƒ%=ğ‘¥Gğ‘¥â€²ğ‘¥ğ‘¥â€²",428,semantic
1473c24e-fc95-4ae6-94e2-f3262d1a3357,IntroductionToClustering.pdf,CSCI_83,25,"Measuring SimilarityCosine Similaritydx,x,=cosğœƒ%=ğ‘¥Gğ‘¥â€²ğ‘¥ğ‘¥â€²
X1
X2
(2,1)
(4,3)â€¢Points at (2,1) and (4,3)â€¢What is the distance between them? â€¢Cosine distanceğ‘‘ğ‘ğ‘œğ‘ ğ‘¥,ğ‘¥*=""âˆ—,	.$âˆ—/""!.$!""/!,!./!""/!=115$/""25$/""=0.984â€¢If vectors point in same direction ğœƒ=0,cosğœƒ=1	â€¢If vectors orthogonalğœƒ=ğœ‹2,cosğœƒ=0
q",285,semantic
05a733d2-8baa-4f8d-8cf2-60106b610125,IntroductionToClustering.pdf,CSCI_83,26,"Relationship Between Similarity and DistanceSome similarity measures can be transformed to distance metricsâ€¢Transform is an inverse functionâ€¢Resulting distance usually has limited range (limited support)â€¢Examples: Similarity MeasureTransform to Distance MetricJaccard Similarityğ‘‘-ğ‘¥,ğ‘¥,=1	âˆ’	ğ‘ -(ğ‘¥,ğ‘¥,)Cosine Similarityğ‘‘.#/01.2ğ‘¥,ğ‘¥,=ğ‘ğ‘œğ‘ +!(ğ‘ 345(ğ‘¥,ğ‘¥â€²)ğœ‹",344,semantic
9ea770f0-271e-47c5-878a-231942cab2a9,IntroductionToClustering.pdf,CSCI_83,27,"Relationship Between Distance and SimilarityIn some cases distance measures can be transformed to similarityâ€¢Typically not a unique transformation   â€¢Similarity must be in proper range:  â€¢[-1,1]â€¢[0,1] â€¢Examples of transformations (e.g. for Euclidean metrics) :  ğ‘ ğ‘¥,ğ‘¥,=11+ğ‘‘6(ğ‘¥,ğ‘¥,)ğ‘ ğ‘¥,ğ‘¥,=1ğ‘’7!(9,9"")",295,semantic
5ab2d563-91ec-448b-a1b5-c3bb64dff69f,IntroductionToClustering.pdf,CSCI_83,28,"K-Means Clusteringâ€¢The K-means clustering algorithm is arguably the most widely used methodâ€¢Long historyâ€¢First proposed as a coding method by Stuart Lloyd in 1957 â€“ not published until 1982â€¢Term â€˜k-meansâ€™ coined by MacQueen in 1967â€¢The goal of the k-means algorithm is to find the best k clustersâ€¢How do we define best for the k-means algorithm?â€¢Dissimilarity metric is sum of squared distances â€¢Minimize dissimilarity within clusters â€“ minimize the inertia within clusters
argminğ‘ª>%&! =>9âˆˆ?#
	(ğ‘¥âˆ’Ì…ğ‘¥%)"",for	k	clusters	ğ‘ªâˆˆğ¶!,ğ¶"",â€¦,ğ¶=",530,semantic
851d908c-95b5-4119-9d0a-f06c4cc25a68,IntroductionToClustering.pdf,CSCI_83,29,"K-Means Clusteringâ€¢The goal of K-means clustering algorithm is to Minimize dissimilarity or inertia within clustersargminğ‘ª>%&! =>9âˆˆ?#
	(ğ‘¥âˆ’Ì…ğ‘¥%)"",for	k	clusters	ğ‘ªâˆˆğ¶!,ğ¶"",â€¦,ğ¶=
â€¢Is there a practical algorithm to directly solve this optimization problem? â€¢Unfortunately, no!",268,semantic
635eb4e4-06f9-454e-ba4f-813e9f9491de,IntroductionToClustering.pdf,CSCI_83,29,"â€¢The unknown number of clusters, k, must be assignedâ€¢For each possible k, there are a combinatorial number of possible cluster assignments to searchâ€¢No simple search algorithm is feasible!",188,semantic
032a26d0-6789-4881-ab30-ea4e8b253e48,IntroductionToClustering.pdf,CSCI_83,30,"K-Means Clusteringâ€¢The goal of K-means clustering algorithm is to Minimize dissimilarity within clustersargminğ‘ª>%&! =>9âˆˆ?#
	(ğ‘¥âˆ’Ì…ğ‘¥%)"",for	k	clusters	ğ‘ªâˆˆğ¶!,ğ¶"",â€¦,ğ¶=
â€¢Search will not work.â€¢Need a good heuristic! â€¢Lloydâ€™s algorithm for k-means clustering1.Select k and starting (initial) centroids 2.Find the nearest cluster centroid for each x, given the means of the clusters , Ì…ğ‘¥' 3.Update the k centroids, Ì…ğ‘¥',ğ‘–âˆˆ1,2,â€¦,ğ‘˜, given new cluster members4.Repeat steps 2 and 3 until change in all Ì…ğ‘¥' is minimal",501,semantic
2fe090f4-b6ae-4455-99db-d5d99766c1fc,IntroductionToClustering.pdf,CSCI_83,31,"K-Means Clusteringâ€¢Lloydâ€™s algorithm for k-means clustering, initializes with random start and iterates over several stepsXXXX X XXXXX XXXX
2.Assign Random cluster centers
1.Start with data points3.Initial cluster assignments 4.Update cluster centers and assignments5.Iterate cluster centers and assignments until convergence",325,semantic
d499d868-8ed0-4774-9179-84d25a73781d,IntroductionToClustering.pdf,CSCI_83,32,"K-Means ClusteringLloydâ€™s algorithm for k-means clustering1.Select k and starting (initial) centroids 2.Find the nearest cluster centroid for each x, given the means of the clusters, Ì…ğ‘¥% min9âˆˆ?#,%âˆˆ{!,"",â€¦=}ğ‘¥âˆ’Ì…ğ‘¥%2,âˆ€	ğ‘–	3.Update the k centroid, Ì…ğ‘¥%,ğ‘–âˆˆ1,2,â€¦,ğ‘˜Ì…ğ‘¥%=1ğ‘›>9âˆˆ?#ğ‘¥ğ‘–
4.Repeat steps 2 and 3 until change in all Ì…ğ‘¥% is minimal - convergence",339,semantic
d51f0561-08de-4354-bd6a-6cc43b25355a,IntroductionToClustering.pdf,CSCI_83,33,"K-Means Clusteringâ€¢There are several difficulties with using the k-means algorithmâ€¢Using sum of square distance leads to only convex clustersâ€¢What is the value of k, the number of clusters?â€¢The number of clusters should reflect fundamental organization of the dataâ€¢Is there anyway to know k in advance for high dimensional problems? â€¢No, k is usually found empirically, and the selection is often a bit subjectiveâ€¢How to find good starting values of the centroids?â€¢At convergence the resulting clusters depend on the starting valuesâ€¢Is there any algorithm for finding good starting values? â€¢No!â€¢So, multiple random starts are often used
Copyright 2020, Stephen F Elston.",670,semantic
0af2b710-0b34-4b15-a5e5-94084138f542,IntroductionToClustering.pdf,CSCI_83,33,All rights reserved.,20,semantic
ffd2a2f2-5fc2-42cf-8545-da1abeb7830d,IntroductionToClustering.pdf,CSCI_83,34,"K-Means Clusteringâ€¢K-means clustering produces convex clustersâ€¢Clusters are linearly separatedâ€¢Clusters are Voronoi regionsâ€¢Analogous to k-NN methods
Credit: Scikit-Learn development team; Pedregosaet al., JMLR 12, pp. 2825-2830, 2011",234,semantic
5ce1eeaf-63ce-4111-9ab6-f8975b23d2fa,IntroductionToClustering.pdf,CSCI_83,35,"Evaluating Clustering Modelsâ€¢How do we evaluate clustering models? â€¢No direct measure as with supervised machine learningâ€¢Can we use cross validation? â€¢No, not directly, we have no labelsâ€¢Use metrics that measure the properties of the clustersâ€¢Compactness of clustersâ€¢Separation between clustersâ€¢For k-means clustering the distance metric is quadraticâ€¢Use sum of square metrics",377,semantic
bba88058-d5ed-497c-9597-f918df380ce8,IntroductionToClustering.pdf,CSCI_83,36,Evaluating Clustering Modelsâ€¢How do we evaluate clustering models? â€¢Within and between cluster sum of squaresâ€¢Can compare cluster modelsâ€¢Determine kâ€¢Other methodsâ€¢Silhouette coefficients â€“ Useful for small datasets  â€¢Calinski-Harabasz indexâ€¢Many othersâ€¦â€¦.â€¢Notice that all of these methods assume clusters are Normally distributed!,330,semantic
37e19b65-ff06-4bf7-a103-c0ece975cb03,IntroductionToClustering.pdf,CSCI_83,37,"Evaluating Clustering Modelsâ€¢Between and within cluster metricsâ€¢Within cluster sum of squares is the sum of squares within each clusterğ‘¤ğ‘ğ‘ ğ‘ =>?#
	 >9$âˆˆ?#(ğ‘¥Câˆ’Ì…ğ‘¥?#)""
â€¢Total sum of squaresğ‘¡ğ‘ ğ‘ =>%(ğ‘¥%âˆ’Ì…ğ‘¥)""
â€¢Between cluster sum of squaresğ‘¡ğ‘ ğ‘ =ğ‘¤ğ‘ğ‘ ğ‘ +ğ‘ğ‘ğ‘ ğ‘ ğ‘ğ‘ğ‘ ğ‘ =ğ‘¡ğ‘ ğ‘ âˆ’ğ‘¤ğ‘ğ‘ ğ‘ ",256,semantic
f2b21198-d906-4aee-8571-25b3dd051491,IntroductionToClustering.pdf,CSCI_83,38,Evaluating Clustering Modelsâ€¢Use WCSS and BCSS to compare clustering modelsâ€¢Limit number of clusters where WCSS and BCSS  have marginal changes,143,semantic
4f4a5e8c-655a-4d03-be8b-3e0a774d1006,IntroductionToClustering.pdf,CSCI_83,39,"Evaluating Clustering Modelsâ€¢Use WCSS to find best number of clustersâ€¢Can use knee in WCSS curve to determine best kâ€¢In the example shown, pick k=4, or k=5â€¢Prefer simpler models â€“ Occamâ€™s razor!â€¢Too many clusters is an over-fit model",233,semantic
49d73b24-2252-4096-a9ea-90887a19b047,IntroductionToClustering.pdf,CSCI_83,40,"Evaluating Clustering Modelsâ€¢The Calinski-Harabasz index or variance ratio criteria measure the ratio between the cluster compactness and separationâ€¢Starting with the WCSS matrix and the BCSS matrix, the Calinski-Harabasz index is a degree of freedom adjusted ration of the matrix trace (sum of diagonal)ğ¶ğ»ğ‘˜=ğ‘¡ğ‘Ÿ(ğµğ¶ğ‘†ğ‘†)ğ‘¡ğ‘Ÿ(ğ‘Šğ¶ğ‘†ğ‘†)ğ‘›Dâˆ’ğ‘˜ğ‘˜âˆ’1â€¢The larger the Calinski-Harabasz index the higher the ratio of cluster separation to cluster compactness.â€¢Since it is based on variance measures, Calinski-Harabasz index tends to favor convex clusters",532,semantic
e4ace662-4390-4e1e-a6a7-87e5e3814a7d,IntroductionToClustering.pdf,CSCI_83,41,Hierarchical Clustering Algorithmsâ€¢K-means algorithms form clusters with maximum compactnessâ€¢Are there other ways to create clusters with compact clusters? â€¢Hierarchical clustering algorithms create compact clusters â€¢Algorithm sequentially considers groups of pointsâ€¢Maximum compact clusters created at each step of sequential algorithmâ€¢Create hierarchy of possible clustersâ€¢One cluster with all samples at the topâ€¢Single sample clusters at the bottom,451,semantic
c4084fa3-7e33-4bb0-96d6-1d1005065bdf,IntroductionToClustering.pdf,CSCI_83,42,"Hierarchical Clustering Algorithmsâ€¢Two possible approaches to hierarchical clusteringâ€¢Divisive clustering â€“ top downâ€¢Start will all samples in one large clustersâ€¢Recursively split into compact clustersâ€¢Stop when only single samples at leaves â€“ singletons  â€¢Not discussed further hereâ€¢Agglomerative clustering â€“ bottom upâ€¢Start with each sample in an individual cluster - singletonsâ€¢Build maximumly compact clusters, until all samples in one clusterâ€¢Forms a tree with singletons at the leaves and all samples at the root",519,semantic
476ebeff-bfd1-452a-b595-de2111fb09ed,IntroductionToClustering.pdf,CSCI_83,43,"Hierarchical Clustering Algorithmsâ€¢Agglomerative clustering â€“ bottom upâ€¢Use dissimilarity between samples to determine cluster compactnessâ€¢Use most any distance metricâ€¢A linkage function determines the clusters to link at the next stepâ€¢Single linkage uses the minimum distance between members of the clusters to link clustersğ‘‘!0ğº,ğ»=min'âˆˆ2,4âˆˆ5ğ‘‘(ğ‘¥',ğ‘¥4)â€¢Complete linkage uses the minimum distance between members of the clusters to link clustersğ‘‘60ğº,ğ»=max'âˆˆ2,4âˆˆ5ğ‘‘(ğ‘¥',ğ‘¥4)â€¢Average linkage uses the minimum average distance between members of the clusters to link clustersğ‘‘!0ğº,ğ»=1ğ‘21ğ‘5L'âˆˆ2
	L4âˆˆ5
	ğ‘‘(ğ‘¥',ğ‘¥4)",599,semantic
3c072c3a-493d-4cb5-ab27-f3da9e88e50e,IntroductionToClustering.pdf,CSCI_83,44,"Hierarchical Clustering Algorithmsâ€¢Single linkage:â€¢Uses single closest pair of samples to link clustersâ€¢Can combine clusters with low threshold, chaining behaviorâ€¢Often produces clusters with poor compactnessâ€¢Complete linkage:â€¢Uses single furthest pair of sample to link clustersâ€¢Creates compact clustersâ€¢May have poor separationâ€¢Average linkage:â€¢Uses average distances to links clustersâ€¢Tries to balance compactness and separation",431,semantic
44aa734c-fefe-4ff8-b2a9-d7b8abfe5e9d,IntroductionToClustering.pdf,CSCI_83,45,"Agglomerative Clustering Exampleâ€¢Start with data points in a 2-dimensional Euclidian spaceâ€¢Use Euclidean distance and average linkage to find the first points to link â€“the leaves of the hierarchy   â€¢Continue to link points into clustersâ€¢At termination of algorithm all points are linked at root of hierarchy1
2
3
4
5
6",318,semantic
42f5fc16-c789-40ae-9c0c-d1ca6cb28747,IntroductionToClustering.pdf,CSCI_83,46,Hierarchical Clusteringâ€¢Hierarchical cluster creates dendrogramâ€¢Number of clusters determined by depth of cut pointâ€¢For example the cut-point shown results in 6 clustersâ€¢Or maybe 3 clusters?â€¢Need some domain knowledge to determine which is useful,246,semantic
9dba6ed1-988d-44b1-8e12-0d7073a175e2,IntroductionToClustering.pdf,CSCI_83,47,"Hierarchical Clusteringâ€¢Example of microassay of human tumorsâ€¢The dendrograms are quite differentâ€¢Choice of linkage function creates different hierarchy
Credit: Hastie, Tibsheirani and Friedeman, 2009",200,semantic
552d1562-40bb-40e8-9b3f-1a2b8c42b506,IntroductionToClustering.pdf,CSCI_83,48,"Evaluating Hierarchical ClusteringHow can we evaluate hierarchical clustering models? â€¢No one best methodâ€¢Often requires some subjective judgementâ€¢Different models may highlight different aspects of data structureâ€¢For Euclidean distance metric use same methods as k-means clusteringâ€¢But, sum of squares is meaningless in non-Euclidian spaces!â€¢And there is no cluster center (mean) except for Euclidean (ğ¿"") spaceâ€¢Instead use a data point â€“ the clusteriodâ€¢Radius of a cluster in non-Euclidean space is not well-defined",517,semantic
901c9115-4be5-417d-8d0e-b7e4e00c2a4f,IntroductionToClustering.pdf,CSCI_83,49,"Evaluating Hierarchical ClusteringHow can we evaluate hierarchical clustering models? â€¢Can use the linkage metric â€¢Consistent with the agglomeration criteriaâ€¢But is not independent evaluation    â€¢Cluster diameter: is the ğ¿) metric of the clusterâ€¢Does not require knowing the cluster center ğ‘‘6$=max',4	âˆ€	'84ğ‘¥'âˆ’ğ‘¥â€²4â€¢The global maximum over all clusters is then:  ğ‘‘9:;ğ¶'=	max6$ğ‘‘6$",376,semantic
f7ee7bb5-2dfd-48a5-87f7-3e9d42d2a222,IntroductionToClustering.pdf,CSCI_83,50,"What Could Possibly Go Wrong? Curse of Dimensionality!â€¢Cluster models scale poorly with dimensionality      â€¢Consider sampling required to maintain the same uniformly distributed density in a hypercube: 
â€¢An example of the Curse of Dimensionality! DimensionsSamples1 102 1003 1,0005 10510 1010",293,semantic
b5dc0ce8-aeed-4a96-a6b0-07f24def17ed,IntroductionToClustering.pdf,CSCI_83,51,"What Could Possibly Go Wrong? Curse of Dimensionalityâ€¢Cluster models scale poorly with dimensionality â€¢Distances all become the same as dimensionality â‡’âˆ      â€¢To understand this problem, consider Euclidean distance measure:   â€¢The volume of a hypersphere inside a d-dimensional hypercube with edge length 2d is:     ğ‘‰ğ‘œğ‘™ğ»ğ‘¦ğ‘ğ‘’ğ‘Ÿğ‘ ğ‘â„ğ‘’ğ‘Ÿğ‘’ğ‘  = ""2%E%/'7F7/""â€¢The proportion of volumes of the hypercube and the spheres:    H41%&'()*'+()(*H41%&'(),-.(=E%/'7""%()F7/""â‡’0,ğ‘ğ‘ 	ğ‘‘	â‡’âˆ â€¢Another example of the Curse of Dimensionality!Î“ğ‘¥ is the Gamma Function",535,semantic
1a88da30-1148-46e7-9220-33f2f4bf0c31,IntroductionToClustering.pdf,CSCI_83,52,What Could Possibly Go Wrong? Curse of Dimensionalityâ€¢The curse of dimensionality means that all clusters are the same in high dimensionsâ€¢Sampling density decreases exponentially   â€¢Distances are converge to the same size in a finite space  â€¢The choice of metric does not help   â€¢Implication is that high-dimensional cluster models are easy to overfit!!â€¢Reducing dimensionality can help,386,semantic
63acb840-8314-49df-9f9f-693963654b9c,IntroductionToClustering.pdf,CSCI_83,53,Affinity Clustering â€¢Can we build a clustering algorithm using a relationship to a few prototypes or exemplar points? â€¢Yesâ€¢Affinity clustering uses a message passing algorithm to find these responsible or exemplar points and create a connected graph of cluster membersâ€¢Affinity clustering provides useful results in several areasâ€¢Document similarityâ€¢Computer visionâ€¢Computational genetics,388,semantic
849116b0-fcbc-4572-96dc-ea6489c7e98e,IntroductionToClustering.pdf,CSCI_83,54,"Affinity Clustering â€¢Can we build a clustering algorithm using a relationship to a few prototypes or exemplar points? â€¢The number of representative points, and clusters, are determined by the algorithmsâ€¢No need to specify number of clusters apriori â€¢The message passing algorithm iteratively determinesâ€¢Responsibility, r(i,k), of sample k to be the exemplar of sample iâ€¢Availability, a(i,k), of sample k to be the exemplar of sample i",434,semantic
23a9a598-8886-4b32-b7c2-268cee9830fa,IntroductionToClustering.pdf,CSCI_83,55,"Affinity Clustering â€¢For the affinity clustering algorithm, points pass messages â€¢Given the similarity between points, s(i,k), the algorithm follows these steps1.Set initial values, r(i,k) = 0, a(i,k) = 0 2.Update the responsibilityğ‘Ÿğ‘–,ğ‘˜â†ğ‘ ğ‘–,ğ‘˜+maxâˆ€<*8<ğ‘ğ‘–,ğ‘˜+ğ‘ ğ‘–,ğ‘˜*	3.Update the availabilityğ‘ğ‘–,ğ‘˜â†mğ‘–ğ‘›0,ğ‘Ÿğ‘˜,ğ‘˜+1'/âˆ€'/8{',<}
	 ğ‘Ÿ(ğ‘–*,ğ‘˜)4.Repeat steps 2 and 3 until convergence",363,semantic
11e8bafe-6a33-4cfc-bce9-78e347f708f0,IntroductionToClustering.pdf,CSCI_83,56,"Affinity Clustering â€¢Affinity propagation determines a number of clustersâ€¢Clusters linearly separated and convexâ€¢Can define clusters as Voronoi regionsâ€¢Different similarity metrics create different clustersâ€¢Algorithm can be slow, O(n2) complexity 
Credit: Scikit-Learn development team; Pedregosaet al., JMLR 12, pp. 2825-2830, 2011",332,semantic
12bb4a78-a35f-4060-87df-e5bb73471af1,IntroductionToClustering.pdf,CSCI_83,57,"Density Clustering â€¢Using sample density is another way to form clustersâ€¢High density points form clustersâ€¢How can we find high density points? â€¢Find points with large number of near neighborsâ€¢Form clusters around these high density pointsâ€¢Naive algorithms have high complexityâ€¢Scalable algorithms are largely heuristicâ€¢Many algorithms, includingâ€¢DBSCANâ€¢OPTICS",360,semantic
7ffe96bd-1582-4027-b45e-742442f9b5b0,IntroductionToClustering.pdf,CSCI_83,58,Density Clustering â€¢DBSCAN is the first (1996) large-scale density-based clustering algorithmâ€¢Still in use todayâ€¢Many variations createdâ€¢Scales to out of memory dataset sizeâ€¢DBSCAN minimizes the number of passes through the data baseâ€¢DBSCAN finds a graph of nearest neighborsâ€¢Core points define the high density areasâ€¢Reachable non-core points are in a cluster but not coreâ€¢Unreachable points are not in a clusterâ€¢Graph edge is bidirectional if both points are coreâ€¢Graph edges from core to reachable non-core point is unidirectional,533,semantic
cf4f9679-71e9-44e6-9e77-132993705f18,IntroductionToClustering.pdf,CSCI_83,59,"Density Clustering â€¢DBSCAN has two hyperparametersâ€¢minPts is the minimum number of near neighbors a core point must haveâ€¢e is the maximum distance between neighborsâ€¢How do we define reachability?â€¢Must have a path on the graph between points p1 and pnâ€¢The path can pass through other points, {p1,p2,â€¦,pn}â€¢Distance to neighbors must be less than eâ€¢Points not reachable from any other point are not reachableâ€¢DBSCAN makes passes thorough a database to create the graphâ€¢Highly scalable, O(n log(n))  â€¢Uses memory, O(n2)",515,semantic
badf7d0e-1bd5-4105-aa6c-a53c1204dbec,IntroductionToClustering.pdf,CSCI_83,60,"Density Clustering â€¢What are the steps of DBSCAN?â€¢Start with some samples or observationsâ€¢For each point, find near neighbors within distance eâ€¢A point is core if it has > minPts near neighbors (2 in this example)â€¢If two point are core, connect with bidirectional edgeâ€¢If point is non-core, connect with unidirectional edgeâ€¢If point further that e from neighbors, point is non-reachable",386,semantic
d7c99f89-58ab-430e-ae18-16b719ebbce3,IntroductionToClustering.pdf,CSCI_83,61,"Density Clustering â€¢What are some properties of DBSCAN?â€¢Can find non-convex clustersâ€¢Is fast and efficient â€¢Optimized for uniform sample density of clusters with fixed e and minPtsâ€¢Selecting maximum distance to neighbors, e, is difficult in high dimensionsâ€¢Clusters depend on distance metricâ€¢Clusters robust to noise, non-reachable samples
Credit: Wikipedia commons",365,semantic
117b6b20-3dd9-40ab-b199-0f546c01ef96,IntroductionToClustering.pdf,CSCI_83,62,Density Clustering â€¢How can we overcome the limitations of DBSCAN?â€¢Optimized for uniform density clustersâ€¢Hard to find good value of e in high dimensions â€¢The OPTICS algorithm is an attempt to improve on DBSCANâ€¢OPTICS uses a heuristic to find two distancesâ€¢Core distance determines if a sample is coreâ€¢Reachability distance determines if one sample is reachable from the other,376,semantic
5ba7285d-20a1-4b1d-96fe-ae1df679232e,IntroductionToClustering.pdf,CSCI_83,63,"Density Clustering â€¢The OPTICS algorithm is an attempt to improve on DBSCANâ€¢Core distance for a point, p, determines if a sample is corecore_dist+,-./012ğ‘=, ğ‘ˆğ‘›ğ‘‘ğ‘’ğ‘“ğ‘–ğ‘›ğ‘’ğ‘‘	ğ‘–ğ‘“ğ‘+<ğ‘šğ‘–ğ‘›ğ‘ƒğ‘¡ğ‘ 	ğ‘šğ‘–ğ‘›ğ‘ƒğ‘¡ğ‘ 	âˆ’ğ‘ ğ‘šğ‘ğ‘™ğ‘™ğ‘’ğ‘ ğ‘¡	ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’	ğ‘–ğ‘›	ğ‘+,ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’â€¢Reachability distance of observation, o, determines if one sample is reachable from the otherreachable_dist+,-./012ğ‘œ,ğ‘=Gğ‘ˆğ‘›ğ‘‘ğ‘’ğ‘“ğ‘–ğ‘›ğ‘’ğ‘‘	ğ‘–ğ‘“ğ‘+<ğ‘šğ‘–ğ‘›ğ‘ƒğ‘¡ğ‘ 	max(core3456+,-./012ğ‘,ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘,ğ‘œ),ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’â€¢Both distances are undefined if density in the, ğ‘+, neighborhood is too lowâ€¢If p and o and nearest neighbors, then eâ€™ < e and p and o are in the same cluster",562,semantic
86665d85-8f6c-466a-95f1-ed8542bf1cef,IntroductionToClustering.pdf,CSCI_83,64,"Density Clustering â€¢How can we understand the OPTICS algorithm?â€¢Start with some samples or observationsâ€¢For a point, p, find the points within e distanceâ€¢If ğ‘I>ğ‘šğ‘–ğ‘›ğ‘ƒğ‘¡ğ‘ , determine core_dist, CDâ€¢Find reachable_dist (RD)â€¢Points beyond reachable_dist are not in the cluster
eCDRD",274,semantic
5c871015-fa59-47bd-a5e9-752eb1fbf237,IntroductionToClustering.pdf,CSCI_83,65,"Density Clustering â€¢What are some properties of OPTICS?â€¢OPTICS builds a graph with the distances determining clustersâ€¢The graph defines a dendogramâ€¢The reachability plot (bottom) shows the path distances in the dendogramâ€¢A cutoff on the reachability plot defines the clusters 
Credit: Wikipedia commons",302,semantic
76ccbca5-4cba-4220-ba7b-898f6eb36114,IntroductionToClustering.pdf,CSCI_83,66,"Density Clustering â€¢What are some properties of OPTICS?â€¢Dynamically determining core_dist helps with with variable sample densityâ€¢Computation complexity higher on average than DBSCAN
Credit: Wikipedia commons",208,semantic
acc34f58-9d59-407b-ba1a-7db10d40c7ce,IntroductionToClustering.pdf,CSCI_83,67,"Summaryâ€¢Unsupervised learning methods learn structure of dataâ€¢Structure is learned by determining association between casesâ€¢Association based on measures of proximity, distance or dissimilarityâ€¢Clustering algorithms are data mining methodsâ€¢Data mining seeks to find interesting relationships in dataâ€¢We have already encountered feature importance as a data mining method",370,semantic
ac7439bf-d5a2-4084-b63f-67bee4aff105,IntroductionToClustering.pdf,CSCI_83,68,"Summaryâ€¢Algorithms can use different distance or dissimilarity metricsâ€¢Structure based on distance metricsâ€¢Different algorithms use different metricsâ€¢What are the ideal properties of clusters? â€¢Good clusters have two propertiesâ€¢Compactness: We what the clusters to be small with members close to each otherâ€¢Separation: We want the clusters are well separated, a closeness property",380,semantic
8423f0cc-4d39-48c7-a8f6-7a75d238372e,IntroductionToClustering.pdf,CSCI_83,69,"Summaryâ€¢Different models and dissimilarity metrics will give different resultsâ€¢Dissimilarity metrics usually matter more than model choiceâ€¢But which one should we use? â€¢Evaluation is a significant problem with for unsupervised learningâ€¢There are no labels for objective evaluationâ€¢Evaluation is often subjectiveâ€¢But, do we have to pick one best model?â€¢Noâ€¢Different models can show different useful relationships",411,semantic
9398555e-2176-4c20-9452-06b3855fe19a,IntroductionToClustering.pdf,CSCI_83,70,"Summaryâ€¢Different models have different results and advantagesAlgorithmMethodCluster characteristicCharacteristicsScalabilityK-means Maximizes cluster compactness ConvexNeed to find kAssumes equal varianceVery high, O(n)
Agglomerative hierarchical Clusters into dendrogramConvexVery dependent on linkage functionVery high, O(n)Affinity clusteringResponsible point in clusterConvexLess scalableLimited, O(n2)DBSCANDensity graphNonconvexUniform density Very High, O(n log(n))OPTICSDensity graphNonconvexVariable densityHigh, O(n2)",528,semantic
71234d01-cc15-4239-ad0c-24106353bd1e,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,0,"Introduction to Nonparametric Bootstrap Methods
Steve Elston
10/06/2022",71,semantic
0a8ab95f-59ec-4436-b587-76d366950d1e,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,1,"Introduction
â€œThere were others who had forced their way to the top from the lowest rung by the aid of their bootstraps.â€James Joyce, â€˜Ulyssesâ€™ 1922
Bootstrap and re-sampling methods are widely applicable statistical methods
Resampling methods are products of the computer age
Use computational resources unimaginable in the early 20th Century
Repeatedly re-sampling the data with nonparametric model relaxes some assumptions of classical statistical methods
Re-sampling methods draw heavily on the law of large number and the central limit theorem",548,semantic
485d4e6a-8c12-4694-b5ad-7176c791dd65,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,2,"Types of Resampling Methods
Commonly used re-sampling methods include:
Randomization or Permutation methods: aka exact tests
Have a long history; e.g. Fisherâ€™s exact test (1922)
Practical approximate algorithms for larger data sets in computer era
Cross validation: resample into multiple folds without replacement
Leave n out method
Has origins in the 1950s
Widely used to evaluate machine learning (ML) models
Jackknife: leave one out re-sampling
Leave one out method
Early general purpose resampling method
Has origins in the 1950s
Nonparametric Bootstrap: resample with equivalent size and replacement - our focus here
Published by Prof Brad Efron in 1979",659,semantic
6b22b803-669a-423f-8bd0-8ed10f3a0608,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,3,"Nonparametric vs. Parametric Statistical Model
Many familiar statistic models are parametric, being based on a assumed likelihood
Likelihood models based on a parametric distributions
Parametric models have low variance estimates for statistics
But susceptible to poor choice of likelihood model
Example, least-squares error model uses a Normal likelihood
Parameters which must be estimated
Location and scale in one-dimension
Betas in higher dimensions",453,semantic
8656e4f3-2a8a-4732-a841-db21badd89b9,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,4,"Nonparametric vs. Parametric Statistical Model
Nonparametric model not based on a parametric likelihood
Use an empirical distribution estimated from observations
No likelihood model assumptions
Statistical properties estimated from this empirical distribution
Potentially high variance estimates
Need sufï¬cient sample size
Example, mean and variance estimates
Examples of nonparametric statistical estimators:
Permutation tests
Jackknife estimates
Nonparametric bootstrap",471,semantic
956eef79-5534-4aed-9ce9-14aceb355707,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,5,"General Characteristics of Nonparametric Resampling Methods
General characteristics of nonparametric resampling methods include
Allow computation of statistics from data samples for statistics with continuous derivatives
Repeatedly compute statistics from multiple resamples of dataset
The result converges to the sample distribution of the statistic being computed
Make minimal distributional assumptions, when compared to classical frequentist statistics",456,semantic
54fc2e11-3f54-41ca-a4f6-a8327b9e19c2,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,6,"Pitfalls of Resampling Methods
Re-sampling methods are general and powerful but, there is no magic involved! There are pitfalls! If a sample is biased, the resampled statistic estimate based on that sample will be biased
Results can be no better than the sample you start with
Example; the bootstrap estimate of mean is the unbiased sample estimate, , not the population parameter, 
The sample variance and Cis can be no better than the sample distribution allows
Often higher variance than parametric models
Be suspicious of overly optimistic conï¬dence intervals
CIs can be optimistically biased
Are computationally intensive, but often highly parallelizable
ğ‘¥Â¯ ğœ‡",664,semantic
3b4b2c9b-0a89-40b1-8965-d8a20481023e,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,7,"Point Estimates vs. Distribution Estimates
The goal of frequentist statistics is to compute a point estimate and conï¬dence interval
Point estimate is the single most likely value for a statistic
Conï¬dence interval expresses the uncertainty of the point estimate
Parametric conï¬dence interval based on the properties of some assumed probability distribution
Are there alternatives to this classical frequentist approach? Here we focus on bootstrap methods which do not require explicit probability model",502,semantic
d42737ab-3a81-4497-89e2-dbc4b64c38ea,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,8,"Bootstrap Distribution
Rather than computing a point estimate directly, bootstrap methods compute a bootstrap distribution of a statistic
Bootstrap distribution is comprised of values of the statistic computed from bootstrap resamples of the original observations (data sample)
Computing bootstrap distribution requires no assumptions about population distribution! Bootstrap resampling substitutes computer power for paper and pencil statistician power
Bootstrap resampling estimates the bootstrap distribution of a statistic
Compute mostly likely point estimate of the statistic, or bootstrap estimate
The bootstrap conï¬dence interval is computed from the bootstrap distribution",680,semantic
9520d4fc-0303-473c-8172-b2ebe03c2ed4,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,9,"Bootstrap Distribution
Rather than repeatedly resample the population, bootstrapping repeatedly resamples an original sample
Resampling to estiamte the bootstrap distribution of a statistic",189,semantic
edad7771-1ade-4735-a9cd-17a6957b68b8,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,10,"Overview of the Bootstrap Algorithm
The bootstrap method follows a simple algorithm. Estimates of the point estimate of a statistic are accumulated by these steps:
1. Randomly Bernoulli sample sample n data with replacement from an original data sample of n values;
The resample is the same size as the original data sample
2.",326,semantic
a70fa7ba-4661-4059-81dd-632d9d399558,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,10,"Re-compute the statistic with each resample
3. Repeat steps 1 and 2 to accumulate the required number of bootstrap samples
4. Accumulated bootstrap values form the bootstrap distribution; An estimate of the sample distribution of the statistic
5. The mean of the computed statistic values is the bootstrap point estimate of the statistic
By law of large numbers, bootstrap point estimate converges
Efrom and Tibshirani (1993) and Efron and Hasti (2016) recommend using at least 200 bootstrap samples for point estimates
Other authors recommend a larger number (e.g. 1,000-2,000) of resamples given low computer cost",615,semantic
2eec29b9-be2c-4347-8275-e1550351387a,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,11,"Overview of the Bootstrap Algorithm
Outline of bootstrap resampling algorithm to compute mean",93,semantic
7d2edcb5-29ff-49b8-afa6-1d093dc543dd,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,12,"Example; One Sample Bootstrap
Use sample of standardized scores of highschool students from UCLA Statistical Consulting
##      female  race  ses  schtyp  prog  read  write  math  science  socst## id                                                                     ## 70        0     4    1       1     1    57     52    41       47     57## 121       1     4    2       1     3    68     59    53       63     61## 86        0     4    3       1     1    44     33    54       58     31## 141       0     4    3       1     3    63     44    47       53     56## 172       0     4    2       1     2    47     52    57       53     61## 113       0     4    2       1     2    44     52    51       63     61## 50        0     3    2       1     1    50     59    42       53     61## 11        0     1    2       1     2    34     46    45       39     36## 84        0     4    2       1     1    63     57    54       58     51## 48        0     3    2       1     2    57     55    52       50     51",1008,semantic
4e1b8f22-4e40-4e2a-80d1-d05018614397,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,13,"Example; One Sample Bootstrap
Histogram of the math scores",58,semantic
212af512-ce86-4779-8130-2317c2e15555,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,14,"Example; One Sample Bootstrap
Function to compute single sample bootstrap estimate of statistic
## Bootstrap point estimate =  52.61
def bootstrap_statistic(x, b, statistic):
    '''    Function Computes b one-sample bootstrap estimates of data x    using statistic function. '''    n_samps = len(x)    boot_vals = []    for _ in range(b):
        ## The heavy work is done here. The statistic is computed         ## using the bootstrap sample of the observations         boot_vals.append(statistic(nr.choice(x, size=n_samps, replace=True)))
    boot_estimate = np.mean(boot_vals)    print('Bootstrap point estimate = {:6.2f}'.format(boot_estimate))    return(boot_estimate, boot_vals)          
bootstrap_mean_estimate, boot_means = bootstrap_statistic(math, 2 0 0 , np.mean)",776,semantic
c479c5e8-2819-4ef1-8e42-044e1c138f90,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,15,"Example; One Sample Bootstrap
Distribution of 200 bootstrap samples of mean estimates
## bootstrap point estimate =  52.66",122,semantic
e6f5cd4c-3d0c-43f2-88e7-069b94533b5e,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,16,"Bootstrap Conï¬dence Intervals
Distribution of 2000 bootstrap bootstrap conï¬dence intervals? Use percentile method:
1. Deï¬ne conï¬dence level, eg. 95% or Î± =0.05
2.",162,semantic
11d178a0-94e0-4784-a3c1-2942ad6668bb,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,16,"Order b bootstrap samples, $, by value
3. Lower CI index; 
4. Upper CI index; 
Percentile method is know to be biased
Bias correction methods available
Efrom and Tibshirani (1993) and Efron and Hasti (2016) recommend using at least 2,000 bootstrap samples to estimate conï¬dence intervals
Other authors recommend a larger number (e.g. 5,000-20,000) of resamples given low computer cost
ğ‘ ğ‘–
ğ‘–=ğ‘âˆ— Î±/2
ğ‘–=ğ‘âˆ— (1âˆ’Î±/2)",409,semantic
6b15d141-0309-48e8-b31e-256e952cf173,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,17,"Bootstrap Conï¬dence Intervals
Bootstrap conï¬dence intervals are known to be biased! Often bootstrap CIs are overly optimistic
Bias can be signiï¬cant for asymmetric distributions",177,semantic
14526383-d126-4139-874f-ed6df4b96845,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,18,"Example; One Sample Bootstrap
Bootstrap distribution of 2000 of mean estimates with conï¬dence intervals
## bootstrap point estimate =  52.63
## At alpha = 0.05, lower and upper bootstrap confidence intervals =  51.37    53.95",225,semantic
0c8fd61a-a075-4a18-8286-501a7619c339,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,19,"Two Sample Bootstrap
How can we apply the bootstrap algorithm for two-sample statistics? Example, difference of means of two independently sampled populations
How to generate bootstrap samples? Can we just sample the concatenation of the two samples? No!",254,semantic
ade752ef-5a2a-46ab-b3df-386284c8fef1,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,19,"There is no guarantee of a correct number of resamples for each group
Imbalanced sampling leads to bias
Must independently sample the two groups or populations
Use two independent bootstrap samples to compute statistic
Step one; compute statistic from independent resamples
Step two; compute (another) statistic from the two bootstrap estimates",344,semantic
e59a1b79-a364-48be-8788-f99993bd5c79,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,20,"Two Sample Bootstrap
Example: algorithm to compute difference of means:
1. Independently randomly Bernoulli sample n data with replacement from each original data sample; The number of resamples for eachpopulations is the number of samples for that population
2. Compute the statistic (e.g. mean) for the two resamples
3.",321,semantic
868a21a4-f728-4c24-814d-67ea31cba495,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,20,"Compute the two-sample statistic; e.g. difference of means
4. Repeat steps 1, 2, and 3 to accumulate the required number of bootstrap samples Accumulated bootstrap values form the bootstrap distribution; anestimate of the sample distribution of the statistic
5. The mean of the computed statistic values is the bootstrap point estimate of the statistic; e.g. difference of means
6. Compute CIs from bootstrap distribution",421,semantic
5c8a22fe-ce39-4018-977d-7b84ab803ee3,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,21,"Two Sample Bootstrap
Example; ï¬nd the bootstrap distribution of the difference in math scores between low and middle SES students.",130,semantic
d42160c5-75e9-4ad3-95e3-1198cc94466a,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,22,"Example, Two Sample Bootstrap
def two_boot_two_stat(sample_1, sample_2, b, statistic_1, two_samp_statistic):    '''    Function computes two sample and two statistic bootstrap estimate. - sample_1 and sample_2, independent obervation vectors   
    - b, number of bootstrap samples to compute    - statistic 1, statistic applied to the two independent bootstrap samples    - two_sample_statistic, statistic applied to the independent bootstrap statistics
    '''    two_boot_values = []    n_samps_1 = len(sample_1)    n_samps_2 = len(sample_2)
    for _ in range(b):          ## Heavy lisfting is done here. First, the two independent bootstrap estimates        ## are computed from independent bootstrap resamples. Second, the statistic 
        ## of the two bootstrap estimates is computed.",794,semantic
47a8a67a-d2bf-449a-80a8-354fa521f34a,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,22,"boot_estimate_1 = statistic_1(nr.choice(sample_1, size=n_samps_1, replace=True))        boot_estimate_2 = statistic_1(nr.choice(sample_2, size=n_samps_2, replace=True))        two_boot_values.append(two_samp_statistic(boot_estimate_1, boot_estimate_2))
    boot_estimate = np.mean(two_boot_values)    print('bootstrap point estimate = {:6.2f}'.format(boot_estimate))    return(boot_estimate, two_boot_values)    
math_low_ses = test_scores.loc[test_scores.loc[:,'ses']==1 ,'math'] math_mid_ses = test_scores.loc[test_scores.loc[:,'ses']==2 ,'math']bootstrap_diff_of_mean, boot_diffs = two_boot_two_stat(math_low_ses, math_mid_ses, 2 0 0 0 , np.mean, lambda x,y: x-y)",666,semantic
d2323728-97fd-4578-87d1-d3e0b34ea14e,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,23,"Example, Two Sample Bootstrap
Compute and display the bootstrap distribution of the difference of student scores
## bootstrap point estimate =  -3.06
## At alpha = 0.05, lower and upper bootstrap confidence intervals =  -6.13     0.14",234,semantic
3396f467-9464-46a3-a563-37891dce1b68,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,24,"Summary
Bootstrap estimation is widely useful and requires minimal assumption
Bootstrap distribution is comprised of values of the statistic computed from bootstrap resamples of the original observations (data sample)
Computing bootstrap distribution requires no assumptions about population distribution! Bootstrap resampling substitutes computer power for paper and pencil statistician power
Bootstrap resampling estimates the bootstrap distribution of a statistic
Compute mostly likely point estimate of the statistic, or bootstrap estimate
The bootstrap conï¬dence interval is computed from the bootstrap distribution",620,semantic
f7f32ad2-d537-4028-91ce-4f777b0aebec,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,25,"Summary
There are several variations of the basi nonparametric bootstrap algorithm
One sample bootstrap
Inference on single populations
Two sample bootstrap
Inference on different populations
Special cases
Correlation coefï¬cients - part of your assignment",255,semantic
c320e891-2243-420f-a2b0-15882d819a9a,Introduction to Nonparametric Bootstrap Methods.pdf,CSCI_83,26,"Summary
Re-sampling methods are general and powerful but, there is no magic involved! There are pitfalls! If a sample is biased, the resampled statistic estimate based on that sample will be biased
Results can be no better than the sample you start with
Example; the bootstrap estimate of mean is the unbiased sample estimate, , not the population parameter, 
The sample variance and Cis can be no better than the sample distribution allows
Be suspicious of overly optimistic conï¬dence intervals
CIs can be optimistically biased
Are computationally intensive, but often highly parallelizable
ğ‘¥Â¯ ğœ‡",596,semantic
9331e4d7-acff-4a25-b946-47a37b453c42,Perception for Scientific Visualization.pdf,CSCI_83,0,"Perception for Scientiï¬c Visualization
Steve Elston
08/28/2022",62,semantic
35cc1956-0eaf-4ba5-ab64-7a09e47e637f,Perception for Scientific Visualization.pdf,CSCI_83,1,"Why This Course? 21st Century datasets are large and complex
Complexity is often harder to address than size
Complexity makes understanding of relationships in data difï¬cult
Complexity addressed with computer-intensive methods
Our focus is on the big ideas of computer-intensive statistics and data analysis arising in the late 20th and early 21st Centuries",357,semantic
b84c638a-3ee9-4a46-b0e3-7dc60dd2ccc7,Perception for Scientific Visualization.pdf,CSCI_83,2,"Why This Course? Data science is the science of understanding data
Complexity makes understanding difï¬cult
Statistics is the science of making principled inferences from data
Inference leads to understanding
Inference is becoming harder with large complex data sets
Doing rigorous data science requires understanding statistics
Statistical practice has advanced signiï¬cantly to address large complex data sets
Statistical practice now dominated by computer-intensive methods",474,semantic
3364c971-61c1-4899-b1b4-30297314dd3e,Perception for Scientific Visualization.pdf,CSCI_83,3,"What Weâ€™ll Cover
Our focus is on the big ideas of computer-intensive statistics and data analysis arising in the late 20th and early 21st Centuries
Exploratory data analysis (EDA) to understand relationships in big complex data sets
Foundations of algorithms used throughout statistics and machine learning such as maximum likelihood
Computer intensive resampling methods for building models and inference, Bootstrapping and MCMC Bayes
Large scale hypothesis testing and sparse models for complex and high-dimensional data sets
Bayesian hierarchical models for complex relationships
Modern time series and forecasting algorithms for data with serial correlation
Robust statistics to deal with data violating model assumptions",725,semantic
36c93ab6-a4ee-4846-8e22-e41b780930c9,Perception for Scientific Visualization.pdf,CSCI_83,4,"Course Objectives
This fast-moving survey course helps build your toolbox for modeling complex data
Broad introduction to the theoretical and methodological basis of data science
Conditional probability theory
Sampling theory
Statistical estimation theory - classical and resampling based
Understand models for complex datasets
Understanding data relationships and inference
How these methods work and when to used them
How conï¬dent should we be in our inferences? Moving beyond a cookbook or blog post approach to data science",527,semantic
dbb3d5d1-3412-42d9-a4eb-8d7f98745ff7,Perception for Scientific Visualization.pdf,CSCI_83,5,"Instructor: Steve Elston
Data science consultant with several decades of experience
Instructor for Harvard since 2016
Lead team that commercialized Bell Labs S, now open source R
Company co-founder and held executive positions in several industries
Creator of multiple edX courses, author of Oâ€™Reily books and articles
Holder of 5 issued patents
BS, physics and math (minor), University of New Mexico
MS and PhD, geophysics, Princeton University â€“ NSF, John von Neuman Supercomputing Fellow",490,semantic
ee79d1fb-f5b7-4b45-8dab-91bb73e3dc57,Perception for Scientific Visualization.pdf,CSCI_83,6,"Teaching Assistant: Moustafa Saleh
A principal data scientist at Oracle Cloud
Received PhD in computer science from University of Texas at San Antonio
Worked previously at Microsoftâ€™s Advanced Threats Protection team developing ML solutions for malware detection
Research mainly focused on applying machine learning solutions to cyber-security challenges",354,semantic
867fbd53-6eb7-4e40-a9ba-50ca066692f1,Perception for Scientific Visualization.pdf,CSCI_83,7,Teaching Assistant: Tatyana Boland,34,semantic
098c4ba1-d9fb-4624-a512-d54838cd7ec7,Perception for Scientific Visualization.pdf,CSCI_83,8,"Grading: Undergraduate
Activity Grade weight
Participation (graded discussions) 10%
Assignments 90%
Hands-on assignments tie theory to practice applied to data examples
Most of us only recall methods we actaully use
Lectures provide introduction only
Expect to work out some details
Do not hesitate to ask for help on concepts or coding!!",338,semantic
8cb2b4c2-2e11-472f-b77e-846b24e36074,Perception for Scientific Visualization.pdf,CSCI_83,9,"Grading: Graduate
Activity Grade weight
Participation (graded discussions) 10%
Assignments 60%
Independent project proposal 5%
Independent project report 25%
Hands-on assignments tie theory to practice applied to data examples
Most of us only recall methods we actaully use
Lectures provide introduction only
Expect to work out some details
Do not hesitate to ask for help on concepts or coding!! You will execute an end-to-end project on an appropriate problem of you choice",475,semantic
ed8cc1a0-a03a-4f22-99a3-17a2d5c334e3,Perception for Scientific Visualization.pdf,CSCI_83,10,"Grading: Graduate
Independent Graduate Project:
An end-to-end project you will execute independently
Pay careful attention to grading rubric in Canvas
Can be great addition to your data science project portfolio
Pick a problem of particular interest to you! Plan to spend about 80 hours on your analysis and report
Sufï¬cient data must be available
Must use analytical methods within the scope of this course - e.g. no advanced ML or deep learning
Start thinking about your project soon - donâ€™t put it off
Resources to help you get started are under the Resources tab in Ed
List of possible data sources
Example project proposals and reports",640,semantic
c52aaca0-eb00-4136-a2d5-2ff1b8f43231,Perception for Scientific Visualization.pdf,CSCI_83,11,"Late Assignment Policy
Timely feedback is an important part of the learning process
To allow the timely release of solutions for assignments this course applied a late assignment policy:
Up to one day late - no penalty
Up to 6 days late - less 20%
More than 6 days late - no credit
Advice: start assignments and your project as soon as you can so you have time to address problems and ask questions! Note: No extension is possible for Graduate Independent Projects!",465,semantic
847758f5-341b-4ec3-9507-3a177e19cdb0,Perception for Scientific Visualization.pdf,CSCI_83,12,"Class Schedule
Class meeting Tuesdays, 6:00 pm US Eastern Time:
Focus on theory to understand concepts
Limited time for code discussions
Section meetings on Wednesday or Thursday, 6:00 pm US Eastern Time:
Focus on answer student questions - your questions! Discus code and coding problems
Background and supplementary material as needed
Poll to ï¬nd best day for class
All class meetings are recorded for on-demand viewing",421,semantic
a4112b43-f1c2-4453-8f69-63c407c276a9,Perception for Scientific Visualization.pdf,CSCI_83,13,"Communications
Communicating with your instructors and other students is a signiï¬cant aspect of participation in this course! Ask questions about the course material, homework, etc. Ask questions in the public forum so others can answer and gain from the discussion: if you have a question others do as well!",308,semantic
bb64a473-0599-48fb-923c-e7c50190a63a,Perception for Scientific Visualization.pdf,CSCI_83,13,"Answer other studentsâ€™ questions
Comment on weekly graded discussion topics
Ed is the primary communications method- Generally use public posts - okay to include code snippets- Option to ask instructors private questions
Ask for the help you need!",247,semantic
caeff8c1-8a32-4a00-94b4-27386b456719,Perception for Scientific Visualization.pdf,CSCI_83,14,"Communications
For private matters, you can directly communicate with the instructional team:- Grades- Absences- Etc
Steve Elston, Instructor, stephen.elston gmail.comMoustafa Saleh, TA, msaleh83 gmail.comTatyana Boland, TA, tatyanaboland gmail.com
Ofï¬ce hours: If you need individual assistance, please ask to schedule ofï¬ce hours. Donâ€™t be shy! Communications by Canvas may be signiï¬cantly delayed!",400,semantic
adc6d4ee-0d75-4208-89bf-b9b9ce098d50,Perception for Scientific Visualization.pdf,CSCI_83,15,"Poll
Then, back to the lecture",30,semantic
c1105274-7fcc-4c98-8e3d-7c285f1dbccc,Perception for Scientific Visualization.pdf,CSCI_83,16,"Why Exploration and Visualization? Exploratory data analysis (EDA) tools are essential to good data science
Isnâ€™t the goal of data science to build machine learning models? Not always!",184,semantic
8afb6ee4-2a57-4978-989a-cbbae1727417,Perception for Scientific Visualization.pdf,CSCI_83,16,"Often we need to understand relationships found in data
Explain scientiï¬c or behavioral relationships
Determine if a relationship is important
Our goal is to gain deep understanding for complex problem
EDA methods
Statistical inference",235,semantic
72cbd461-70a4-425e-bc45-12561b198391,Perception for Scientific Visualization.pdf,CSCI_83,17,"Why Exploration and Visualization? Exploratory data analysis (EDA) tools are essential to good data science
Why not just start building machine learning models? Understanding relationships in data saves missteps and unexplained poor model performance
Which variables are actually important? How do these variables behave?",321,semantic
f9d6a101-fe56-4ffe-b3e5-c1ca854a77c8,Perception for Scientific Visualization.pdf,CSCI_83,17,"Are there errors and outliers in the data? How good is a model ï¬t? Communications is an important component of data science
Analytic results are only useful if they are understood and trusted
Graphical presentation greatly assists understanding by less technical colleagues",273,semantic
6d900b96-b0da-4b93-9786-2cdc15cd57fb,Perception for Scientific Visualization.pdf,CSCI_83,18,"Why is Perception Important? Goal: Communicate information visually
Visualization technique maximize the information a viewer perceives
Gain insights when exploring relationships in data
Communicate insights to others
limits of human perception are a signiï¬cant factor in understanding complex relationships
Can apply results of the considerable research on human perceptions for data visualization",398,semantic
7ea6c9ed-4cef-4a6e-850a-ed6b128a005e,Perception for Scientific Visualization.pdf,CSCI_83,19,"Use Aesthetics to Improve Perception
Use aesthetics to improve perception
We take a very broad view of the term â€˜aestheticâ€™ here
A plot aesthetics is any property of a visualization which highlight aspects of the data relationships
Aesthetics are used to project additional dimensions of complex data
Plots generally restricted to 2-dimensional surface
Must projet multiple dimensions of complex data on 2-d surface",415,semantic
17ba13ba-7b23-4371-9455-e853982eb9c3,Perception for Scientific Visualization.pdf,CSCI_83,20,"Organization of Plot Aesthetics
We can organize aesthetics by their effectiveness:
1. Easy to perceive plot aesthetics: help most people gain understanding of data relationships
2. Aesthetics with moderate perceptive power: useful properties to project data relationships when used sparingly
3.",294,semantic
42d07b1d-1d27-4e50-8965-bc0b4aec5255,Perception for Scientific Visualization.pdf,CSCI_83,20,Aesthetics with limited perceptive power: useful within strict limits,69,semantic
c920a371-af49-442f-b941-c549c2786e2e,Perception for Scientific Visualization.pdf,CSCI_83,21,"Properties of Common Aesthetics
Property or AestheticPerceptionData Types
Aspect ratio Good Numeric
Regression lines Good Numeric plus categorical
Marker position Good Numeric
Bar length Good Counts, numeric
Sequential color palette Moderate Numeric, ordered categorical
Marker size Moderate Numeric, ordered categorical
Line types Limited Categorical
Qualitative color paletteLimited Categorical
Marker shape Limited Categorical
Area Limited Numeric or categorical
Angle Limited Numeric",487,semantic
a095bfc5-ff29-4b59-9c29-2b36c02f6f54,Perception for Scientific Visualization.pdf,CSCI_83,22,"Aspect Ratio
Aspect ratio has a signiï¬cant inï¬‚uence on how a viewer perceives a chart
Correct aspect ratio can help highlight important relationships in complex data sets
But, wrong aspect ratio can hide or mislead! We express aspect ratio as follows:
Banking angle is key to understanding how the aspect ratio affects perception
ğ‘ğ‘ ğ‘ğ‘’ğ‘ğ‘¡Â ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œ= Â :1ğ‘¤ğ‘–ğ‘‘ğ‘¡â„ â„ ğ‘’ğ‘–ğ‘”â„ ğ‘¡",361,semantic
796d7630-d579-492a-b360-4d3c91c1bb86,Perception for Scientific Visualization.pdf,CSCI_83,23,"Example of Changing Aspect Ratio
Longest scientiï¬c time series is the sunspot count:
##      YEAR  SUNACTIVITY## 0  1700.0          5.0## 1  1701.0         11.0## 2  1702.0         16.0## 3  1703.0         23.0## 4  1704.0         36.0",235,semantic
8af89d51-7f6c-4931-b6b4-fac080683187,Perception for Scientific Visualization.pdf,CSCI_83,24,"Example of Changing Aspect Ratio
Example uses data from 1700 to 1980
Can you perceive the asymmetry in these sunspot cycles?",124,semantic
09553455-3e4d-4cf2-8a34-94f1c0030337,Perception for Scientific Visualization.pdf,CSCI_83,25,"Example of Changing Aspect Ratio
Notice how changing aspect ratio change perception of the asymmetry?",101,semantic
4a9069d0-7d04-441f-989f-a5808a24f129,Perception for Scientific Visualization.pdf,CSCI_83,26,"Sequential and Divergent Color Palettes
Use of color as an aesthetic in visualization is a complicated subject. color is often used, also often abused
A qualitative palette is a palette of individual colors for categorical values
Sequential palettes and divergent palettes are a sequence of colors
Numeric variables
Ordered categorical variable",344,semantic
8496a93c-813a-4bdc-9f25-f7df37f7bdcd,Perception for Scientific Visualization.pdf,CSCI_83,27,"Auto Weight by Sequential Color Palette
fig, ax = plt.subplots(figsize=(8 ,1 0 ))fig.subplots_adjust(top=0 . 8 , bottom=0 . 2 )ax = sns.scatterplot(x='city_mpg', y='curb_weight', data=auto_price,                      hue = 'price', palette = 'magma', ax=ax)
_=ax.set_title('City MPG vs.",286,semantic
ce39b880-4c7f-4ce5-9282-84ee618ad669,Perception for Scientific Visualization.pdf,CSCI_83,27,"Weight \nPrice by color') #, fontsize=12)_=ax.set_xlabel('City MPG') # , fontsize=12)_=ax.set_ylabel('Vehicle Weight') #, fontsize=12) 
plt.show()",146,semantic
41ae8161-9d85-46ce-bf50-2a80f2b37d02,Perception for Scientific Visualization.pdf,CSCI_83,28,Auto Weight by Sequential Color Palette,39,semantic
9261b35f-4731-48fe-a248-6d88236057a5,Perception for Scientific Visualization.pdf,CSCI_83,29,"Limits of color
Regardless of the approach there are some signiï¬cant limitations
A signiï¬cant number of people are color blind. Red-green color blindness is most common
Even the best sequential or divergent palettes show only relative value of numeric variables
Perception of exact numeric values is difï¬cult, except in special cases
Cannot perceive large number of colors for categories",387,semantic
63cf220d-3acf-4bc6-99c4-ae414b500fb9,Perception for Scientific Visualization.pdf,CSCI_83,30,"Marker Size
Marker size is moderately effective aesthetic
Used properly, marker size can highlight important realationships in complex data sets
Numeric values
Ordinal variables
Viewers can generally perceive relative differences, but not actual values
Small size differences are not preceptable
Only relative relationship in numeric variables
Limited steps of categorical variables",382,semantic
9af51469-df7e-4f55-9a50-ecd417a43e72,Perception for Scientific Visualization.pdf,CSCI_83,31,"Engine Size by Marker Size and Price by Sequential Color Palette
fig, ax = plt.subplots(figsize=(8 ,7 ))ax = sns.scatterplot(x='city_mpg', y='curb_weight', data=auto_price,                     hue = 'price', palette = 'magma',                     size = 'engine_size', sizes = (5 0 . 0 , 4 0 0 . 0 ),
                     ax=ax)_=ax.set_title('City MPG vs.",356,semantic
c83b7ca7-481d-48a4-a44d-1277b36bbc2f,Perception for Scientific Visualization.pdf,CSCI_83,31,"Weight \nPrice by color, engine size by size') #, fontsize=18)_=ax.set_xlabel('City MPG') #, fontsize=16)
_=ax.set_ylabel('Vehicle Weight')# , fontsize=16) plt.show()",166,semantic
dfd6457b-1216-44c7-a981-affc403edd1e,Perception for Scientific Visualization.pdf,CSCI_83,32,Engine Size by Marker Size and Price by Sequential Color Palette,64,semantic
9c9cd3b2-58a1-4820-a6fe-0bdf537f6b00,Perception for Scientific Visualization.pdf,CSCI_83,33,"Line Plots and Line Type
Line plots connect discrete, ordered, data points by a line
Can use different colors and line pattern types to differentiate categories
Only useful for a limited number of lines on one graph
Too many similar colors and line patterns on one plot leads to viewer confusion and poor perception",315,semantic
26316129-12cb-4181-93c3-4531f9c48ea6,Perception for Scientific Visualization.pdf,CSCI_83,34,Limits of Line Type,19,semantic
ddb1ba87-e669-49fb-830a-c2f2c41a3663,Perception for Scientific Visualization.pdf,CSCI_83,35,"Marker Shape
Marker shape is useful for displaying categorical relationships
This aesthetic is only useful when two conditions are met:
1. The number of categories is small
2. Distinctive shape are chosen for the markers
Human perception limits the number of shapes humans can perceive well",290,semantic
8c20f833-92bd-4af1-a98d-e0df056e9cc6,Perception for Scientific Visualization.pdf,CSCI_83,36,Aspiration by Marker Shape,26,semantic
876d5741-4c53-485c-a42d-2921be189ca2,Perception for Scientific Visualization.pdf,CSCI_83,37,"Regression Lines
Regression lines draw viewers attention
Typically use a nonlinear regression line
Polynomial
Splines - piece wise model
Lowess - local nonlinear regression
Bootstrap conï¬dence intervals show range of probable trends
More about bootstrap resampling later
Ideally want strait line relationship
Nonlinear relationships often arise from non-Normal distributions
Linear relationship is more intuitive",412,semantic
ea104da0-f50b-48ce-9f40-2fb435ccb29b,Perception for Scientific Visualization.pdf,CSCI_83,38,"Regression Line and Tramsformation Example
Regression lines highlight the trends for gas and diesel engines
Second order polynomial ï¬t",134,semantic
9db1ab0d-717b-459f-adf9-f32c35a3169e,Perception for Scientific Visualization.pdf,CSCI_83,39,"Regression Line and Tramsformation Example
What are the distributions of these variables? Notice the right skew of these distributions",134,semantic
7b28939d-039c-4e11-87f9-1cfaabc426d0,Perception for Scientific Visualization.pdf,CSCI_83,40,"Regression Line and Tramsformation Example
Can transform distribution of variables
Want distribution closer to Normal
Many possible transformations
Logarithmic: Often good choice for variables with values 
Square and square root: Good choice for many physical systems
Power transformation: Find best ï¬t transformation
Fit to paramteric distribution: Test if variable follows a known distribution
>0",398,semantic
d07b6236-41fb-4de2-b346-7160a41a87ec,Perception for Scientific Visualization.pdf,CSCI_83,41,"Regression Line and Tramsformation Example
Can transform distribution of variables
Multiple algorithms have been developed
Box-Cox, the ï¬rst and still widely used for values 
Yeo-Johnson, works for values 
The Box-Cox transform ï¬ts a value for  that minimizes error with respect to a Normal distribution
See the Scikit-Learn Usersâ€™ Guide for more details
>0
â‰¤0
ğœ†
=ğ‘¥( ğœ†) ğ‘–
â§
â©
â¨âª
âª
âˆ’1ğ‘¥ğœ†ğ‘–ğœ†
ln()ğ‘¥ğ‘–
ifÂ ğœ†â‰ 0,
ifÂ ğœ†=0,",410,semantic
424d9602-d285-4eea-9721-e54a3e5539c8,Perception for Scientific Visualization.pdf,CSCI_83,42,"Regression Line and Tramsformation Example
Transform the distributions
pt = PowerTransformer(method='box-cox')
pt.fit(np.array(auto_price['price']).reshape(-1 , 1 ))auto_price['power_price'] = pt.transform(np.array(auto_price['price']).reshape(-1 , 1 ))
fig, ax = plt.subplots(1 ,2 ,figsize=(1 2 , 4 ))ax[0 ]=sns.kdeplot(x = 'power_price',                       hue = 'fuel_type',                      data=auto_price,
                      ax=ax[1 ]);ax[0 ].set_title('Density of power transformed price');ax[0 ].set_xlabel('Transformed price');
ax[1 ]=sns.kdeplot(x = 'curb_weight',                       hue = 'fuel_type',                      data=auto_price,                      ax=ax[1 ]);
ax[1 ].set(xscale=""log"");ax[1 ].set_title('Density of log vehicle weight');ax[1 ].set_xlabel('Log vehicle weight');plt.show()",822,semantic
a78ba6f8-4666-4d3f-8361-e6859e37692c,Perception for Scientific Visualization.pdf,CSCI_83,43,"Regression Line and Tramsformation Example
Transform the distributions
Distribution of logarithm of the curb weight",115,semantic
7c508751-c280-49d1-9d60-e44edcaf71dc,Perception for Scientific Visualization.pdf,CSCI_83,44,"Regression Line and Tramsformation Example
Transform the distributions
Distribution of power transformed price, 
Transformed distribution has minimal skew! ğœ†=âˆ’0.64",163,semantic
5b87cd58-b40c-4cc6-b152-66b2950429a4,Perception for Scientific Visualization.pdf,CSCI_83,45,Regression Line and Tramsformation Example,42,semantic
32c8f210-0415-4eb9-8a38-1ee01c5c1746,Perception for Scientific Visualization.pdf,CSCI_83,46,Second order polynomial ï¬t,26,semantic
6d2165d0-5b6b-4d93-8036-ba076894e808,Perception for Scientific Visualization.pdf,CSCI_83,47,"Summary
We have explored these key points- Visualization is a powerful EDA method- Understand relationships in data- Communicate data scince insights
Proper use of plot aesthetics enable projection of multiple dimensions of complex data onto the 2-dimensional plot surface. All plot aesthetics have limitations which must be understood to use them effectively
The effectiveness of a plot aesthetic varies with the type and the application
Regression lines help to focus viewer on trends
Transformations to linear relationships can be informative",545,semantic
31ffaf6f-ad10-4f7e-b491-cdfe37dcd0c9,Visualization of Large Complex Data.pdf,CSCI_83,0,"Visualization of Large Complex Data
Steve Elston
09/11/2023",59,semantic
328fc2ec-4842-425f-9507-f9dbfc235a21,Visualization of Large Complex Data.pdf,CSCI_83,1,"Review: Why is Perception Important? Goal: Communicate information visually
Visualization technique maximize the information a viewer perceives
Limits of human perception are a signiï¬cant factor in understanding complex relationships
Apply results of research on human perceptions for data visualization",303,semantic
3c72e656-7dd1-4a36-a018-f956aec7620a,Visualization of Large Complex Data.pdf,CSCI_83,2,"Use Aesthetics to Improve Perception
Use aesthetics to improve perception
Take a very broad view of the term â€˜aestheticâ€™
A plot aesthetics is any property of a visualization which highlight aspects of the data relationships
Aesthetics are used to project additional dimensions of complex data onto the 2-dimensional plot surface",328,semantic
5aa4e915-8cbc-4539-8daf-999c18281811,Visualization of Large Complex Data.pdf,CSCI_83,3,"Regression Lines
Regression lines draw viewers attention
Typically use a nonlinear regression line
Polynomial
Splines - piece wise model
Lowess - local nonlinear regression
Bootstrap conï¬dence intervals show range of probable trends
More or bootstrap resampling later
Ideally want strait line relationship
Nonlinear relationships often arise from non-Normal distributions
Linear relationship is more intuitive",409,semantic
3c998444-7357-4967-ba5e-db4692189892,Visualization of Large Complex Data.pdf,CSCI_83,4,"Properties of Common Aesthetics
Property or AestheticPerceptionData Types
Aspect ratio Good Numeric
Regression lines Good Numeric plus categorical
Marker position Good Numeric
Bar length Good Counts, numeric
Sequential color palette Moderate Numeric, ordered categorical
Marker size Moderate Numeric, ordered categorical
Line types Limited Categorical
Qualitative color paletteLimited Categorical
Marker shape Limited Categorical
Area Limited Numeric or categorical
Angle Limited Numeric",487,semantic
b78dc44e-0118-4712-8093-ad8cfc6377e3,Visualization of Large Complex Data.pdf,CSCI_83,5,"Visualizing Large Complex Data is Difï¬cult
Problem: Modern data sets are growing in size and complexity
Goal: Understand key relationships in large complex data sets
Difï¬culty: Large data volume
Modern computational systems have massive capacity
Example: Use map-reduce algorithms on cloud clusters
Difï¬culty: Large numbers of variables
Huge number of variables with many potential relationships
This is the hard part! Note: we will address use of dimensionality reduction techniques in another lesson, time permitting",518,semantic
efc75cf6-62d0-430b-873a-79eb9ccd371e,Visualization of Large Complex Data.pdf,CSCI_83,6,"Limitation of Scientiï¬c Graphics
All scientiï¬c graphics are limited to a 2-dimensional projection
But, complex data sets have a great many dimensions
We need methods to project large complex data onto 2-dimensions
Generally, multiple views are required to understand complex data sets
Donâ€™t expect one view to show all important relationships
Develop understanding over many views
Try many views, donâ€™t expect most to be very useful",432,semantic
4295a2c6-183d-4453-b3fe-b1891bd19400,Visualization of Large Complex Data.pdf,CSCI_83,7,"Approaches to display of complex data relationships
Generally combine multiple methods to effectively display complex data
Use plots that inherently scale
Avoid over-plotting to ensure plot is understandable
Choose plot types that do not exhibit overplotting
Often a creative case speciï¬c plot type is best
Use multi-axis plots
Scatter plot matricies
Facet plots
Filter cases using cognositics",393,semantic
1f01f49c-d423-4323-b01c-898579939cf5,Visualization of Large Complex Data.pdf,CSCI_83,8,"Scalable Chart Types
Some chart types are inherently scalable. Bar plots: Counts can be computed; e.g. use map-reduce
Histograms: Data is binned in parallel
Box plots: Finding the quartiles is a scalable counting process
KDE and violin plots: Similarly to the box plot, using kernel density estimation",301,semantic
2cd44d17-db71-4e43-b3b5-60fbd3747542,Visualization of Large Complex Data.pdf,CSCI_83,9,"Over-plotting
Over-plotting occurs in plots when the markers lie one on another. Common, even in relatively small data sets
Scatter plots can look like a blob and be completely uninterpretable
Over-plotting is a signiï¬cant problem in EDA and presentation graphics",263,semantic
35930ea8-8886-4416-82ef-7d8a01304e06,Visualization of Large Complex Data.pdf,CSCI_83,10,"Dealing with Over-plotting
What can we do about over-plotting? Marker transparency: so one can see markers underneath; useful in cases with minimal overlap of markers
Marker size: smaller marker size reduces over-plotting within limits
Adding jitter: adding a bit of random jitter to variables with limited number of values
Random down-sampling: for very large data sets, you may only need a representative sample to understand key data relationships",450,semantic
a88c8fac-0ed9-4ab0-9d44-ea7e7da0e3b4,Visualization of Large Complex Data.pdf,CSCI_83,11,Example of Overplotting,23,semantic
4b022ef0-9125-4cd0-87e1-a579a099071a,Visualization of Large Complex Data.pdf,CSCI_83,13,"Use Transparency, Marker Size, Downsampling
Down sample to 20%, alpha = 0.1, size = 2",85,semantic
33845c81-0259-4bf9-9e9b-9b9180a3ebfc,Visualization of Large Complex Data.pdf,CSCI_83,15,"Other Methods to Display Large Data Sets
Alternatives to avoid over-plotting for truly large data sets
Hex bin plots: the 2-dimensional equivalent of the histogram
Frequency of values is tabulated into 2-dimensional hexagonal bins
Displayed using a sequential color palette
2-d kernel density estimation plots: natural extension of the 1-dimensional KDE plot
Good for moderately large data
Heat map: values of one variable against another
Categorical (count) or continuous variables
Carefully choose color pallet, sequential or divergent
Mosaic plots: display multidimensional count (categorical) data
Uses tile size and color to project multiple dimensions
2-d equivalent of a multivariate bar chart
Dimensionality reduction: we will discuss this later in the course",767,semantic
5980d2fe-1977-4952-afd2-bb732cc27392,Visualization of Large Complex Data.pdf,CSCI_83,16,"Hexbin Plot
Example: Density of sale price by time",50,semantic
7de2e9b0-7d67-4858-b6ec-c5276587102e,Visualization of Large Complex Data.pdf,CSCI_83,18,"Countour Plot
Example: Contour plot of 2-D KDE of sale price vs. time",69,semantic
aaf6b3c0-80cb-4cfd-930d-70317db6be9d,Visualization of Large Complex Data.pdf,CSCI_83,19,"Heat Map
Example: Airline passenger counts by month and year displayed by squential heat map
Heat map of",104,semantic
1b2bf1f9-11fa-4e87-b55a-c0c8868d70f7,Visualization of Large Complex Data.pdf,CSCI_83,20,"Mosaic Plots
How can we display multidimensional count (categorical) data at scale? Mosaic plots displays the relative proportion of counts of a contingency table
Plot area is divided and fully ï¬lled by a set of tiles
The larger the count, the larger tile area",260,semantic
5a1435df-4c52-40a3-b078-9babd7474e79,Visualization of Large Complex Data.pdf,CSCI_83,21,"Mosaic Plots
Example: How do counts change with working vs. non-working day, weather and year? Mosaic plot displays tiles with size proportional to count
Counts conditioned on variables, working day, weather, year
bike_share_df = pd.read_csv('../data/BikeSharing.csv')
## Add month column with namesbike_share_df.loc[:,'month'] = [calendar.month_abbr[i] for i in bike_share_df.loc[:,'mnth']]
## Add column with human readable weather conditionsweather = {1 :'Clear', 2 :'Mist', 3 :'Light precipitation', 4 :'Heavy precipitation'}
bike_share_df.loc[:,'Weather'] = [weather[i] for i in bike_share_df.loc[:,'weathersit']]
season = {1 :'winter', 2 :'spring', 3 :'summer', 4 :'fall'}bike_share_df.loc[:,'Season'] = [season[i] for i in bike_share_df.loc[:,'season']]
year = {0 :'2011', 1 :'2012'}bike_share_df.loc[:,'year'] = [year[i] for i in bike_share_df.loc[:,'yr']]working = {0 :'No_Work', 1 :'Working'}
bike_share_df.loc[:,'WorkingDay'] = [working[i] for i in bike_share_df.loc[:,'workingday']]
categorical_cols = ['year','Season', 'WorkingDay', 'Weather']
fig, ax = plt.subplots(figsize=(2 5 , 2 0 ))plt.rcParams.update({'font.size': 1 8 })
_=mosaicplot.mosaic(bike_share_df.loc[:,categorical_cols],                     index=list(categorical_cols),                     title = 'Counts of weather conditions',                    ax=ax)
plt.show()",1347,semantic
655310da-c921-4d3b-8ec7-0b57a40442ca,Visualization of Large Complex Data.pdf,CSCI_83,22,Mosaic Plots,12,semantic
bdce6979-26ee-4de8-9bf2-241896b03500,Visualization of Large Complex Data.pdf,CSCI_83,23,Facet plot of wind by month,27,semantic
98490140-55d7-4b9b-a1fa-0558755074cc,Visualization of Large Complex Data.pdf,CSCI_83,24,"Other Methods to Display Large Data Sets
Sometimes a creative alternative is best
Often situation speciï¬c; many possibilities
Finding a good one can require signiï¬cant creativity! Example, choropleth for multi-dimensional geographic data
Example, time series of box plots",271,semantic
a25648e6-4f5d-4ce6-8ed4-e0451ace1547,Visualization of Large Complex Data.pdf,CSCI_83,25,"Time Series of Box Plots
Example: Time ordered box plots of quarterly sales price",81,semantic
f248fc1a-e392-4f26-b642-d29cab71d455,Visualization of Large Complex Data.pdf,CSCI_83,26,"Displays for Complex Data
How can we understand the relationships in complex high-dimensional data with many variables? Must confront limitations for 2-dimensional projection inherent in computer graphics
Need methods to scale to higher dimensions
Apply combination of arrays of plots and ï¬ltering
Aesthetics: used to increase the number of projected dimensions",361,semantic
687e3add-c7e0-4002-ac86-dbfe36547edd,Visualization of Large Complex Data.pdf,CSCI_83,27,"Displays for Complex Data
How can we understand the relationships in complex high-dimensional data with many variables? Arrays of plots: subsets show relationships in a complex data set
Pairwise scatter plots: matrix of all pairwise combinations of variables
pairwise scatter plots can be created for subsets of large and complex data sets. Faceting: uses values of categorical or numeric variables to plot subsets
Subsets are displayed on an array of plots
Typically use axes on same scale to ensure correct perception of relationships
Faceting goes by several other monikers, conditional plotting, method of small multiples, lattice plotting
Cognostics: sort large number of variables to ï¬nd important relationships",717,semantic
ae3f7eb7-15dc-4fb3-894b-84dd203eda90,Visualization of Large Complex Data.pdf,CSCI_83,28,"Arrays of Plots
Display multiple plot views in an array or grid
Create an array of plots which project multiple related views of data relationships
Organize axes to give multidimensional view
Example, scatterplot with kde plots on the margins
Supported by Seaborn jointplot",273,semantic
4074e8e7-93fe-45e4-a38a-de1cf17fd810,Visualization of Large Complex Data.pdf,CSCI_83,29,"Scatter Plot Matrix
Scatter plot matrix used to investigate relationships between a number of variables
Key idea: Display a scatter plots of each variable versus all other variables
Primarily EDA tool
Conveys lots of information - requires study! Each pairwise relationship is displayed twice
Two possible orientations
Or two different plot types
Can place histograms and KDE plots on diagonal",393,semantic
9aed1aff-74cc-4bcb-8cb1-f5ba5e7976bd,Visualization of Large Complex Data.pdf,CSCI_83,30,"Scatter Plot Matrix
Scatter plot matrix create two dimensional array of plots of variable pairs- Upper triangular plots: Scatter plots with regression lines- Lower triangular plots: Hexbin plots of density- Diagonal plots: Histograms of variables
g = sns.PairGrid(diabetes.drop('Sex', axis=1 ), hue='sex_categorical', palette=""Set2"", height=1 . 5 )_=g.map_upper(sns.regplot, order=1 , truncate=True, scatter_kws={'s':0 . 5 })_=g.map_lower(plt.hexbin, alpha=0 .",460,semantic
bafec02d-72d3-4dfb-af53-3c87830e60f9,Visualization of Large Complex Data.pdf,CSCI_83,30,"5 , cmap='Blues', gridsize=1 5 , linewi dths=0 )
_=g.map_diag(plt.hist, histtype=""step"",linewidth=1 )plt.show()",111,semantic
12383e44-4381-4060-8a63-3b91e430e86f,Visualization of Large Complex Data.pdf,CSCI_83,31,"Scatter Plot Matrix
Scatterplot matrix with different plot types",64,semantic
bea251e9-2d5a-47f2-af6d-c5ceaa856ee4,Visualization of Large Complex Data.pdf,CSCI_83,32,"Facet Plots
Facet plots revolutionized statistical graphics starting about 30 years ago
Facet plots extend the number of dimensions projected onto 2-d plot surface
Key idea: Create array of plots of subsets of the data
Create subsets using a group-by operation on other variables
Lay out grid of plots on axes with same scale
Organize by row and column grouping variables
Display same plot type for each group
Can add speciï¬c aesthetics, etc.",442,semantic
19a43b39-511b-42f8-842b-12509a13b03e,Visualization of Large Complex Data.pdf,CSCI_83,33,"Facet Plots
Like many good ideas facet plotting was invented several times
Multiple contemporaneous inventors and names
Tufte, 1990, introduced method of small multiples
Cleveland, 1992, introduced trellis plotting
Also known as conditioned plots
Nowdays, most packages use term facet plot",289,semantic
5a9b7332-978d-446f-9487-d91417848777,Visualization of Large Complex Data.pdf,CSCI_83,34,"Facet Plot with wind speed by Month
Facet plot projected on a grid
Grid deï¬ned by variable values for row and column
Can tile or shingle numeric variables for grid
Tile categorical variables
Overlapping shingles of numeric variables
Can create variables by combinations of categories from other variables
Example: Histogram of wind speed conditioned on month
Plot grid has month in columns
Histogram of windspeed is created for each monthly subset
g = sns.FacetGrid(bike_share_df, col='month', col_order=calendar.month_abbr[1 :] ,col_wrap = 4 , height=5 )g = g.map(plt.hist, ""windspeed"", bins=2 0 , color=""b"", alpha=0 . 5 )",623,semantic
c14a13b3-c819-47d0-a42f-8465c4281128,Visualization of Large Complex Data.pdf,CSCI_83,35,"Facet Plot with wind speed by Month
Facet plot of wind by month",63,semantic
85ac8619-1632-4dc2-85bf-46b9a91da634,Visualization of Large Complex Data.pdf,CSCI_83,36,"Facet Plot of Hourly Counts by Weather and Season
Example: Plot count of riders by hour, conditioned on weather and season
Grid deï¬ned by season on the rows and weather in the columns
Scatter plot of count of riders by hour of the data for each group
g = sns.FacetGrid(bike_share_df, col=""Weather"", col_order=weather.values(), row=""Season"", height=2 , aspect=2 )g = g.map(sns.scatterplot, ""hr"", ""cnt"", s=3 , alpha=0 . 2 )
for ax in g.axes.flat:    ax.set_title(ax.get_title(), fontsize=1 0 )",491,semantic
f3f66f9d-a5b7-4d72-9725-fcf0c2c78e0e,Visualization of Large Complex Data.pdf,CSCI_83,37,"Facet Plot of Hourly Counts by Weather and Season
Example: Plot count of riders by hour, conditioned on weather and season
Facet plot of count by season and weather",164,semantic
a184840c-e900-465f-9c23-0220f4d80c5a,Visualization of Large Complex Data.pdf,CSCI_83,38,"Facet Plot of Hourly Counts by Weather and Season
Example: Plot count of riders by hour, conditioned on weather and season
Grid deï¬ned by season on the rows and weather in the columns
Boxplots of count of riders by hour of the data for each group
Time-ordered box plots can improve perception over scatter plots
Eye tends to follow the lines
g = sns.FacetGrid(bike_share_df, col=""Weather"", col_order=weather.values(), row=""Season"", height=2 , aspect=2 )
g = g.map(sns.boxplot, ""hr"", 'cnt', color='lightgray', order=None)for ax in g.axes.flat:    ax.set_title(ax.get_title(), fontsize=1 0 )for i in range(4 ):
    g.axes[i,0 ].set_ylabel('Count')",645,semantic
657c5a5f-0d1e-488b-9667-0b8222ecd6c0,Visualization of Large Complex Data.pdf,CSCI_83,39,"Facet Plot of Hourly Counts by Weather and Season
Example: Box plot counts of riders by hour, conditioned on weather and season
Facet plot of count by season and weather",169,semantic
e3bd6c36-b816-42df-9ae1-c8f691ecda7f,Visualization of Large Complex Data.pdf,CSCI_83,40,"Congnostics
How can we visualize very high dimensional data? Modern data sets have thousands to millions of variables
Cannot possibly look at all of these
Idea: need to ï¬nd the most important relationships
Use a cognostic to sort relationship
Cognostic is a statistic to sort data
Sort the variables or relationships by the cognostic
Plot relationships with most interesting cognostic
Idea originally proposed by Tukey, 1982, 1985",430,semantic
2dcf3d22-b1ae-48b5-9009-67f2e6649a74,Visualization of Large Complex Data.pdf,CSCI_83,41,"Cognistic: States With Fastest Rate of Housing Price Increase
## Add an intercept column to the data framehousing.loc[:,'intercept'] = [1 . 0 ] * housing.shape[0 ]## Demean the decimal time columnmean_time = housing.loc[:,'time_decimal'].mean()
housing.loc[:,'time_demean'] = housing.loc[:,'time_decimal'].subtract(mean_time)
## Find the slope coefficients for each state
def prepare_temp(df, group_value, group_variable = 'state'):    temp = df.loc[df.loc[:,group_variable]==group_value,:].copy()    mean_price = np.mean(temp.loc[:,'log_medSoldPriceSqft'])    temp.loc[:,'log_medSoldPriceSqft'] = np.subtract(temp.loc[:,'log_medSoldPriceSqft'], mean_price)
    std_price = np.std(temp.loc[:,'log_medSoldPriceSqft'])    temp.loc[:,'log_medSoldPriceSqft'] = np.divide(temp.loc[:,'log_medSoldPriceSqft'], std_price)    return(temp, mean_price, std_price)
 
def compute_slopes(df, column, group_variable='state'):    slopes = []
    entities = []    intercepts = []    for e in df.loc[:,column].unique():
        temp, mean_price, std_price = prepare_temp(df, e, group_variable=column)        temp_OLS = sm.OLS(temp.loc[:,'log_medSoldPriceSqft'],temp.loc[:,['intercept','time_demean']]).fit()        slopes.append(temp_OLS.params.time_demean)        intercepts.append(temp_OLS.params.intercept)
        entities.append(e)     slopes_df = pd.DataFrame({'slopes':slopes, 'intercept_coef':intercepts, 'entity_name':entities})        slopes_df.sort_values(by='slopes', ascending=False, inplace=True)
    slopes_df.reset_index(inplace=True, drop=True)     return slopes_df
#compute_slopes(housing, 'state')state_slopes = compute_slopes(housing, 'state')
## PLot states with the fastest growing pricing
def find_changes(df, slopes, start, end, col='state'):    increase  = slopes.loc[start:end,'entity_name']    increase_df = df.loc[df.loc[:,col].isin(increase),:]
    increase_df = increase_df.merge(slopes, how='left', right_on='entity_name', left_on=col)    return(increase_df, increase)big_increase_states, increase_states = find_changes(housing, state_slopes, 0 , 7 )    
## Display scatterplot vs timedef plot_price_by_entity(df, order, entity='state', xlims=[2 0 0 7 . 5 , 2 0 1 6 . 5 ]):    g = sns.FacetGrid(df, col=entity, col_wrap=4 , height=2 , col_order=order)
    g = g.map(sns.regplot, 'time_decimal', 'log_medSoldPriceSqft',               line_kws={'color':'red'}, scatter_kws={'alpha': 0 .",2397,semantic
05c02f05-34b0-4a76-b58b-95df13f68603,Visualization of Large Complex Data.pdf,CSCI_83,41,"1 , 's':0 . 5 })    g.set(xlim=(xlims[0 ],xlims[1 ]))    plt.show()
_=plot_price_by_entity(big_increase_states, increase_states)",128,semantic
3f96de5a-3720-4992-b182-34e2ca6c964e,Visualization of Large Complex Data.pdf,CSCI_83,42,"Cognistic: States With Fastest Rate of Housing Price Increase
States with greatest increase in housing price",108,semantic
d49a8855-d1af-4af3-b039-3a4e8af67e00,Visualization of Large Complex Data.pdf,CSCI_83,43,"Summary
We have explored these key points
Proper use of plot aesthetics enable projection of multiple dimensions of complex data onto the 2-dimensional plot surface. All plot aesthetics have limitations which must be understood to use them effectively
The effectiveness of a plot aesthetic varies with the type and the application
Visualization of modern data sets, growing in size and complexity
Visualization limited by 2-dimensional projection
Goal: Understand key relationships in large complex data sets
Difï¬culty: Large data volume
Modern computational systems have massive capacity
Example: Use map-reduce algorithms on cloud clusters
Difï¬culty: Large numbers of variables
Huge number of variables with many potential relationships
This is the hard part!",761,semantic
86335df1-d211-4a1d-bc81-145801469ecc,Visualization of Large Complex Data.pdf,CSCI_83,44,"Summary
Generally combine multiple methods to effectively display complex data
Use plots that inherently scale
Avoid over-plotting to ensure plot is understandable
Choose plot types that do not exhibit overplotting
Often a creative case speciï¬c plot type is best
Use multi-axis plots
Scatter plot matricies
Facet plots
Filter cases using cognositics",349,semantic
